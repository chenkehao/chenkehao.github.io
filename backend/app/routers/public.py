"""
Public API Router - å…¬å¼€æ¥å£ï¼Œæ— éœ€ç™»å½•
"""

import json
from typing import List, Optional
from datetime import datetime, timedelta

from fastapi import APIRouter, Depends, Query, Body, HTTPException
from sqlalchemy import select, func
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import selectinload

from app.database import get_db, AsyncSessionLocal
from app.models.job import Job, JobTag, JobStatus
from app.models.flow import Flow, FlowStatus, FlowStage
from app.models.candidate import Candidate, CandidateProfile
from app.schemas.job import JobListResponse, JobResponse

router = APIRouter()


# ============ èŒä½ç›¸å…³å…¬å¼€æ¥å£ ============

@router.get("/jobs", response_model=JobListResponse)
async def list_public_jobs(
    page: int = Query(1, ge=1),
    page_size: int = Query(20, ge=1, le=100),
    search: Optional[str] = None,
    location: Optional[str] = None,
    job_type: Optional[str] = Query(None, description="èŒä½ç±»å‹: full_time/part_time/contract/internship/remote"),
    tag: Optional[str] = Query(None, description="æ ‡ç­¾åç§°ç­›é€‰"),
    experience: Optional[str] = Query(None, description="ç»éªŒè¦æ±‚ç­›é€‰"),
    salary_min: Optional[int] = Query(None, description="æœ€ä½è–ªèµ„(å…ƒ/æœˆ)"),
    salary_max: Optional[int] = Query(None, description="æœ€é«˜è–ªèµ„(å…ƒ/æœˆ)"),
    sort: Optional[str] = Query("newest", description="æ’åº: newest/salary_desc/salary_asc/match"),
    db: AsyncSession = Depends(get_db)
):
    """è·å–å…¬å¼€èŒä½åˆ—è¡¨ï¼ˆæ”¯æŒæœç´¢ã€æ ‡ç­¾ã€è–ªèµ„ã€ç±»å‹ç­‰å¤šç»´ç­›é€‰ï¼‰"""
    from sqlalchemy import or_
    from app.models.job import JobTag, job_tags_association
    
    query = select(Job).options(selectinload(Job.tags)).where(Job.status == JobStatus.ACTIVE)
    
    if search:
        query = query.where(
            or_(
                Job.title.ilike(f"%{search}%"),
                Job.company.ilike(f"%{search}%"),
                Job.description.ilike(f"%{search}%")
            )
        )
    
    if location:
        query = query.where(Job.location.ilike(f"%{location}%"))
    
    if job_type:
        query = query.where(Job.job_type == job_type)
    
    if experience:
        query = query.where(Job.experience_required.ilike(f"%{experience}%"))
    
    if salary_min is not None:
        query = query.where(Job.salary_min >= salary_min)
    
    if salary_max is not None:
        query = query.where(Job.salary_max <= salary_max)
    
    if tag:
        query = query.join(job_tags_association).join(JobTag).where(JobTag.name.ilike(f"%{tag}%"))
    
    count_query = select(func.count()).select_from(query.subquery())
    total_result = await db.execute(count_query)
    total = total_result.scalar()
    
    # æ’åº
    if sort == "salary_desc":
        query = query.order_by(Job.salary_max.desc().nullslast(), Job.created_at.desc())
    elif sort == "salary_asc":
        query = query.order_by(Job.salary_min.asc().nullslast(), Job.created_at.desc())
    else:
        query = query.order_by(Job.created_at.desc())
    
    offset = (page - 1) * page_size
    query = query.offset(offset).limit(page_size)
    
    result = await db.execute(query)
    jobs = result.scalars().all()
    
    return JobListResponse(
        items=jobs,
        total=total,
        page=page,
        page_size=page_size,
        pages=(total + page_size - 1) // page_size if total > 0 else 0
    )


@router.get("/jobs/{job_id}")
async def get_public_job(
    job_id: int,
    db: AsyncSession = Depends(get_db)
):
    """è·å–èŒä½è¯¦æƒ…"""
    result = await db.execute(
        select(Job).options(selectinload(Job.tags)).where(Job.id == job_id)
    )
    job = result.scalar_one_or_none()
    
    if not job:
        return {"error": "èŒä½ä¸å­˜åœ¨"}
    
    # å¢åŠ æµè§ˆé‡
    job.view_count += 1
    await db.commit()
    
    return {
        "id": job.id,
        "title": job.title,
        "company": job.company,
        "location": job.location,
        "description": job.description,
        "salary_min": job.salary_min,
        "salary_max": job.salary_max,
        "salary": f"Â¥{job.salary_min//1000}k - Â¥{job.salary_max//1000}k" if job.salary_min and job.salary_max else "é¢è®®",
        "job_type": job.job_type,
        "requirements": job.requirements,
        "benefits": job.benefits,
        "tags": [tag.name for tag in job.tags],
        "logo": job.logo or "ğŸ’¼",
        "ai_intro": job.ai_intro or "AI æ™ºèƒ½ä½“æ­£åœ¨åˆ†æèŒä½åŒ¹é…åº¦",
        "view_count": job.view_count,
        "created_at": job.created_at.isoformat() if job.created_at else None,
    }


@router.get("/job-tags")
async def get_all_job_tags(db: AsyncSession = Depends(get_db)):
    """è·å–æ‰€æœ‰å²—ä½æ ‡ç­¾ï¼ˆå»é‡ï¼‰"""
    from app.models.job import JobTag
    result = await db.execute(select(JobTag).order_by(JobTag.name))
    tags = result.scalars().all()
    return [{"id": t.id, "name": t.name, "category": t.category} for t in tags]


@router.get("/jobs-recommended")
async def get_recommended_jobs(
    limit: int = Query(5, ge=1, le=20),
    db: AsyncSession = Depends(get_db)
):
    """è·å–æ¨èèŒä½åˆ—è¡¨"""
    result = await db.execute(
        select(Job)
        .options(selectinload(Job.tags))
        .where(Job.status == JobStatus.ACTIVE)
        .order_by(Job.created_at.desc())
        .limit(limit)
    )
    jobs = result.scalars().all()
    
    return [{
        "id": job.id,
        "title": job.title,
        "company": job.company,
        "location": job.location,
        "salary": f"Â¥{job.salary_min//1000}k - Â¥{job.salary_max//1000}k" if job.salary_min and job.salary_max else "é¢è®®",
        "salary_min": job.salary_min,
        "salary_max": job.salary_max,
        "match": 85 + (job.id % 15),  # æ¨¡æ‹ŸåŒ¹é…åº¦
        "tags": [tag.name for tag in job.tags][:3],
        "logo": job.logo or "ğŸ’¼",
        "aiIntro": job.ai_intro or "AI æ™ºèƒ½ä½“æ­£åœ¨åˆ†æèŒä½åŒ¹é…åº¦",
        "job_type": job.job_type.value if job.job_type else None,
        "experience_required": job.experience_required,
        "education_required": job.education_required,
        "created_at": job.created_at.isoformat() if job.created_at else None,
    } for job in jobs]


# ============ å·¥ä½œæµç›¸å…³å…¬å¼€æ¥å£ ============

def _format_salary(job) -> str:
    """æ ¼å¼åŒ–è–ªèµ„æ˜¾ç¤º"""
    if not job:
        return "é¢è®®"
    if job.salary_display:
        return job.salary_display
    if job.salary_min and job.salary_max:
        return f"Â¥{job.salary_min // 1000}k-{job.salary_max // 1000}k"
    if job.salary_min:
        return f"Â¥{job.salary_min // 1000}k+"
    return "é¢è®®"


@router.get("/flows")
async def get_public_flows(
    limit: int = Query(10, ge=1, le=50),
    user_id: Optional[int] = Query(None, description="å½“å‰ç™»å½•ç”¨æˆ·IDï¼ŒæŒ‰è§’è‰²è¿‡æ»¤"),
    db: AsyncSession = Depends(get_db)
):
    """è·å–å·¥ä½œæµåˆ—è¡¨ â€” æ ¹æ®ç”¨æˆ·è§’è‰²è¿”å›ä¸åŒè§†è§’çš„æ•°æ®"""
    from app.models.user import User, UserRole

    # ç¡®å®šç”¨æˆ·è§’è‰²
    user_role = None
    if user_id:
        u_result = await db.execute(select(User).where(User.id == user_id))
        u = u_result.scalar_one_or_none()
        if u:
            user_role = u.role

    # æ„å»ºæŸ¥è¯¢
    query = (
        select(Flow)
        .options(selectinload(Flow.steps), selectinload(Flow.timeline))
    )

    if user_role in (UserRole.RECRUITER, UserRole.ADMIN):
        # ä¼ä¸šæ–¹ï¼šåªçœ‹è‡ªå·±å‘èµ·çš„æ‹›è˜æµç¨‹
        query = query.where(Flow.recruiter_id == user_id)
    elif user_role == UserRole.CANDIDATE:
        # äººæ‰æ–¹ï¼šé€šè¿‡ candidate è¡¨å…³è”æŸ¥è‡ªå·±çš„ flow
        cand_result = await db.execute(
            select(Candidate.id).where(Candidate.user_id == user_id)
        )
        cand_id = cand_result.scalar_one_or_none()
        if cand_id:
            query = query.where(Flow.candidate_id == cand_id)
        else:
            return []  # è¯¥ç”¨æˆ·æ²¡æœ‰ candidate è®°å½•

    query = query.order_by(Flow.updated_at.desc()).limit(limit)
    result = await db.execute(query)
    flows = result.scalars().all()

    # è·å–å…³è”çš„èŒä½å’Œå€™é€‰äººä¿¡æ¯
    flow_list = []
    for flow in flows:
        job_result = await db.execute(select(Job).where(Job.id == flow.job_id))
        job = job_result.scalar_one_or_none()

        candidate_result = await db.execute(
            select(Candidate)
            .options(selectinload(Candidate.profile))
            .where(Candidate.id == flow.candidate_id)
        )
        candidate = candidate_result.scalar_one_or_none()

        # æ˜ å°„çŠ¶æ€
        status_val = flow.status.value if flow.status else "pending"
        status_map = {
            "interviewing": "active",
            "offer": "completed",
            "accepted": "completed",
            "rejected": "rejected",
            "evaluating": "screening",
        }
        frontend_status = status_map.get(status_val, "pending")

        # æ ¹æ®è§’è‰²æ„å»ºä¸åŒçš„è¿›åº¦èŠ‚ç‚¹å’Œæè¿°
        if user_role == UserRole.CANDIDATE:
            # äººæ‰æ–¹è§†è§’ï¼šæ±‚èŒæŠ•é€’ â†’ æ™ºèƒ½åŒ¹é… â†’ ç­›é€‰è¯„ä¼° â†’ ç»“æœ
            stage_val = flow.current_stage.value if flow.current_stage else "parse"
            stage_map = {"parse": 1, "benchmark": 2, "first_interview": 3, "final": 4}
            step = stage_map.get(stage_val, 1)
            nodes = ['æŠ•é€’', 'åŒ¹é…', 'ç­›é€‰', 'ç»“æœ']
            # äººæ‰æ–¹çš„ lastAction
            if frontend_status == "completed":
                last_action = "ç­›é€‰é€šè¿‡ï¼Œè”ç³»æ–¹å¼å·²äº’æ¢"
            elif frontend_status == "rejected":
                last_action = "æœªé€šè¿‡ç­›é€‰"
            elif frontend_status == "screening":
                last_action = "ä¼ä¸šæ­£åœ¨ç­›é€‰ä¸­"
            else:
                last_action = flow.last_action or "AI å·²æŠ•é€’ï¼Œç­‰å¾…ä¼ä¸šå›å¤"
            queue_type = "delivery"  # æ™ºèƒ½æŠ•é€’
        else:
            # ä¼ä¸šæ–¹è§†è§’ï¼šé‚€è¯· â†’ åŒ¹é… â†’ ç­›é€‰ â†’ ç»“æœ
            stage_val = flow.current_stage.value if flow.current_stage else "parse"
            stage_map = {"parse": 1, "benchmark": 2, "first_interview": 3, "final": 4}
            step = stage_map.get(stage_val, 1)
            nodes = ['é‚€è¯·', 'åŒ¹é…', 'ç­›é€‰', 'ç»“æœ']
            if frontend_status == "completed":
                last_action = "ç­›é€‰é€šè¿‡ï¼Œè”ç³»æ–¹å¼å·²äº’æ¢"
            elif frontend_status == "rejected":
                last_action = flow.last_action or "æ™ºèƒ½ç­›é€‰ - æœªé€šè¿‡"
            elif frontend_status == "screening":
                last_action = "æ™ºèƒ½ç­›é€‰è¯„ä¼°ä¸­"
            else:
                last_action = flow.last_action or f"æ™ºèƒ½é‚€è¯·åŒ¹é…ï¼ˆåŒ¹é…åº¦ {flow.match_score or 0:.0f}%ï¼‰"
            queue_type = "recruit"  # æ™ºèƒ½æ‹›è˜

        flow_list.append({
            "id": flow.id,
            "candidateName": candidate.profile.display_name if candidate and candidate.profile else "æœªçŸ¥",
            "candidateId": flow.candidate_id,
            "role": job.title if job else "æœªçŸ¥èŒä½",
            "company": job.company if job else "æœªçŸ¥å…¬å¸",
            "jobId": flow.job_id,
            "stage": stage_val,
            "status": frontend_status,
            "matchScore": flow.match_score or 0,
            "currentStep": step,
            "totalSteps": 4,
            "nodes": nodes,
            "lastAction": last_action,
            "queueType": queue_type,
            "salary": _format_salary(job),
            "tokensConsumed": flow.tokens_consumed,
            "agentsUsed": flow.agents_used or [],
            "timeline": [{
                "action": t.action,
                "agent": t.agent_name,
                "time": t.timestamp.isoformat() if t.timestamp else None,
            } for t in (flow.timeline or [])[:5]],
            "updatedAt": flow.updated_at.isoformat() if flow.updated_at else None,
        })

    return flow_list


@router.get("/flows/{flow_id}")
async def get_public_flow(
    flow_id: int,
    db: AsyncSession = Depends(get_db)
):
    """è·å–å·¥ä½œæµè¯¦æƒ…"""
    result = await db.execute(
        select(Flow)
        .options(selectinload(Flow.steps), selectinload(Flow.timeline))
        .where(Flow.id == flow_id)
    )
    flow = result.scalar_one_or_none()
    
    if not flow:
        return {"error": "æµç¨‹ä¸å­˜åœ¨"}
    
    # è·å–èŒä½ä¿¡æ¯
    job_result = await db.execute(select(Job).where(Job.id == flow.job_id))
    job = job_result.scalar_one_or_none()
    
    # è·å–å€™é€‰äººä¿¡æ¯
    candidate_result = await db.execute(
        select(Candidate)
        .options(selectinload(Candidate.profile))
        .where(Candidate.id == flow.candidate_id)
    )
    candidate = candidate_result.scalar_one_or_none()
    
    return {
        "id": flow.id,
        "candidateName": candidate.profile.display_name if candidate and candidate.profile else "æœªçŸ¥",
        "role": job.title if job else "æœªçŸ¥èŒä½",
        "company": job.company if job else "æœªçŸ¥å…¬å¸",
        "stage": flow.current_stage.value,
        "status": flow.status.value,
        "matchScore": flow.match_score or 0,
        "currentStep": flow.current_step,
        "totalSteps": 5,
        "tokensConsumed": flow.tokens_consumed,
        "agentsUsed": flow.agents_used or [],
        "steps": [{
            "name": s.name,
            "stage": s.stage.value,
            "isCompleted": s.is_completed,
            "completedAt": s.completed_at.isoformat() if s.completed_at else None,
        } for s in sorted(flow.steps or [], key=lambda x: x.order)],
        "timeline": [{
            "action": t.action,
            "agent": t.agent_name,
            "tokens": t.tokens_used,
            "time": t.timestamp.isoformat() if t.timestamp else None,
        } for t in (flow.timeline or [])],
    }


# ============ å€™é€‰äºº/äººæ‰ç›¸å…³å…¬å¼€æ¥å£ ============

@router.get("/talents")
async def get_public_talents(
    page: int = Query(1, ge=1),
    page_size: int = Query(20, ge=1, le=100),
    search: Optional[str] = Query(None, description="æœç´¢å§“å/èŒä½/æŠ€èƒ½"),
    skill: Optional[str] = Query(None, description="æŠ€èƒ½ç­›é€‰"),
    experience_min: Optional[int] = Query(None, description="æœ€ä½ç»éªŒå¹´é™"),
    experience_max: Optional[int] = Query(None, description="æœ€é«˜ç»éªŒå¹´é™"),
    sort: Optional[str] = Query("match", description="æ’åº: match/newest/experience"),
    limit: int = Query(None, ge=1, le=50, description="ç®€å• limitï¼ˆå…¼å®¹æ—§è°ƒç”¨ï¼‰"),
    db: AsyncSession = Depends(get_db)
):
    """è·å–æ¨èäººæ‰åˆ—è¡¨ï¼ˆæ”¯æŒæœç´¢ã€æŠ€èƒ½ç­›é€‰ã€ç»éªŒç­›é€‰ã€åˆ†é¡µï¼‰"""
    from sqlalchemy import or_
    from app.models.candidate import Skill as SkillModel

    query = (
        select(Candidate)
        .options(selectinload(Candidate.profile), selectinload(Candidate.skills))
        .where(Candidate.is_profile_complete == True)
    )

    # æœç´¢ï¼šå§“å/èŒä½/ç®€ä»‹
    if search:
        query = query.join(Candidate.profile).where(
            or_(
                CandidateProfile.display_name.ilike(f"%{search}%"),
                CandidateProfile.current_role.ilike(f"%{search}%"),
                CandidateProfile.summary.ilike(f"%{search}%"),
            )
        )

    # æŠ€èƒ½ç­›é€‰
    if skill:
        query = query.join(Candidate.skills).where(SkillModel.name.ilike(f"%{skill}%"))

    # ç»éªŒç­›é€‰
    if experience_min is not None or experience_max is not None:
        if not search:
            query = query.join(Candidate.profile)
        if experience_min is not None:
            query = query.where(CandidateProfile.experience_years >= experience_min)
        if experience_max is not None:
            query = query.where(CandidateProfile.experience_years <= experience_max)

    # å…¼å®¹æ—§çš„ç®€å• limit è°ƒç”¨
    if limit and page == 1 and page_size == 20:
        result = await db.execute(query.limit(limit))
        candidates = result.scalars().unique().all()
        return [_format_talent(c) for c in candidates]

    # è®¡æ•°
    count_q = select(func.count()).select_from(query.subquery())
    total = (await db.execute(count_q)).scalar() or 0

    # æ’åº
    if sort == "newest":
        query = query.order_by(Candidate.created_at.desc())
    elif sort == "experience":
        if not search and experience_min is None and experience_max is None:
            query = query.join(Candidate.profile)
        query = query.order_by(CandidateProfile.experience_years.desc())
    else:
        query = query.order_by(Candidate.id.desc())  # match é»˜è®¤

    offset = (page - 1) * page_size
    result = await db.execute(query.offset(offset).limit(page_size))
    candidates = result.scalars().unique().all()

    return {
        "items": [_format_talent(c) for c in candidates],
        "total": total,
        "page": page,
        "page_size": page_size,
        "pages": (total + page_size - 1) // page_size if total > 0 else 0,
    }


def _format_talent(c):
    """æ ¼å¼åŒ–å•ä¸ªå€™é€‰äººæ•°æ®"""
    p = c.profile
    return {
        "id": c.id,
        "user_id": c.user_id,
        "name": p.display_name if p else "æœªçŸ¥",
        "role": p.current_role if p else "æœªçŸ¥èŒä½",
        "experienceYears": p.experience_years if p else 0,
        "skills": [s.name for s in (c.skills or [])][:6],
        "summary": (p.summary or "")[:120] if p else "",
        "salary_range": p.salary_range if p else None,
        "matchScore": 85 + (c.id % 15),
        "status": "active",
        "created_at": c.created_at.isoformat() if c.created_at else None,
    }


@router.get("/talent-skills")
async def get_all_talent_skills(db: AsyncSession = Depends(get_db)):
    """è·å–æ‰€æœ‰å€™é€‰äººæŠ€èƒ½æ ‡ç­¾ï¼ˆå»é‡ï¼‰"""
    from app.models.candidate import Skill as SkillModel
    result = await db.execute(
        select(SkillModel.name).distinct().order_by(SkillModel.name)
    )
    return [row[0] for row in result.fetchall()]


@router.get("/talents/{candidate_id}")
async def get_talent_detail(
    candidate_id: int,
    db: AsyncSession = Depends(get_db)
):
    """è·å–å€™é€‰äººå®Œæ•´ç”»åƒ â€” ä¾›äººæ‰è¯¦æƒ…é¡µå’Œä¸ªäººä¸»é¡µä½¿ç”¨"""
    from app.models.user import User
    from app.models.profile import UserProfile, ProfileType
    
    result = await db.execute(
        select(Candidate)
        .options(selectinload(Candidate.profile), selectinload(Candidate.skills))
        .where(Candidate.id == candidate_id)
    )
    candidate = result.scalar_one_or_none()
    if not candidate:
        raise HTTPException(status_code=404, detail="å€™é€‰äººä¸å­˜åœ¨")
    
    profile = candidate.profile
    user_result = await db.execute(select(User).where(User.id == candidate.user_id))
    user_info = user_result.scalar_one_or_none()
    
    # æŸ¥è¯¢ UserProfile è·å–æ›´ä¸°å¯Œçš„ candidate_dataï¼ˆæ•™è‚²/å·¥ä½œ/é¡¹ç›®ç­‰ï¼‰
    up_result = await db.execute(
        select(UserProfile)
        .where(UserProfile.user_id == candidate.user_id, UserProfile.profile_type == ProfileType.CANDIDATE)
    )
    user_profile = up_result.scalar_one_or_none()
    candidate_data = {}
    if user_profile and user_profile.candidate_data:
        cd = user_profile.candidate_data
        if isinstance(cd, dict):
            candidate_data = cd
        elif isinstance(cd, str):
            try:
                candidate_data = json.loads(cd)
            except (json.JSONDecodeError, TypeError):
                candidate_data = {}
    
    # æ„å»ºé›·è¾¾å›¾æ•°æ®ï¼ˆè½¬ä¸ºå‰ç«¯æ ¼å¼ï¼‰
    radar_raw = profile.radar_data if profile else {}
    radar_data = []
    if isinstance(radar_raw, dict):
        for k, v in radar_raw.items():
            radar_data.append({"subject": k, "value": v})
    elif isinstance(radar_raw, list):
        radar_data = radar_raw
    
    return {
        "id": str(candidate.id),
        "name": profile.display_name if profile else (user_info.name if user_info else "æœªçŸ¥"),
        "role": profile.current_role if profile else "æœªçŸ¥èŒä½",
        "experienceYears": profile.experience_years if profile else 0,
        "skills": [s.name for s in (candidate.skills or [])] or candidate_data.get("skills", []),
        "radarData": radar_data or candidate_data.get("radar_data", []),
        "summary": profile.summary if profile else candidate_data.get("summary", ""),
        "idealJobPersona": profile.ideal_job_persona if profile else candidate_data.get("ideal_job", ""),
        "salaryRange": profile.salary_range if profile else candidate_data.get("expected_salary", ""),
        "marketDemand": profile.market_demand if profile else "",
        "matchScore": 80 + (candidate.id % 20),
        "status": "active",
        "tokensConsumed": 0,
        "interviewQuestions": profile.interview_questions if profile else [],
        "optimizationSuggestions": profile.optimization_suggestions if profile else [],
        "agentFeedbacks": profile.agent_feedbacks if profile else [],
        "certifications": profile.certifications if profile else [],
        "awards": profile.awards if profile else [],
        "credentials": profile.credentials if profile else [],
        "email": user_info.email if user_info else None,
        "phone": user_info.phone if user_info else None,
        "wechat": user_info.phone if user_info else None,
        "avatarUrl": user_info.avatar_url if user_info else None,
        # ä¸ªäººä¸»é¡µé¢å¤–å­—æ®µ
        "education": candidate_data.get("education", []),
        "experience": candidate_data.get("experience", []),
        "projects": candidate_data.get("projects", []),
        "expectedSalary": candidate_data.get("expected_salary", ""),
        "expectedLocation": candidate_data.get("expected_location", ""),
        "careerPath": candidate_data.get("career_path", []),
    }


# ============ ç»Ÿè®¡ç›¸å…³å…¬å¼€æ¥å£ ============

@router.get("/stats/token-usage")
async def get_token_usage_stats(
    db: AsyncSession = Depends(get_db)
):
    """è·å– Token ä½¿ç”¨ç»Ÿè®¡"""
    # æ¨¡æ‹Ÿç»Ÿè®¡æ•°æ®
    return {
        "balance": 1245000,
        "used_today": 12500,
        "used_this_week": 85000,
        "used_this_month": 320000,
        "history": [
            {"date": (datetime.now() - timedelta(days=i)).strftime("%Y-%m-%d"), 
             "amount": 8000 + (i * 500) % 5000, 
             "action": ["ç®€å†è§£æ", "èŒä½åŒ¹é…", "é¢è¯•æ¨¡æ‹Ÿ", "å¸‚åœºåˆ†æ"][i % 4]}
            for i in range(7)
        ],
        "chart": [
            {"name": f"{i+1}æœˆ", "tokens": 20000 + (i * 5000) % 30000}
            for i in range(6)
        ],
    }


@router.get("/stats/qualifications")
async def get_qualifications_stats(
    db: AsyncSession = Depends(get_db)
):
    """è·å–èµ„è´¨è®¤è¯ç»Ÿè®¡"""
    return [
        {"name": "AI æ‹›è˜èµ„æ ¼è®¤è¯", "status": "å·²è®¤è¯", "icon": "ğŸ†"},
        {"name": "æ•°æ®å®‰å…¨åˆè§„", "status": "é€šè¿‡", "icon": "ğŸ”"},
        {"name": "GDPR åˆè§„", "status": "å·²è®¤è¯", "icon": "ğŸŒ"},
    ]


# ============ è®°å¿†/ä»»åŠ¡ç›¸å…³å…¬å¼€æ¥å£ ============

from app.models.memory import Memory, MemoryType, MemoryImportance, MemoryScope
from pydantic import BaseModel

class MemoryCreate(BaseModel):
    type: str
    content: str
    importance: str = "Medium"
    scope: str = "candidate"  # candidate æˆ– employer

# ç±»å‹åˆ°é¢œè‰²çš„æ˜ å°„
TYPE_COLOR_MAP = {
    "culture": "border-rose-300",
    "tech": "border-indigo-300",
    "skill": "border-emerald-300",
    "experience": "border-amber-300",
    "salary": "border-green-300",
    "location": "border-sky-300",
    "reporting": "border-violet-300",
    "team": "border-teal-300",
    "project": "border-amber-300",
    "goal": "border-rose-300",
    "preference": "border-purple-300",
    "company": "border-blue-300",
    "requirement": "border-orange-300",
    "benefit": "border-cyan-300",
    "action": "border-violet-300",
    "strategy": "border-fuchsia-300",
}

# ç±»å‹åç§°æ˜ å°„
TYPE_NAME_MAP = {
    "culture": "æ–‡åŒ–",
    "tech": "æŠ€æœ¯",
    "skill": "æŠ€èƒ½",
    "experience": "ç»éªŒ",
    "salary": "è–ªé…¬",
    "location": "åœ°ç‚¹",
    "reporting": "æ±‡æŠ¥",
    "team": "å›¢é˜Ÿ",
    "project": "é¡¹ç›®",
    "goal": "ç›®æ ‡",
    "preference": "åå¥½",
    "company": "å…¬å¸",
    "requirement": "è¦æ±‚",
    "benefit": "ç¦åˆ©",
    "action": "åŠ¨ä½œ",
    "strategy": "ç­–ç•¥",
}

# æ ¹æ®è®°å¿†ç±»å‹å’Œæ¥æºç”Ÿæˆ Agent æ¨ç†é€»è¾‘
def _generate_reasoning(mem_type: str, scope: str) -> str:
    """æ ¹æ®è®°å¿†ç±»å‹è‡ªåŠ¨ç”Ÿæˆä¸åˆ†ç±»åŒ¹é…çš„ Agent æ¨ç†é€»è¾‘"""
    if scope == "employer":
        reasoning_map = {
            "requirement": "åŸºäºç”¨æˆ·åœ¨å¯¹è¯ä¸­æ˜ç¡®æå‡ºçš„æ‹›è˜è¦æ±‚å›ºåŒ–ï¼Œåç»­ç”Ÿæˆå²—ä½æè¿°ã€ç­›é€‰å€™é€‰äººæ—¶å°†è‡ªåŠ¨éµå¾ªæ­¤è§„åˆ™ã€‚",
            "culture": "ä»ç”¨æˆ·æè¿°ä¸­æå–çš„ä¼ä¸šæ–‡åŒ–ä¸å·¥ä½œæ–¹å¼åå¥½ï¼Œå°†å½±å“å²—ä½æè¿°ä¸­çš„å›¢é˜Ÿæ°›å›´å’Œå·¥ä½œç¯å¢ƒéƒ¨åˆ†ã€‚",
            "tech": "ç”¨æˆ·æŒ‡å®šçš„æŠ€æœ¯æ ˆåå¥½æˆ–æŠ€æœ¯è¦æ±‚ï¼Œåç»­å²—ä½ç”Ÿæˆå’Œå€™é€‰äººåŒ¹é…æ—¶å°†ä¼˜å…ˆåŒ¹é…æ­¤æŠ€æœ¯æ–¹å‘ã€‚",
            "strategy": "ä»ç”¨æˆ·æ“ä½œä¸­æç‚¼çš„æ‹›è˜ç­–ç•¥ä¸é¢è¯•æ–¹æ³•è®ºï¼Œå°†æŒ‡å¯¼ Agent åç»­çš„æ‹›è˜æµç¨‹å’Œå€™é€‰äººè¯„ä¼°æ–¹å¼ã€‚",
            "benefit": "ç”¨æˆ·è®¾å®šçš„ç¦åˆ©å¾…é‡æ ‡å‡†ï¼Œç”Ÿæˆå²—ä½æ—¶å°†è‡ªåŠ¨é™„å¸¦æ­¤ç¦åˆ©ä¿¡æ¯ï¼Œæå‡å²—ä½å¸å¼•åŠ›ã€‚",
            "action": "åŸºäºç”¨æˆ·æ“ä½œè¡Œä¸ºå’ŒæŒ‡ä»¤è®°å½•çš„åŠ¨ä½œåå¥½ï¼ŒAgent åç»­æ‰§è¡Œç›¸ä¼¼ä»»åŠ¡æ—¶å°†å‚è€ƒæ­¤æ¨¡å¼ã€‚",
            "preference": "ç”¨æˆ·çš„é€šç”¨æ‹›è˜åå¥½ï¼ˆå¦‚å­¦å†ã€ç»éªŒç­‰ï¼‰ï¼Œè´¯ç©¿æ‰€æœ‰å²—ä½ç”Ÿæˆå’Œå€™é€‰äººç­›é€‰ç¯èŠ‚ã€‚",
            "experience": "ç”¨æˆ·å¯¹å€™é€‰äººç»éªŒçš„è¦æ±‚æ ‡å‡†ï¼ŒåŒ¹é…å€™é€‰äººæ—¶å°†ä»¥æ­¤ä½œä¸ºæ ¸å¿ƒç­›é€‰ç»´åº¦ã€‚",
            "salary": "ç”¨æˆ·è®¾å®šçš„è–ªé…¬èŒƒå›´çº¦æŸï¼Œæ‰€æœ‰å²—ä½è–ªèµ„å°†ä¸¥æ ¼éµå¾ªæ­¤åŒºé—´ã€‚",
            "company": "ä¼ä¸šåŸºç¡€ä¿¡æ¯ä¸è¡Œä¸šå®šä½ï¼Œç”¨äºå²—ä½æè¿°ä¸­çš„å…¬å¸ä»‹ç»å’Œè¡Œä¸šæ ‡ç­¾ç”Ÿæˆã€‚",
        }
    else:
        reasoning_map = {
            "skill": "ä»ç”¨æˆ·ç®€å†æˆ–å¯¹è¯ä¸­è¯†åˆ«çš„æ ¸å¿ƒæŠ€èƒ½ï¼Œå°†ç”¨äºå²—ä½åŒ¹é…æ—¶çš„èƒ½åŠ›è¯„ä¼°å’Œæ¨èæ’åºã€‚",
            "experience": "ç”¨æˆ·çš„å·¥ä½œç»å†è®°å½•ï¼ŒåŒ¹é…å²—ä½æ—¶å°†å¯¹ç…§ç»éªŒå¹´é™å’Œè¡Œä¸šèƒŒæ™¯è¿›è¡Œç²¾å‡†æ¨èã€‚",
            "preference": "ç”¨æˆ·è¡¨è¾¾çš„æ±‚èŒåå¥½ï¼ˆå¦‚å…¬å¸ç±»å‹ã€å›¢é˜Ÿè§„æ¨¡ç­‰ï¼‰ï¼Œè¿‡æ»¤æ¨èå²—ä½æ—¶ä¼˜å…ˆæ»¡è¶³è¿™äº›æ¡ä»¶ã€‚",
            "goal": "ç”¨æˆ·çš„èŒä¸šå‘å±•ç›®æ ‡ï¼Œæ¨èå²—ä½æ—¶å°†è€ƒè™‘å²—ä½çš„æˆé•¿ç©ºé—´æ˜¯å¦åŒ¹é…ç”¨æˆ·é•¿æœŸè§„åˆ’ã€‚",
            "salary": "ç”¨æˆ·çš„è–ªé…¬æœŸæœ›åŒºé—´ï¼Œæ¨èå²—ä½æ—¶è‡ªåŠ¨è¿‡æ»¤è–ªèµ„ä¸åŒ¹é…çš„æœºä¼šã€‚",
            "location": "ç”¨æˆ·çš„å·¥ä½œåœ°ç‚¹åå¥½ï¼Œæ¨èå²—ä½æ—¶ä¼˜å…ˆæ¨èç¬¦åˆåœ°ç‚¹è¦æ±‚çš„æœºä¼šã€‚",
            "tech": "ç”¨æˆ·æŒæ¡çš„æŠ€æœ¯æ ˆï¼ŒåŒ¹é…å²—ä½æ—¶å°†ä»¥æŠ€æœ¯å¥‘åˆåº¦ä½œä¸ºæ ¸å¿ƒæ¨èä¾æ®ã€‚",
            "culture": "ç”¨æˆ·åå¥½çš„å·¥ä½œæ–‡åŒ–ä¸å›¢é˜Ÿæ°›å›´ï¼Œæ¨èæ—¶å€¾å‘åŒ¹é…æ–‡åŒ–å¥‘åˆçš„ä¼ä¸šã€‚",
            "requirement": "ç”¨æˆ·å¯¹å²—ä½çš„ç¡¬æ€§è¦æ±‚ï¼Œæ¨èæ—¶å°†æ­¤ä½œä¸ºå¿…è¦ç­›é€‰æ¡ä»¶ä¸¥æ ¼æ‰§è¡Œã€‚",
            "action": "åŸºäºç”¨æˆ·æ“ä½œè¡Œä¸ºè®°å½•çš„åå¥½ï¼ŒAgent åç»­ä»»åŠ¡å°†å‚è€ƒæ­¤æ¨¡å¼è‡ªåŠ¨æ‰§è¡Œã€‚",
        }
    return reasoning_map.get(mem_type, f"Agent è‡ªåŠ¨è®°å½•çš„{TYPE_NAME_MAP.get(mem_type, mem_type)}ä¿¡æ¯ï¼Œç”¨äºä¼˜åŒ–åç»­åŒ¹é…ã€‚")


@router.get("/memories")
async def get_memories(
    user_id: int = Query(1, description="ç”¨æˆ·ID"),
    scope: str = Query("candidate", description="è®°å¿†èŒƒå›´: candidate(äººæ‰ç”»åƒ) æˆ– employer(ä¼ä¸šç”»åƒ)"),
    db: AsyncSession = Depends(get_db)
):
    """è·å–ç”¨æˆ·è®°å¿† - åŒºåˆ†äººæ‰ç”»åƒå’Œä¼ä¸šç”»åƒ"""
    # è§£æ scope
    try:
        memory_scope = MemoryScope(scope.lower())
    except ValueError:
        memory_scope = MemoryScope.CANDIDATE
    
    result = await db.execute(
        select(Memory)
        .where(Memory.user_id == user_id)
        .where(Memory.scope == memory_scope)
        .order_by(Memory.created_at.desc())
    )
    memories = result.scalars().all()
    
    # å¦‚æœæ²¡æœ‰è®°å¿†ï¼Œè¿”å›ç©ºæ•°ç»„ï¼ˆè®©ç”¨æˆ·è‡ªå·±æ·»åŠ æ•°æ®ï¼‰
    if not memories:
        return []
    
    return [{
        "id": m.id,
        "type": TYPE_NAME_MAP.get(m.type.value, m.type.value).upper(),
        "raw_type": m.type.value,
        "content": m.content,
        "date": m.created_at.strftime("%Y-%m"),
        "color": m.color or TYPE_COLOR_MAP.get(m.type.value, "border-slate-300"),
        "importance": m.importance.value if m.importance else "Medium",
        "scope": m.scope.value if m.scope else "candidate",
        "emphasis_count": m.emphasis_count or 1,
        "ai_reasoning": m.ai_reasoning,
        "version_history": m.version_history or [],
    } for m in memories]


@router.post("/memories")
async def create_memory(
    memory: MemoryCreate,
    user_id: int = Query(1, description="ç”¨æˆ·ID"),
    force_create: bool = Query(False, description="æ˜¯å¦å¼ºåˆ¶åˆ›å»ºï¼ˆå¿½ç•¥é‡å¤æ£€æŸ¥ï¼‰"),
    db: AsyncSession = Depends(get_db)
):
    """åˆ›å»ºæ–°è®°å¿† - è‡ªåŠ¨æ£€æŸ¥é‡å¤å†…å®¹ï¼Œé‡å¤åˆ™å¢åŠ å¼ºè°ƒæ¬¡æ•°"""
    # éªŒè¯ç±»å‹
    try:
        memory_type = MemoryType(memory.type.lower())
    except ValueError:
        memory_type = MemoryType.SKILL  # é»˜è®¤ä¸ºæŠ€èƒ½ç±»å‹
    
    # éªŒè¯é‡è¦æ€§
    try:
        importance = MemoryImportance(memory.importance)
    except ValueError:
        importance = MemoryImportance.MEDIUM
    
    # éªŒè¯ scope
    try:
        memory_scope = MemoryScope(memory.scope.lower())
    except ValueError:
        memory_scope = MemoryScope.CANDIDATE
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸ä¼¼çš„è®°å¿†ï¼ˆåŒç±»å‹ã€åŒèŒƒå›´ã€å†…å®¹ç›¸ä¼¼ï¼‰
    if not force_create:
        result = await db.execute(
            select(Memory)
            .where(Memory.user_id == user_id)
            .where(Memory.type == memory_type)
            .where(Memory.scope == memory_scope)
        )
        existing_memories = result.scalars().all()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å†…å®¹ç›¸åŒæˆ–é«˜åº¦ç›¸ä¼¼çš„è®°å¿†
        content_lower = memory.content.lower().strip()
        for existing in existing_memories:
            existing_content = existing.content.lower().strip()
            # å®Œå…¨ç›¸åŒæˆ–å†…å®¹åŒ…å«å…³ç³»
            if (content_lower == existing_content or 
                content_lower in existing_content or 
                existing_content in content_lower):
                # å¢åŠ å¼ºè°ƒæ¬¡æ•°è€Œä¸æ˜¯åˆ›å»ºæ–°è®°å¿†
                existing.emphasis_count = (existing.emphasis_count or 1) + 1
                existing.updated_at = datetime.utcnow()
                # å¦‚æœæ–°å†…å®¹æ›´é•¿æ›´è¯¦ç»†ï¼Œæ›´æ–°å†…å®¹
                if len(memory.content) > len(existing.content):
                    existing.content = memory.content
                # æ›´æ–°æ¨ç†é€»è¾‘ï¼Œæ ‡æ³¨è¢«å¤šæ¬¡å¼ºè°ƒ
                base_reasoning = _generate_reasoning(memory_type.value, memory_scope.value)
                existing.ai_reasoning = f"ç”¨æˆ·å·²åå¤å¼ºè°ƒ {existing.emphasis_count} æ¬¡ã€‚{base_reasoning}"
                await db.commit()
                await db.refresh(existing)
                
                return {
                    "id": existing.id,
                    "type": TYPE_NAME_MAP.get(memory_type.value, memory_type.value).upper(),
                    "content": existing.content,
                    "date": existing.updated_at.strftime("%Y-%m"),
                    "color": existing.color,
                    "importance": existing.importance.value,
                    "emphasis_count": existing.emphasis_count,
                    "message": f"è®°å¿†å·²å¼ºè°ƒ {existing.emphasis_count} æ¬¡",
                    "is_duplicate": True
                }
    
    # è·å–é¢œè‰²
    color = TYPE_COLOR_MAP.get(memory_type.value, "border-slate-300")
    
    # æ ¹æ®ç±»å‹å’Œæ¥æºè‡ªåŠ¨ç”Ÿæˆ ai_reasoning
    reasoning = _generate_reasoning(memory_type.value, memory_scope.value)
    
    # åˆ›å»ºæ–°è®°å¿†
    new_memory = Memory(
        user_id=user_id,
        type=memory_type,
        content=memory.content,
        importance=importance,
        scope=memory_scope,
        color=color,
        source="manual",
        emphasis_count=1,
        ai_reasoning=reasoning,
    )
    
    db.add(new_memory)
    await db.commit()
    await db.refresh(new_memory)
    
    return {
        "id": new_memory.id,
        "type": TYPE_NAME_MAP.get(memory_type.value, memory_type.value).upper(),
        "content": new_memory.content,
        "date": new_memory.created_at.strftime("%Y-%m"),
        "color": new_memory.color,
        "importance": new_memory.importance.value,
        "emphasis_count": 1,
        "message": "è®°å¿†åˆ›å»ºæˆåŠŸ",
        "is_duplicate": False
    }


@router.put("/memories/{memory_id}")
async def update_memory(
    memory_id: int,
    memory: MemoryCreate,
    db: AsyncSession = Depends(get_db)
):
    """æ›´æ–°è®°å¿†"""
    result = await db.execute(select(Memory).where(Memory.id == memory_id))
    existing = result.scalar_one_or_none()
    
    if not existing:
        return {"error": "è®°å¿†ä¸å­˜åœ¨"}
    
    try:
        existing.type = MemoryType(memory.type.lower())
    except ValueError:
        existing.type = MemoryType.PREFERENCE
    
    existing.content = memory.content
    
    try:
        existing.importance = MemoryImportance(memory.importance)
    except ValueError:
        existing.importance = MemoryImportance.MEDIUM
    
    existing.color = TYPE_COLOR_MAP.get(memory.type.lower(), "border-slate-300")
    existing.ai_reasoning = _generate_reasoning(memory.type.lower(), existing.scope.value if existing.scope else "candidate")
    
    await db.commit()
    await db.refresh(existing)
    
    return {
        "id": existing.id,
        "type": TYPE_NAME_MAP.get(existing.type.value, existing.type.value),
        "content": existing.content,
        "importance": existing.importance.value,
        "color": existing.color,
        "message": "è®°å¿†æ›´æ–°æˆåŠŸ"
    }


@router.delete("/memories/{memory_id}")
async def delete_memory(
    memory_id: int,
    db: AsyncSession = Depends(get_db)
):
    """åˆ é™¤è®°å¿†"""
    result = await db.execute(select(Memory).where(Memory.id == memory_id))
    memory = result.scalar_one_or_none()
    
    if not memory:
        return {"error": "è®°å¿†ä¸å­˜åœ¨"}
    
    await db.delete(memory)
    await db.commit()
    
    return {"message": "è®°å¿†å·²åˆ é™¤"}


@router.post("/memories/optimize")
async def optimize_memories(
    user_id: int = Query(..., description="ç”¨æˆ·ID"),
    scope: str = Query("candidate", description="è®°å¿†èŒƒå›´: candidate æˆ– employer"),
    db: AsyncSession = Depends(get_db)
):
    """AI é©±åŠ¨çš„è®°å¿†ä¼˜åŒ– â€” åˆå¹¶åŒç±»ä¸åˆ é™¤ï¼Œä¿ç•™ç‰ˆæœ¬å†å²
    
    1. åŒç±»å‹ç›¸ä¼¼è®°å¿†åˆå¹¶å†…å®¹ï¼ˆä¸åˆ é™¤ï¼Œåˆå¹¶åˆ°ä¸€æ¡ä¸­ï¼‰ï¼Œè®°å½•ç‰ˆæœ¬å†å²
    2. åªåˆ é™¤å®Œå…¨é‡å¤çš„è®°å¿†
    3. æ£€æŸ¥è®°å¿†åˆ†ç±»çš„å‡†ç¡®æ€§å¹¶ä¿®æ­£
    4. ä¸ºæ‰€æœ‰è®°å¿†æ›´æ–° Agent æ¨ç†é€»è¾‘
    """
    import json as json_module
    from datetime import datetime
    
    try:
        memory_scope = MemoryScope(scope.lower())
    except ValueError:
        memory_scope = MemoryScope.CANDIDATE
    
    # è·å–ç”¨æˆ·æ‰€æœ‰è®°å¿†
    result = await db.execute(
        select(Memory)
        .where(Memory.user_id == user_id)
        .where(Memory.scope == memory_scope)
        .order_by(Memory.created_at.desc())
    )
    memories = result.scalars().all()
    
    if not memories:
        return {
            "success": True,
            "message": "æ²¡æœ‰éœ€è¦ä¼˜åŒ–çš„è®°å¿†",
            "actions": [],
            "summary": {"merged": 0, "deleted": 0, "reclassified": 0, "created": 0, "reasoning_updated": 0}
        }
    
    # ========== æœ¬åœ°æ™ºèƒ½åˆå¹¶ï¼ˆä¸ä¾èµ– AIï¼‰ ==========
    actions = []
    summary = {"merged": 0, "deleted": 0, "reclassified": 0, "created": 0, "reasoning_updated": 0}
    
    # æŒ‰ç±»å‹åˆ†ç»„
    type_groups: dict = {}
    for m in memories:
        type_key = m.type.value if m.type else 'unknown'
        if type_key not in type_groups:
            type_groups[type_key] = []
        type_groups[type_key].append(m)
    
    processed_ids = set()
    
    # å¯¹æ¯ä¸ªç±»å‹ç»„è¿›è¡Œåˆå¹¶
    for type_key, group in type_groups.items():
        if len(group) < 2:
            continue
        
        # æŒ‰æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰
        group.sort(key=lambda x: x.created_at, reverse=True)
        
        # èšç±»ï¼šæ‰¾å‡ºå†…å®¹ç›¸ä¼¼çš„è®°å¿†å­ç»„
        clusters: list = []
        used = set()
        
        for i, m1 in enumerate(group):
            if m1.id in used:
                continue
            cluster = [m1]
            used.add(m1.id)
            c1 = m1.content.lower().strip()
            
            for j in range(i + 1, len(group)):
                m2 = group[j]
                if m2.id in used:
                    continue
                c2 = m2.content.lower().strip()
                
                # ç›¸ä¼¼åº¦åˆ¤æ–­ï¼šå®Œå…¨åŒ…å« æˆ– å…±åŒå…³é”®è¯ >= 50%
                is_similar = False
                if c1 == c2:
                    is_similar = True  # å®Œå…¨é‡å¤
                elif c1 in c2 or c2 in c1:
                    is_similar = True  # åŒ…å«å…³ç³»
                else:
                    # å…³é”®è¯äº¤é›†æ¯”è¾ƒ
                    words1 = set(c1.replace('ï¼š', ' ').replace('ï¼Œ', ' ').replace('ã€‚', ' ').split())
                    words2 = set(c2.replace('ï¼š', ' ').replace('ï¼Œ', ' ').replace('ã€‚', ' ').split())
                    if words1 and words2:
                        overlap = len(words1 & words2) / min(len(words1), len(words2))
                        if overlap >= 0.5:
                            is_similar = True
                
                if is_similar:
                    cluster.append(m2)
                    used.add(m2.id)
            
            if len(cluster) >= 2:
                clusters.append(cluster)
        
        # å¤„ç†æ¯ä¸ªèšç±»
        for cluster in clusters:
            # ä¿ç•™æœ€æ–°çš„ä¸€æ¡ä½œä¸ºä¸»è®°å¿†
            primary = cluster[0]  # å·²æŒ‰æ—¶é—´æ’åºï¼Œæœ€æ–°åœ¨å‰
            others = cluster[1:]
            
            # æ£€æŸ¥æ˜¯å¦å®Œå…¨é‡å¤
            primary_content = primary.content.strip()
            all_exact_dup = all(o.content.strip() == primary_content for o in others)
            
            if all_exact_dup:
                # å®Œå…¨é‡å¤ï¼šåˆ é™¤æ—§çš„ï¼Œå¢åŠ å¼ºåº¦
                for o in others:
                    # è®°å½•ç‰ˆæœ¬å†å²
                    history = list(primary.version_history or [])
                    history.append({
                        "version": len(history) + 1,
                        "action": "merge_duplicate",
                        "content": o.content,
                        "date": o.created_at.strftime("%Y-%m-%d"),
                        "source": f"åˆå¹¶å®Œå…¨é‡å¤çš„è®°å¿† (ID:{o.id})"
                    })
                    primary.version_history = history
                    
                    await db.delete(o)
                    processed_ids.add(o.id)
                
                primary.emphasis_count = (primary.emphasis_count or 1) + len(others)
                primary.updated_at = datetime.utcnow()
                primary.ai_reasoning = f"ç”¨æˆ·å·²åå¤æåŠ {primary.emphasis_count} æ¬¡ã€‚{_generate_reasoning(type_key, scope)}"
                
                actions.append({
                    "action": "merge",
                    "kept_id": primary.id,
                    "deleted_ids": [o.id for o in others],
                    "reason": f"åˆå¹¶ {len(others)} æ¡å®Œå…¨é‡å¤çš„{TYPE_NAME_MAP.get(type_key, type_key)}è®°å¿†"
                })
                summary["merged"] += len(others)
            else:
                # ç›¸ä¼¼ä½†ä¸å®Œå…¨é‡å¤ï¼šåˆå¹¶å†…å®¹ï¼Œä¸åˆ é™¤
                # å°†å…¶ä»–è®°å¿†çš„ç‹¬æœ‰å†…å®¹æ•´åˆåˆ°ä¸»è®°å¿†ä¸­
                old_content = primary.content
                
                # æå–æ¯æ¡è®°å¿†çš„æ ¸å¿ƒå†…å®¹ï¼Œä¿ç•™ç¬¬ä¸€æ¡çš„å‰ç¼€ï¼ˆå¦‚"æ‹›è˜éœ€æ±‚ï¼š"ï¼‰
                # æ£€æµ‹ä¸»è®°å¿†æ˜¯å¦æœ‰å‰ç¼€æ ‡ç­¾
                primary_text = primary.content.strip()
                prefix_label = ''
                for prefix in ['æ‹›è˜éœ€æ±‚ï¼š', 'æ‹›è˜éœ€æ±‚:', 'è¦æ±‚ï¼š', 'è¦æ±‚:', 'æŠ€æœ¯è¦æ±‚ï¼š', 'æŠ€æœ¯è¦æ±‚:']:
                    if primary_text.startswith(prefix):
                        prefix_label = prefix.rstrip('ï¼š:') + 'ï¼š'  # ç»Ÿä¸€ç”¨ä¸­æ–‡å†’å·
                        primary_text = primary_text[len(prefix):].strip()
                        break
                
                # æå–æ‰€æœ‰è®°å¿†çš„å»å‰ç¼€æ ¸å¿ƒå†…å®¹
                contents = [primary_text]
                for m in cluster[1:]:
                    c = m.content.strip()
                    for prefix in ['æ‹›è˜éœ€æ±‚ï¼š', 'æ‹›è˜éœ€æ±‚:', 'è¦æ±‚ï¼š', 'è¦æ±‚:', 'æŠ€æœ¯è¦æ±‚ï¼š', 'æŠ€æœ¯è¦æ±‚:']:
                        if c.startswith(prefix):
                            c = c[len(prefix):]
                            break
                    contents.append(c.strip())
                
                # åˆå¹¶ï¼šå»é‡åæ‹¼æ¥
                unique_parts = []
                seen = set()
                for c in contents:
                    c_key = c.lower().strip()
                    if c_key not in seen and len(c_key) > 0:
                        seen.add(c_key)
                        unique_parts.append(c)
                
                if len(unique_parts) > 1:
                    # ä¿ç•™åŸå§‹å‰ç¼€æ ‡ç­¾ï¼ˆå¦‚"æ‹›è˜éœ€æ±‚ï¼š"ï¼‰
                    merged_content = prefix_label + 'ï¼›'.join(unique_parts)
                    
                    # è®°å½•ç‰ˆæœ¬å†å²
                    history = list(primary.version_history or [])
                    for o in others:
                        history.append({
                            "version": len(history) + 1,
                            "action": "merge_similar",
                            "content": o.content,
                            "date": o.created_at.strftime("%Y-%m-%d"),
                            "source": f"åˆå¹¶ç›¸ä¼¼è®°å¿† (ID:{o.id})"
                        })
                    # è®°å½•åˆå¹¶å‰çš„å†…å®¹
                    history.append({
                        "version": len(history) + 1,
                        "action": "optimize",
                        "content": old_content,
                        "date": datetime.utcnow().strftime("%Y-%m-%d"),
                        "source": f"è®°å¿†ä¼˜åŒ–ï¼šåˆå¹¶ {len(cluster)} æ¡ç›¸ä¼¼è®°å¿†"
                    })
                    primary.version_history = history
                    
                    primary.content = merged_content
                    primary.emphasis_count = (primary.emphasis_count or 1) + len(others)
                    primary.updated_at = datetime.utcnow()
                    primary.ai_reasoning = f"åˆå¹¶äº† {len(cluster)} æ¡ç›¸ä¼¼è®°å¿†ã€‚{_generate_reasoning(type_key, scope)}"
                    
                    # åˆ é™¤è¢«åˆå¹¶çš„æ—§è®°å¿†
                    for o in others:
                        await db.delete(o)
                        processed_ids.add(o.id)
                    
                    actions.append({
                        "action": "merge",
                        "kept_id": primary.id,
                        "deleted_ids": [o.id for o in others],
                        "reason": f"åˆå¹¶ {len(cluster)} æ¡ç›¸ä¼¼çš„{TYPE_NAME_MAP.get(type_key, type_key)}è®°å¿†ï¼Œä¿ç•™å®Œæ•´ä¿¡æ¯"
                    })
                    summary["merged"] += len(others)
    
    # ä¸ºæ‰€æœ‰æœªå¤„ç†çš„è®°å¿†æ›´æ–°æ¨ç†é€»è¾‘
    for m in memories:
        if m.id not in processed_ids and not m.ai_reasoning:
            m.ai_reasoning = _generate_reasoning(m.type.value if m.type else 'preference', scope)
            m.updated_at = datetime.utcnow()
            summary["reasoning_updated"] += 1
    
    await db.commit()
    
    return {
        "success": True,
        "message": f"è®°å¿†ä¼˜åŒ–å®Œæˆï¼šåˆå¹¶ {summary['merged']} æ¡ï¼Œé‡åˆ†ç±» {summary['reclassified']} æ¡ï¼Œæ›´æ–°æ¨ç† {summary['reasoning_updated']} æ¡",
        "actions": actions,
        "summary": summary
    }


from app.models.todo import Todo, TodoStatus, TodoPriority, TodoSource, TodoType

class TodoCreate(BaseModel):
    title: str
    description: str = ""
    priority: str = "medium"
    source: str = "user"
    todo_type: str = "system"
    ai_advice: str = ""
    steps: list = []
    due_date: str = None


@router.get("/todos")
async def get_todos(
    user_id: int = Query(1, description="ç”¨æˆ·ID"),
    db: AsyncSession = Depends(get_db)
):
    """è·å–å¾…åŠä»»åŠ¡ - çº¯åŠ¨æ€æ•°æ®"""
    result = await db.execute(
        select(Todo)
        .where(Todo.user_id == user_id)
        .order_by(Todo.created_at.desc())
    )
    todos = result.scalars().all()
    
    # å¦‚æœæ²¡æœ‰ä»»åŠ¡ï¼Œè¿”å›ç©ºæ•°ç»„ï¼ˆä¸ä½¿ç”¨é™æ€æ•°æ®ï¼‰
    if not todos:
        return []
    
    return [{
        "id": str(t.id),
        "title": t.title,
        "task": t.title,  # å…¼å®¹å‰ç«¯å­—æ®µå
        "description": t.description or "",
        "status": t.status.value.upper() if t.status else "PENDING",
        "priority": t.priority.value.upper() if t.priority else "MEDIUM",
        "progress": t.progress or 0,
        "source": t.source.value.upper() if t.source else "USER",
        "icon": t.icon or "Calendar",
        "todo_type": t.todo_type.value.upper() if t.todo_type else "SYSTEM",
        "type": t.todo_type.value.upper() if t.todo_type else "SYSTEM",
        "aiAdvice": t.ai_advice or "",
        "steps": json.loads(t.steps) if isinstance(t.steps, str) else (t.steps or []),
        "dueDate": t.due_date.strftime("%Y-%m-%d") if t.due_date else None,
        "createdAt": t.created_at.strftime("%Y-%m-%d") if t.created_at else None,
        "created_at": t.created_at.isoformat() if t.created_at else None,
        "updated_at": t.updated_at.isoformat() if t.updated_at else None,
        "completed_at": t.completed_at.isoformat() if t.completed_at else None,
    } for t in todos]


@router.post("/todos")
async def create_todo(
    todo: TodoCreate,
    user_id: int = Query(1, description="ç”¨æˆ·ID"),
    db: AsyncSession = Depends(get_db)
):
    """åˆ›å»ºå¾…åŠä»»åŠ¡"""
    # è§£æä¼˜å…ˆçº§
    try:
        priority = TodoPriority(todo.priority.upper())
    except ValueError:
        priority = TodoPriority.MEDIUM
    
    # è§£ææ¥æº
    try:
        source = TodoSource(todo.source.upper())
    except ValueError:
        source = TodoSource.USER
    
    # è§£æç±»å‹
    try:
        todo_type = TodoType(todo.todo_type.upper())
    except ValueError:
        todo_type = TodoType.SYSTEM
    
    # è§£ææˆªæ­¢æ—¥æœŸ
    due_date = None
    if todo.due_date:
        try:
            due_date = datetime.strptime(todo.due_date, "%Y-%m-%d")
        except ValueError:
            pass
    
    new_todo = Todo(
        user_id=user_id,
        title=todo.title,
        description=todo.description,
        status=TodoStatus.PENDING,
        priority=priority,
        source=source,
        todo_type=todo_type,
        progress=0,
        ai_advice=todo.ai_advice,
        steps=todo.steps,
        due_date=due_date,
        icon="Calendar",
    )
    
    db.add(new_todo)
    await db.commit()
    await db.refresh(new_todo)
    
    return {
        "id": str(new_todo.id),
        "title": new_todo.title,
        "message": "ä»»åŠ¡åˆ›å»ºæˆåŠŸ"
    }


class TodoUpdateBody(BaseModel):
    """æ›´æ–°ä»»åŠ¡è¯·æ±‚ä½“ï¼ˆå¯é€‰å­—æ®µé€šè¿‡ body ä¼ é€’ï¼‰"""
    steps: Optional[list] = None
    ai_advice: Optional[str] = None
    published_job_ids: Optional[list] = None  # å…³è”çš„å·²å‘å¸ƒå²—ä½ ID åˆ—è¡¨
    current_step: Optional[str] = None  # å½“å‰æ‹›è˜é˜¶æ®µ

@router.put("/todos/{todo_id}")
async def update_todo(
    todo_id: int,
    status: str = Query(None),
    progress: int = Query(None),
    body: Optional[TodoUpdateBody] = Body(None),
    db: AsyncSession = Depends(get_db)
):
    """æ›´æ–°å¾…åŠä»»åŠ¡"""
    result = await db.execute(select(Todo).where(Todo.id == todo_id))
    todo = result.scalar_one_or_none()
    
    if not todo:
        return {"error": "ä»»åŠ¡ä¸å­˜åœ¨"}
    
    if status:
        try:
            todo.status = TodoStatus(status.upper())
            if status.upper() == "COMPLETED":
                todo.completed_at = datetime.utcnow()
                todo.progress = 100
        except ValueError:
            pass
    
    if progress is not None:
        todo.progress = progress
        if progress >= 100:
            todo.status = TodoStatus.COMPLETED
            todo.completed_at = datetime.utcnow()
    
    # æ›´æ–° body ä¸­çš„å¯é€‰å­—æ®µ
    if body:
        import json as json_mod
        if body.ai_advice is not None:
            todo.ai_advice = body.ai_advice
        
        # ç»Ÿä¸€å¤„ç† steps + metadata çš„å­˜å‚¨
        # å…ˆè§£æç°æœ‰æ•°æ®
        existing_steps = []
        metadata = {}
        if todo.steps:
            try:
                parsed = json_mod.loads(todo.steps) if isinstance(todo.steps, str) else todo.steps
                if isinstance(parsed, dict):
                    existing_steps = parsed.get('steps', [])
                    metadata = parsed.get('metadata', {})
                elif isinstance(parsed, list):
                    existing_steps = parsed
            except:
                existing_steps = []
        
        # æ›´æ–° steps åˆ—è¡¨
        if body.steps is not None:
            existing_steps = body.steps
        
        # æ›´æ–° metadata
        if body.published_job_ids is not None:
            metadata['published_job_ids'] = body.published_job_ids
        if body.current_step is not None:
            metadata['current_step'] = body.current_step
        
        # ç»Ÿä¸€å­˜å‚¨ä¸º {steps: [...], metadata: {...}} æ ¼å¼
        if metadata:
            todo.steps = json_mod.dumps({'steps': existing_steps, 'metadata': metadata})
        elif body.steps is not None:
            todo.steps = json_mod.dumps(existing_steps)
    
    await db.commit()
    
    return {"message": "ä»»åŠ¡æ›´æ–°æˆåŠŸ"}


@router.delete("/todos/{todo_id}")
async def delete_todo(
    todo_id: int,
    db: AsyncSession = Depends(get_db)
):
    """åˆ é™¤å¾…åŠä»»åŠ¡"""
    result = await db.execute(select(Todo).where(Todo.id == todo_id))
    todo = result.scalar_one_or_none()
    
    if not todo:
        return {"error": "ä»»åŠ¡ä¸å­˜åœ¨"}
    
    await db.delete(todo)
    await db.commit()
    
    return {"message": "ä»»åŠ¡å·²åˆ é™¤"}


@router.delete("/todos/cleanup/duplicates")
async def cleanup_duplicate_profile_tasks(
    user_id: int = Query(..., description="ç”¨æˆ·ID"),
    db: AsyncSession = Depends(get_db)
):
    """æ¸…ç†é‡å¤çš„ã€Œå®Œå–„ç®€å†èµ„æ–™ã€ä»»åŠ¡ï¼Œåªä¿ç•™æœ€æ–°çš„ä¸€ä¸ª"""
    from sqlalchemy import or_
    
    # æŸ¥æ‰¾æ‰€æœ‰ã€Œå®Œå–„ç®€å†èµ„æ–™ã€ç±»å‹çš„ä»»åŠ¡ï¼ˆåŒ¹é…ç±»å‹æˆ–ç²¾ç¡®æ ‡é¢˜ï¼‰
    result = await db.execute(
        select(Todo)
        .where(Todo.user_id == user_id)
        .where(
            or_(
                Todo.todo_type == 'profile_complete',
                Todo.title == 'å®Œå–„ä¸ªäººç®€å†èµ„æ–™',  # ç²¾ç¡®åŒ¹é…
                Todo.title.ilike('%å®Œå–„%ç®€å†%'),
                Todo.title.ilike('%å®Œå–„%èµ„æ–™%')
            )
        )
        .order_by(Todo.created_at.desc())
    )
    profile_tasks = result.scalars().all()
    
    if len(profile_tasks) <= 1:
        return {
            "message": "æ— éœ€æ¸…ç†",
            "deleted_count": 0,
            "kept_task_id": profile_tasks[0].id if profile_tasks else None
        }
    
    # ä¿ç•™æœ€æ–°çš„ä¸€ä¸ªï¼Œåˆ é™¤å…¶ä»–çš„
    kept_task = profile_tasks[0]
    deleted_count = 0
    
    for task in profile_tasks[1:]:
        await db.delete(task)
        deleted_count += 1
    
    await db.commit()
    
    return {
        "message": f"å·²æ¸…ç† {deleted_count} ä¸ªé‡å¤ä»»åŠ¡",
        "deleted_count": deleted_count,
        "kept_task_id": kept_task.id
    }


@router.get("/tasks")
async def get_tasks(
    user_id: int = Query(1, description="ç”¨æˆ·ID"),
    db: AsyncSession = Depends(get_db)
):
    """è·å–ä»»åŠ¡åˆ—è¡¨ï¼ˆç”¨äº AI åŠ©æ‰‹ä¾§è¾¹æ ï¼‰- çº¯åŠ¨æ€æ•°æ®"""
    result = await db.execute(
        select(Todo)
        .where(Todo.user_id == user_id)
        .order_by(Todo.updated_at.desc())
        .limit(10)
    )
    todos = result.scalars().all()
    
    # å¦‚æœæ²¡æœ‰ä»»åŠ¡ï¼Œè¿”å›ç©ºæ•°ç»„ï¼ˆä¸ä½¿ç”¨é™æ€æ•°æ®ï¼‰
    if not todos:
        return []
    
    # è¡¥å¿æ£€æŸ¥ï¼šèº«ä»½è®¤è¯å·²å®Œæˆä½† DISC ä»»åŠ¡ä¸å­˜åœ¨åˆ™è‡ªåŠ¨åˆ›å»º
    has_disc = any(t.title == 'DISCæ€§æ ¼æµ‹è¯•' for t in todos)
    if not has_disc:
        from app.models.settings import PersonalCertification
        cert_result = await db.execute(
            select(PersonalCertification)
            .where(PersonalCertification.user_id == user_id)
            .where(PersonalCertification.category == 'identity')
        )
        has_identity = cert_result.scalars().first() is not None
        if has_identity:
            disc_task = Todo(
                user_id=user_id,
                title='DISCæ€§æ ¼æµ‹è¯•',
                description='é€šè¿‡DISCæµ‹è¯•äº†è§£æ‚¨çš„è¡Œä¸ºé£æ ¼ï¼Œæå‡æ±‚èŒåŒ¹é…åº¦',
                priority=TodoPriority.MEDIUM,
                status=TodoStatus.PENDING,
                progress=0,
                source=TodoSource.AGENT,
                todo_type=TodoType.CANDIDATE,
                icon='UserIcon',
            )
            db.add(disc_task)
            await db.commit()
            await db.refresh(disc_task)
            todos = [disc_task] + list(todos)  # æŠŠæ–°ä»»åŠ¡åŠ åˆ°åˆ—è¡¨æœ€å‰é¢
    
    # çŠ¶æ€æ˜ å°„
    status_map = {
        TodoStatus.PENDING: "pending",
        TodoStatus.IN_PROGRESS: "running",
        TodoStatus.RUNNING: "running",
        TodoStatus.COMPLETED: "completed",
        TodoStatus.CANCELLED: "completed",
    }
    
    return [{
        "id": str(t.id),
        "title": t.title,
        "status": status_map.get(t.status, "pending"),
        "time": t.updated_at.strftime("%H:%M") if t.updated_at else "--:--",
        "icon": t.icon or "Calendar",
        "priority": t.priority.value.upper() if t.priority else "MEDIUM",
        "source": t.source.value.upper() if t.source else "USER",
        "todo_type": t.todo_type.value.upper() if t.todo_type else "SYSTEM",
        "type": t.todo_type.value.upper() if t.todo_type else "SYSTEM",
        "progress": t.progress or 0,
        "description": t.description or "",
        "aiAdvice": t.ai_advice or "",
        "ai_advice": t.ai_advice or "",
        "steps": json.loads(t.steps) if isinstance(t.steps, str) else (t.steps or []),
        "createdAt": t.created_at.strftime("%Y-%m-%d") if t.created_at else None,
        "created_at": t.created_at.isoformat() if t.created_at else None,
        "updated_at": t.updated_at.isoformat() if t.updated_at else None,
        "completed_at": t.completed_at.isoformat() if t.completed_at else None,
    } for t in todos]


# ============ AI èŠå¤©æ¥å£ ============

def generate_smart_response(message: str, context: str, model: str) -> dict:
    """ç”Ÿæˆæ™ºèƒ½æœ¬åœ°å“åº”ï¼ˆå½“ AI API ä¸å¯ç”¨æ—¶ï¼‰"""
    import re
    
    message_lower = message.lower()
    response = ""
    
    # é¢è¯•ç›¸å…³
    if any(kw in message_lower for kw in ["é¢è¯•", "interview", "å‡†å¤‡", "æŠ€å·§"]):
        if "å‰ç«¯" in message_lower or "frontend" in message_lower:
            response = """å…³äºå‰ç«¯å¼€å‘é¢è¯•å‡†å¤‡ï¼Œæˆ‘å»ºè®®ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ç€æ‰‹ï¼š

**1. æ ¸å¿ƒæŠ€æœ¯æ ˆ**
â€¢ HTML5/CSS3 è¯­ä¹‰åŒ–ã€Flexbox/Grid å¸ƒå±€
â€¢ JavaScript åŸºç¡€ï¼šé—­åŒ…ã€åŸå‹é“¾ã€äº‹ä»¶å¾ªç¯ã€Promise/async-await
â€¢ æ¡†æ¶æ·±å…¥ï¼šReact Hooks åŸç†ã€Vue å“åº”å¼åŸç†ã€è™šæ‹Ÿ DOM

**2. å·¥ç¨‹åŒ–èƒ½åŠ›**
â€¢ Webpack/Vite æ„å»ºä¼˜åŒ–
â€¢ ä»£ç è§„èŒƒä¸ ESLint/Prettier
â€¢ Git å·¥ä½œæµä¸ CI/CD

**3. æ€§èƒ½ä¼˜åŒ–**
â€¢ é¦–å±åŠ è½½ä¼˜åŒ–ã€æ‡’åŠ è½½
â€¢ ç¼“å­˜ç­–ç•¥ã€CDN é…ç½®
â€¢ Web Vitals æŒ‡æ ‡ä¼˜åŒ–

**4. ç®—æ³•ä¸æ•°æ®ç»“æ„**
â€¢ å¸¸è§æ’åºã€æŸ¥æ‰¾ç®—æ³•
â€¢ æ ‘ã€é“¾è¡¨ã€æ ˆã€é˜Ÿåˆ—åŸºæœ¬æ“ä½œ
â€¢ LeetCode åˆ·é¢˜ï¼ˆå»ºè®® 100-200 é“ï¼‰

**5. é¡¹ç›®ç»éªŒå‡†å¤‡**
â€¢ ç”¨ STAR æ³•åˆ™å‡†å¤‡é¡¹ç›®ä»‹ç»
â€¢ å‡†å¤‡æŠ€æœ¯éš¾ç‚¹ä¸è§£å†³æ–¹æ¡ˆ
â€¢ äº†è§£ä¸šåŠ¡æŒ‡æ ‡ä¸æŠ€æœ¯ä»·å€¼

éœ€è¦æˆ‘é’ˆå¯¹æŸä¸ªæ–¹é¢è¯¦ç»†å±•å¼€å—ï¼Ÿ"""
        elif "åç«¯" in message_lower or "backend" in message_lower:
            response = """åç«¯å¼€å‘é¢è¯•å‡†å¤‡è¦ç‚¹ï¼š

**1. è¯­è¨€åŸºç¡€**
â€¢ å†…å­˜ç®¡ç†ã€å¹¶å‘æ¨¡å‹ã€GC æœºåˆ¶
â€¢ å¸¸ç”¨è®¾è®¡æ¨¡å¼ä¸åº”ç”¨åœºæ™¯

**2. æ•°æ®åº“**
â€¢ SQL ä¼˜åŒ–ã€ç´¢å¼•åŸç†
â€¢ äº‹åŠ¡éš”ç¦»çº§åˆ«ã€MVCC
â€¢ NoSQL ä½¿ç”¨åœºæ™¯

**3. ç³»ç»Ÿè®¾è®¡**
â€¢ åˆ†å¸ƒå¼ç³»ç»ŸåŸºç¡€
â€¢ ç¼“å­˜ç­–ç•¥ã€æ¶ˆæ¯é˜Ÿåˆ—
â€¢ é«˜å¯ç”¨ã€é«˜å¹¶å‘è®¾è®¡

**4. ç½‘ç»œä¸å®‰å…¨**
â€¢ HTTP/HTTPSã€TCP/IP
â€¢ è®¤è¯æˆæƒæœºåˆ¶
â€¢ å¸¸è§å®‰å…¨æ¼æ´é˜²æŠ¤

éœ€è¦é’ˆå¯¹å“ªä¸ªæ–¹å‘æ·±å…¥äº†è§£ï¼Ÿ"""
        else:
            response = """é¢è¯•å‡†å¤‡å»ºè®®ï¼š

**1. æŠ€æœ¯å‡†å¤‡**
â€¢ å¤¯å®åŸºç¡€çŸ¥è¯†ï¼Œç†è§£åº•å±‚åŸç†
â€¢ åˆ·ç®—æ³•é¢˜ï¼Œä¿æŒæ‰‹æ„Ÿ
â€¢ å‡†å¤‡é¡¹ç›®æ·±åº¦è®²è§£

**2. è½¯å®åŠ›**
â€¢ è‡ªæˆ‘ä»‹ç»ç²¾ç‚¼åˆ°ä½
â€¢ æ²Ÿé€šè¡¨è¾¾æ¸…æ™°æœ‰æ¡ç†
â€¢ å±•ç°å­¦ä¹ èƒ½åŠ›ä¸æ½œåŠ›

**3. å…¬å¸è°ƒç ”**
â€¢ äº†è§£å…¬å¸ä¸šåŠ¡ä¸æŠ€æœ¯æ ˆ
â€¢ å‡†å¤‡æœ‰ä»·å€¼çš„æé—®

éœ€è¦æˆ‘å¸®æ‚¨å‡†å¤‡å…·ä½“çš„å†…å®¹å—ï¼Ÿ"""
    
    # ç®€å†ç›¸å…³
    elif any(kw in message_lower for kw in ["ç®€å†", "resume", "cv", "ä¼˜åŒ–"]):
        response = """ç®€å†ä¼˜åŒ–å»ºè®®ï¼š

**1. åŸºæœ¬ä¿¡æ¯**
â€¢ è”ç³»æ–¹å¼å®Œæ•´ã€é‚®ç®±ä¸“ä¸š
â€¢ æ±‚èŒæ„å‘æ˜ç¡®

**2. å·¥ä½œç»å†**
â€¢ ä½¿ç”¨ STAR æ³•åˆ™æè¿°é¡¹ç›®
â€¢ é‡åŒ–æˆæœï¼ˆæå‡ XX%ã€èŠ‚çœ XX æˆæœ¬ï¼‰
â€¢ çªå‡ºæŠ€æœ¯äº®ç‚¹ä¸åˆ›æ–°ç‚¹

**3. æŠ€æœ¯æ ˆ**
â€¢ æŒ‰ç†Ÿç»ƒåº¦æ’åº
â€¢ ä¸ç›®æ ‡å²—ä½åŒ¹é…
â€¢ é¿å…å†™ä¸ç†Ÿæ‚‰çš„æŠ€æœ¯

**4. æ ¼å¼æ’ç‰ˆ**
â€¢ ä¸€é¡µçº¸åŸåˆ™
â€¢ é‡ç‚¹ä¿¡æ¯çªå‡º
â€¢ PDF æ ¼å¼æŠ•é€’

éœ€è¦æˆ‘å¸®æ‚¨é’ˆå¯¹ç‰¹å®šå²—ä½ä¼˜åŒ–ç®€å†å—ï¼Ÿ"""
    
    # èŒä½/å²—ä½ç›¸å…³
    elif any(kw in message_lower for kw in ["èŒä½", "å²—ä½", "æ¨è", "å·¥ä½œ", "job"]):
        response = """å…³äºèŒä½æ¨èï¼Œæˆ‘å¯ä»¥å¸®æ‚¨ï¼š

**1. å²—ä½åŒ¹é…åˆ†æ**
â€¢ åˆ†ææ‚¨çš„æŠ€èƒ½ä¸ç»éªŒ
â€¢ åŒ¹é…é€‚åˆçš„èŒä½ç±»å‹
â€¢ è¯„ä¼°è–ªèµ„æœŸæœ›åˆç†æ€§

**2. è¡Œä¸šè¶‹åŠ¿**
â€¢ å½“å‰çƒ­é—¨æŠ€æœ¯æ–¹å‘
â€¢ å„è¡Œä¸šæ‹›è˜éœ€æ±‚
â€¢ è¿œç¨‹/æ··åˆåŠå…¬æœºä¼š

**3. æ±‚èŒç­–ç•¥**
â€¢ ç›®æ ‡å…¬å¸ç­›é€‰
â€¢ æŠ•é€’æ—¶æœºæŠŠæ¡
â€¢ å¤šæ¸ é“æ±‚èŒå»ºè®®

è¯·å‘Šè¯‰æˆ‘æ‚¨çš„èƒŒæ™¯å’ŒæœŸæœ›ï¼Œæˆ‘æ¥ä¸ºæ‚¨æ¨èåˆé€‚çš„æ–¹å‘ã€‚"""
    
    # ç“¶é¢ˆ/é—®é¢˜åˆ†æ
    elif any(kw in message_lower for kw in ["ç“¶é¢ˆ", "é—®é¢˜", "åˆ†æ", "å›°éš¾", "æŒ‘æˆ˜"]):
        if context:
            response = f"""æ ¹æ®å½“å‰ä»»åŠ¡ã€Œ{context}ã€ï¼Œæˆ‘æ¥å¸®æ‚¨åˆ†æï¼š

**å¯èƒ½çš„ç“¶é¢ˆç‚¹ï¼š**

1. **çŸ¥è¯†å‚¨å¤‡**
   â€¢ æ˜¯å¦æœ‰çŸ¥è¯†ç›²åŒºéœ€è¦è¡¥å……ï¼Ÿ
   â€¢ æŠ€æœ¯æ·±åº¦æ˜¯å¦è¶³å¤Ÿï¼Ÿ

2. **æ—¶é—´ç®¡ç†**
   â€¢ å‡†å¤‡æ—¶é—´æ˜¯å¦å……è¶³ï¼Ÿ
   â€¢ æ˜¯å¦éœ€è¦è°ƒæ•´ä¼˜å…ˆçº§ï¼Ÿ

3. **å®è·µç»éªŒ**
   â€¢ æ˜¯å¦ç¼ºå°‘é¡¹ç›®å®æˆ˜ï¼Ÿ
   â€¢ éœ€è¦æ›´å¤šæ¨¡æ‹Ÿç»ƒä¹ ï¼Ÿ

**å»ºè®®çš„çªç ´æ–¹å‘ï¼š**
â€¢ åˆ¶å®šæ˜ç¡®çš„å­¦ä¹ è®¡åˆ’
â€¢ æ‰¾åˆ°è–„å¼±ç¯èŠ‚é‡ç‚¹çªç ´
â€¢ å¯»æ±‚åé¦ˆæŒç»­æ”¹è¿›

æ‚¨è§‰å¾—ä¸»è¦å¡åœ¨å“ªä¸ªæ–¹é¢ï¼Ÿæˆ‘å¯ä»¥æä¾›æ›´é’ˆå¯¹æ€§çš„å»ºè®®ã€‚"""
        else:
            response = """å¸®æ‚¨åˆ†æå½“å‰å¯èƒ½çš„ç“¶é¢ˆï¼š

**å¸¸è§ç“¶é¢ˆç±»å‹ï¼š**

1. **æŠ€æœ¯ç“¶é¢ˆ** - æŸäº›çŸ¥è¯†ç‚¹ä¸å¤Ÿæ·±å…¥
2. **é¡¹ç›®ç“¶é¢ˆ** - ç¼ºå°‘äº®ç‚¹é¡¹ç›®ç»éªŒ
3. **æ²Ÿé€šç“¶é¢ˆ** - è¡¨è¾¾ä¸å¤Ÿæ¸…æ™°æœ‰åŠ›
4. **å¿ƒæ€ç“¶é¢ˆ** - é¢è¯•ç´§å¼ å½±å“å‘æŒ¥

è¯·æè¿°ä¸€ä¸‹æ‚¨ç›®å‰é‡åˆ°çš„å…·ä½“æƒ…å†µï¼Œæˆ‘æ¥å¸®æ‚¨æ·±å…¥åˆ†æã€‚"""
    
    # è®¡åˆ’/å»ºè®®ç›¸å…³
    elif any(kw in message_lower for kw in ["è®¡åˆ’", "å»ºè®®", "è§„åˆ’", "æ€ä¹ˆåš", "å¦‚ä½•"]):
        response = """åˆ¶å®šæ‰§è¡Œè®¡åˆ’çš„å»ºè®®ï¼š

**1. æ˜ç¡®ç›®æ ‡**
â€¢ ç¡®å®šå…·ä½“å¯è¡¡é‡çš„ç›®æ ‡
â€¢ è®¾å®šåˆç†çš„æ—¶é—´èŠ‚ç‚¹

**2. æ‹†åˆ†ä»»åŠ¡**
â€¢ å°†å¤§ç›®æ ‡åˆ†è§£ä¸ºå°ä»»åŠ¡
â€¢ æ¯ä¸ªä»»åŠ¡æœ‰æ˜ç¡®çš„äº¤ä»˜ç‰©

**3. ä¼˜å…ˆæ’åº**
â€¢ è¯†åˆ«å…³é”®è·¯å¾„
â€¢ å…ˆè§£å†³é‡è¦ç´§æ€¥çš„äº‹é¡¹

**4. æ‰§è¡Œä¸å¤ç›˜**
â€¢ æ¯æ—¥/æ¯å‘¨æ£€æŸ¥è¿›åº¦
â€¢ åŠæ—¶è°ƒæ•´è®¡åˆ’

éœ€è¦æˆ‘å¸®æ‚¨åˆ¶å®šå…·ä½“çš„è¡ŒåŠ¨è®¡åˆ’å—ï¼Ÿ"""
    
    # é»˜è®¤å“åº”
    else:
        response = f"""æ”¶åˆ°æ‚¨çš„é—®é¢˜ï¼šã€Œ{message}ã€

æˆ‘æ˜¯ Devnors AI æ‹›è˜åŠ©æ‰‹ï¼Œå¯ä»¥å¸®æ‚¨ï¼š

â€¢ **æ±‚èŒå‡†å¤‡** - ç®€å†ä¼˜åŒ–ã€é¢è¯•æŠ€å·§ã€æŠ€æœ¯æå‡
â€¢ **èŒä½æ¨è** - æ ¹æ®æ‚¨çš„èƒŒæ™¯åŒ¹é…åˆé€‚å²—ä½
â€¢ **èŒä¸šè§„åˆ’** - åˆ†æå‘å±•è·¯å¾„ã€è¡Œä¸šè¶‹åŠ¿
â€¢ **ä»»åŠ¡æ‰§è¡Œ** - ååŠ©å®Œæˆæ‹›è˜ç›¸å…³ä»»åŠ¡

è¯·é—®æ‚¨éœ€è¦å“ªæ–¹é¢çš„å¸®åŠ©ï¼Ÿæˆ–è€…å¯ä»¥æ›´å…·ä½“åœ°æè¿°æ‚¨çš„éœ€æ±‚ã€‚"""
    
    return {
        "response": response,
        "tokens_used": 0,
        "model": model
    }


class ChatRequest(BaseModel):
    message: str
    history: list = []
    model: str = "Devnors 1.0"
    context: str = ""  # å¯é€‰çš„ä¸Šä¸‹æ–‡ï¼Œå¦‚ä»»åŠ¡ä¿¡æ¯


@router.post("/chat")
async def chat_with_assistant(
    request: ChatRequest,
    db: AsyncSession = Depends(get_db)
):
    """AI åŠ©æ‰‹èŠå¤©æ¥å£ - ä¼˜å…ˆä½¿ç”¨ MiniMax"""
    import httpx
    from app.config import settings
    
    # ç³»ç»Ÿæç¤º
    system_prompt = """ä½ æ˜¯ Devnors å¾—è‹¥ AI æ™ºèƒ½æ‹›è˜åŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·è§£å†³æ‹›è˜ç›¸å…³çš„é—®é¢˜ã€‚ä½ çš„èƒ½åŠ›åŒ…æ‹¬ï¼š

1. è§£ç­”æ‹›è˜ç›¸å…³é—®é¢˜
2. æä¾›æ±‚èŒ/æ‹›è˜å»ºè®®
3. åˆ†æèŒä½åŒ¹é…åº¦
4. ä¼˜åŒ–ç®€å†å’ŒèŒä½æè¿°
5. è§„åˆ’èŒä¸šå‘å±•æ–¹å‘
6. å¸®åŠ©æ‰§è¡Œæ‹›è˜ä»»åŠ¡

è¯·ç”¨ç®€æ´ã€ä¸“ä¸šã€å‹å¥½çš„æ–¹å¼å›å¤ç”¨æˆ·ã€‚å›å¤è¯·ä½¿ç”¨ä¸­æ–‡ã€‚"""
    
    if request.context:
        system_prompt += f"\n\nå½“å‰ä»»åŠ¡ä¸Šä¸‹æ–‡ï¼š{request.context}"
    
    # ä¼˜å…ˆä½¿ç”¨ MiniMax APIï¼ˆä½¿ç”¨ OpenAI å…¼å®¹æ ¼å¼ï¼Œä¸éœ€è¦ GROUP_IDï¼‰
    minimax_api_key = settings.minimax_api_key or ""
    
    if minimax_api_key:
        try:
            # æ„å»º OpenAI å…¼å®¹çš„æ¶ˆæ¯æ ¼å¼
            messages = [
                {"role": "system", "content": system_prompt}
            ]
            
            # æ·»åŠ å†å²æ¶ˆæ¯
            for msg in request.history[-10:]:
                role = msg.get("role", "user")
                content = msg.get("content", "")
                messages.append({"role": role, "content": content})
            
            # æ·»åŠ å½“å‰æ¶ˆæ¯
            messages.append({"role": "user", "content": request.message})
            
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    "https://api.minimax.chat/v1/text/chatcompletion_v2",
                    headers={
                        "Authorization": f"Bearer {minimax_api_key}",
                        "Content-Type": "application/json",
                    },
                    json={
                        "model": "abab6.5s-chat",
                        "messages": messages,
                        "max_tokens": 2048,
                        "temperature": 0.7,
                        "top_p": 0.9,
                    }
                )
                
                result = response.json()
                
                # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
                if result.get("base_resp", {}).get("status_code", 0) == 0:
                    if "choices" in result and len(result["choices"]) > 0:
                        reply = result["choices"][0].get("message", {}).get("content", "")
                        tokens = result.get("usage", {}).get("total_tokens", 0)
                        
                        return {
                            "response": reply,
                            "tokens_used": tokens,
                            "model": request.model
                        }
                else:
                    error_msg = result.get("base_resp", {}).get("status_msg", "")
                    print(f"MiniMax API error: {error_msg}")
        except Exception as e:
            print(f"MiniMax API error: {e}")
    
    # å¤‡é€‰ï¼šä½¿ç”¨ Gemini API
    gemini_api_key = settings.gemini_api_key or ""
    
    if gemini_api_key:
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                contents = []
                contents.append({"role": "user", "parts": [{"text": f"[ç³»ç»ŸæŒ‡ä»¤] {system_prompt}"}]})
                contents.append({"role": "model", "parts": [{"text": "å¥½çš„ï¼Œæˆ‘æ˜¯ Devnors AI æ™ºèƒ½æ‹›è˜åŠ©æ‰‹ï¼Œéšæ—¶ä¸ºæ‚¨æä¾›ä¸“ä¸šçš„æ‹›è˜æœåŠ¡ã€‚"}]})
                
                for msg in request.history[-10:]:
                    role = msg.get("role", "user")
                    content = msg.get("content", "")
                    if role == "user":
                        contents.append({"role": "user", "parts": [{"text": content}]})
                    else:
                        contents.append({"role": "model", "parts": [{"text": content}]})
                
                contents.append({"role": "user", "parts": [{"text": request.message}]})
                
                response = await client.post(
                    f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={gemini_api_key}",
                    headers={"Content-Type": "application/json"},
                    json={
                        "contents": contents,
                        "generationConfig": {"temperature": 0.7, "topP": 0.9, "maxOutputTokens": 2048}
                    }
                )
                
                result = response.json()
                
                if "candidates" in result and len(result["candidates"]) > 0:
                    reply = result["candidates"][0].get("content", {}).get("parts", [{}])[0].get("text", "")
                    tokens = result.get("usageMetadata", {}).get("totalTokenCount", 0)
                    return {"response": reply, "tokens_used": tokens, "model": request.model}
        except Exception as e:
            print(f"Gemini API error: {e}")
    
    # å¦‚æœæ‰€æœ‰ AI API éƒ½ä¸å¯ç”¨ï¼Œä½¿ç”¨æ™ºèƒ½æœ¬åœ°å“åº”
    return generate_smart_response(request.message, request.context, request.model)


# ============ èŠå¤©æ¶ˆæ¯æŒä¹…åŒ–æ¥å£ ============

from app.models.todo import ChatMessage

class SaveMessageRequest(BaseModel):
    role: str  # user / assistant
    content: str
    todo_id: Optional[int] = None  # å…³è”çš„ä»»åŠ¡IDï¼ŒNoneè¡¨ç¤ºé€šç”¨å¯¹è¯


class SaveMessageBatchRequest(BaseModel):
    messages: list  # [{role, content, todo_id?}]


@router.get("/chat-messages")
async def get_chat_messages(
    user_id: int = Query(1, description="ç”¨æˆ·ID"),
    todo_id: Optional[int] = Query(None, description="ä»»åŠ¡IDï¼Œä¸ä¼ åˆ™è·å–é€šç”¨å¯¹è¯"),
    limit: int = Query(100, description="æœ€å¤§æ¡æ•°"),
    db: AsyncSession = Depends(get_db)
):
    """è·å–èŠå¤©å†å²æ¶ˆæ¯"""
    query = select(ChatMessage).where(ChatMessage.user_id == user_id)
    if todo_id is not None:
        query = query.where(ChatMessage.todo_id == todo_id)
    else:
        query = query.where(ChatMessage.todo_id == None)
    
    query = query.order_by(ChatMessage.created_at.asc()).limit(limit)
    result = await db.execute(query)
    messages = result.scalars().all()
    
    return [{
        "id": m.id,
        "role": m.role,
        "content": m.content,
        "todo_id": m.todo_id,
        "created_at": m.created_at.isoformat() if m.created_at else None,
    } for m in messages]


@router.post("/chat-messages")
async def save_chat_message(
    msg: SaveMessageRequest,
    user_id: int = Query(1, description="ç”¨æˆ·ID"),
    db: AsyncSession = Depends(get_db)
):
    """ä¿å­˜å•æ¡èŠå¤©æ¶ˆæ¯"""
    chat_msg = ChatMessage(
        user_id=user_id,
        todo_id=msg.todo_id,
        role=msg.role,
        content=msg.content,
    )
    db.add(chat_msg)
    await db.commit()
    await db.refresh(chat_msg)
    return {"id": chat_msg.id, "message": "æ¶ˆæ¯å·²ä¿å­˜"}


@router.post("/chat-messages/batch")
async def save_chat_messages_batch(
    batch: SaveMessageBatchRequest,
    user_id: int = Query(1, description="ç”¨æˆ·ID"),
    db: AsyncSession = Depends(get_db)
):
    """æ‰¹é‡ä¿å­˜èŠå¤©æ¶ˆæ¯ï¼ˆç”¨äºåŒæ­¥æœ¬åœ°å¯¹è¯åˆ°æœåŠ¡ç«¯ï¼‰"""
    saved = []
    for item in batch.messages:
        chat_msg = ChatMessage(
            user_id=user_id,
            todo_id=item.get("todo_id"),
            role=item["role"],
            content=item["content"],
        )
        db.add(chat_msg)
        saved.append(chat_msg)
    await db.commit()
    return {"saved_count": len(saved), "message": f"å·²ä¿å­˜ {len(saved)} æ¡æ¶ˆæ¯"}


@router.delete("/chat-messages")
async def clear_chat_messages(
    user_id: int = Query(1, description="ç”¨æˆ·ID"),
    todo_id: Optional[int] = Query(None, description="ä»»åŠ¡IDï¼Œä¸ä¼ åˆ™æ¸…é™¤é€šç”¨å¯¹è¯"),
    db: AsyncSession = Depends(get_db)
):
    """æ¸…é™¤æŒ‡å®šå¯¹è¯çš„èŠå¤©è®°å½•"""
    query = select(ChatMessage).where(ChatMessage.user_id == user_id)
    if todo_id is not None:
        query = query.where(ChatMessage.todo_id == todo_id)
    else:
        query = query.where(ChatMessage.todo_id == None)
    
    result = await db.execute(query)
    messages = result.scalars().all()
    for m in messages:
        await db.delete(m)
    await db.commit()
    return {"deleted_count": len(messages), "message": f"å·²æ¸…é™¤ {len(messages)} æ¡æ¶ˆæ¯"}


@router.get("/recruitment-suggestions")
async def get_recruitment_suggestions(
    user_id: int = Query(1, description="ç”¨æˆ·ID"),
    db: AsyncSession = Depends(get_db)
):
    """æ ¹æ®ä¼ä¸šèµ„æ–™è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆä¸ªæ€§åŒ–çš„æ‹›è˜å²—ä½å»ºè®®"""
    import httpx
    import json
    from app.config import settings
    from app.models.settings import UserSettings, EnterpriseCertification
    
    try:
        # 1. è·å–ä¼ä¸šè®¾ç½®ä¿¡æ¯
        stmt = select(UserSettings).where(UserSettings.user_id == user_id)
        result = await db.execute(stmt)
        user_settings = result.scalar_one_or_none()
        
        company_name = ''
        industry = ''
        company_size = ''
        location = ''
        benefits = ''
        description = ''
        
        if user_settings:
            company_name = user_settings.display_name or user_settings.short_name or ''
            industry = user_settings.industry or ''
            company_size = user_settings.company_size or ''
            location = f"{user_settings.city or ''}{user_settings.district or ''}"
            benefits = user_settings.benefits or ''
            description = user_settings.description or ''
        
        # 2. è·å–ä¼ä¸šè®¤è¯æ•°æ®ï¼ˆè¥ä¸šæ‰§ç…§ä¸­çš„ç»è¥èŒƒå›´ç­‰ï¼‰
        stmt = select(EnterpriseCertification).where(
            EnterpriseCertification.user_id == user_id
        )
        result = await db.execute(stmt)
        enterprise_certs = result.scalars().all()
        
        business_scope = ''
        registered_capital = ''
        cert_names = []
        
        for cert in enterprise_certs:
            if cert.business_scope:
                business_scope = cert.business_scope
            if cert.registered_capital:
                registered_capital = cert.registered_capital
            if cert.name:
                cert_names.append(cert.name)
        
        # 3. è·å–ç”¨æˆ·åŸºæœ¬ä¿¡æ¯
        from app.models.user import User
        stmt = select(User).where(User.id == user_id)
        result = await db.execute(stmt)
        user_obj = result.scalar_one_or_none()
        
        if not company_name and user_obj:
            company_name = user_obj.company_name or 'è´µå…¬å¸'
        
        # 4. è·å–ä¼ä¸šè®°å¿†ï¼ˆè¦æ±‚ç±» + é«˜å¼ºåº¦è®°å¿†ï¼‰
        memory_result = await db.execute(
            select(Memory)
            .where(Memory.user_id == user_id)
            .where(Memory.scope == MemoryScope.EMPLOYER)
            .order_by(Memory.emphasis_count.desc(), Memory.created_at.desc())
        )
        all_memories = memory_result.scalars().all()
        
        memory_context_lines = []
        for m in all_memories:
            # è¦æ±‚ç±»å…¨éƒ¨çº³å…¥
            if m.type.value == 'requirement':
                strength = f"ï¼ˆå¼ºè°ƒÃ—{m.emphasis_count}ï¼‰" if (m.emphasis_count or 1) > 1 else ""
                memory_context_lines.append(f"â€¢ [è¦æ±‚] {m.content}{strength}")
            # åŠ¨ä½œ/ç­–ç•¥ç±»å…¨éƒ¨çº³å…¥
            elif m.type.value in ('action', 'strategy'):
                strength = f"ï¼ˆå¼ºè°ƒÃ—{m.emphasis_count}ï¼‰" if (m.emphasis_count or 1) > 1 else ""
                type_label = TYPE_NAME_MAP.get(m.type.value, m.type.value)
                memory_context_lines.append(f"â€¢ [{type_label}] {m.content}{strength}")
            # å…¶ä»–ç±»å‹ï¼šå¼ºåº¦>=2çš„çº³å…¥
            elif (m.emphasis_count or 1) >= 2:
                type_label = TYPE_NAME_MAP.get(m.type.value, m.type.value)
                strength = f"ï¼ˆå¼ºè°ƒÃ—{m.emphasis_count}ï¼‰"
                memory_context_lines.append(f"â€¢ [{type_label}] {m.content}{strength}")
        
        memory_context = ""
        if memory_context_lines:
            memory_context = "\n\nã€ä¼ä¸šè®°å¿† â€” å¿…é¡»ä¸¥æ ¼éµå®ˆã€‘\n" + "\n".join(memory_context_lines)
        
        # 5. æ„å»ºä¼ä¸šç”»åƒä¸Šä¸‹æ–‡
        enterprise_context = f"""ä¼ä¸šåç§°ï¼š{company_name}
æ‰€å±è¡Œä¸šï¼š{industry or 'æœªçŸ¥'}
ä¼ä¸šè§„æ¨¡ï¼š{company_size or 'æœªçŸ¥'}
æ‰€åœ¨åœ°åŒºï¼š{location or 'æœªçŸ¥'}
ä¼ä¸šç¦åˆ©ï¼š{benefits or 'æœªçŸ¥'}
ä¼ä¸šç®€ä»‹ï¼š{description or 'æœªçŸ¥'}
ç»è¥èŒƒå›´ï¼š{business_scope or 'æœªçŸ¥'}
æ³¨å†Œèµ„æœ¬ï¼š{registered_capital or 'æœªçŸ¥'}
å·²æœ‰è®¤è¯ï¼š{', '.join(cert_names) if cert_names else 'æ— '}{memory_context}"""
        
        # 6. è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆä¸ªæ€§åŒ–æ‹›è˜å»ºè®®
        ai_prompt = f"""ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„ HR æ‹›è˜é¡¾é—®ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¼ä¸šä¿¡æ¯å’Œä¼ä¸šè®°å¿†åå¥½ï¼Œåˆ†æè¯¥ä¼ä¸šæœ€å¯èƒ½éœ€è¦æ‹›è˜çš„å²—ä½ç±»å‹ï¼Œç»™å‡º 3-5 ä¸ªç²¾å‡†çš„æ‹›è˜å»ºè®®æç¤ºè¯­ã€‚

{enterprise_context}

è¦æ±‚ï¼š
1. æ ¹æ®ä¼ä¸šçš„è¡Œä¸šã€ç»è¥èŒƒå›´ã€è§„æ¨¡æ¥æ¨æ–­æœ€é€‚åˆçš„å²—ä½
2. æ¯æ¡å»ºè®®è¦ç®€çŸ­æœ‰åŠ›ï¼Œåƒç”¨æˆ·è‡ªç„¶è¯´å‡ºæ¥çš„è¯ï¼Œä¸è¶…è¿‡20ä¸ªå­—
3. è¦ä½“ç°è¡Œä¸šç‰¹è‰²ï¼Œä¸è¦å¤ªé€šç”¨
4. å¦‚æœæ˜¯ç§‘æŠ€/äº’è”ç½‘å…¬å¸ï¼Œå»ºè®®å¼€å‘ã€äº§å“ã€è¿è¥ç›¸å…³å²—ä½
5. å¦‚æœæ˜¯ä¼ ç»Ÿè¡Œä¸šï¼Œå»ºè®®é”€å”®ã€å¸‚åœºã€ç®¡ç†ç›¸å…³å²—ä½
6. å¦‚æœä¼ä¸šä¿¡æ¯ä¸è¶³ï¼Œç»™å‡ºé€šç”¨ä½†æœ‰ä»·å€¼çš„å»ºè®®
7. å¦‚æœä¼ä¸šè®°å¿†ä¸­æœ‰æ˜ç¡®çš„è¦æ±‚ï¼ˆå¦‚è–ªèµ„èŒƒå›´ã€å­¦å†åå¥½ã€å·¥ä½œæ–¹å¼ç­‰ï¼‰ï¼Œå»ºè®®ä¸­å¿…é¡»ä½“ç°è¿™äº›çº¦æŸ
8. å¦‚æœä¼ä¸šè®°å¿†ä¸­æœ‰å†å²æ‹›è˜åŠ¨ä½œè®°å½•ï¼Œå‚è€ƒè¿™äº›ä¿¡æ¯æ¨èç›¸ä¼¼æˆ–äº’è¡¥çš„å²—ä½

è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹ JSON æ ¼å¼è¿”å›ï¼ˆç›´æ¥è¿”å›JSONï¼Œä¸è¦markdownæ ‡è®°ï¼‰ï¼š
{{
  "company_summary": "ä¸€å¥è¯æè¿°ä¼ä¸šç‰¹å¾å’Œæ‹›è˜åå¥½ï¼ˆä¸è¶…è¿‡40å­—ï¼Œå¦‚æœæœ‰ä¼ä¸šè®°å¿†ä¸­çš„å…³é”®çº¦æŸè¯·æåŠï¼‰",
  "suggestions": [
    "å»ºè®®æç¤ºè¯­1",
    "å»ºè®®æç¤ºè¯­2", 
    "å»ºè®®æç¤ºè¯­3"
  ]
}}"""

        # å°è¯• MiniMax
        minimax_api_key = settings.minimax_api_key or ""
        if minimax_api_key:
            try:
                async with httpx.AsyncClient(timeout=30.0) as client:
                    response = await client.post(
                        "https://api.minimax.chat/v1/text/chatcompletion_v2",
                        headers={
                            "Authorization": f"Bearer {minimax_api_key}",
                            "Content-Type": "application/json",
                        },
                        json={
                            "model": "abab6.5s-chat",
                            "messages": [
                                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªèµ„æ·± HR æ‹›è˜é¡¾é—®ï¼Œæ“…é•¿åˆ†æä¼ä¸šéœ€æ±‚å¹¶æä¾›ç²¾å‡†çš„æ‹›è˜å»ºè®®ã€‚è¯·åªè¿”å›JSONæ ¼å¼ç»“æœã€‚"},
                                {"role": "user", "content": ai_prompt}
                            ],
                            "max_tokens": 1024,
                            "temperature": 0.7,
                        }
                    )
                    result = response.json()
                    if result.get("choices"):
                        reply = result["choices"][0].get("message", {}).get("content", "")
                        # è§£æ JSON
                        import re
                        json_match = re.search(r'\{[\s\S]*\}', reply)
                        if json_match:
                            data = json.loads(json_match.group())
                            return {
                                "company_name": company_name,
                                "company_summary": data.get("company_summary", ""),
                                "suggestions": data.get("suggestions", []),
                                "enterprise_context": enterprise_context
                            }
            except Exception as e:
                print(f"[Recruitment Suggestions] MiniMax error: {e}")
        
        # Gemini fallback
        gemini_api_key = settings.gemini_api_key or ""
        if gemini_api_key:
            try:
                async with httpx.AsyncClient(timeout=30.0) as client:
                    response = await client.post(
                        f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={gemini_api_key}",
                        json={
                            "contents": [{"parts": [{"text": ai_prompt}]}],
                            "generationConfig": {"temperature": 0.7, "maxOutputTokens": 1024}
                        }
                    )
                    result = response.json()
                    if "candidates" in result:
                        reply = result["candidates"][0]["content"]["parts"][0].get("text", "")
                        import re
                        json_match = re.search(r'\{[\s\S]*\}', reply)
                        if json_match:
                            data = json.loads(json_match.group())
                            return {
                                "company_name": company_name,
                                "company_summary": data.get("company_summary", ""),
                                "suggestions": data.get("suggestions", []),
                                "enterprise_context": enterprise_context
                            }
            except Exception as e:
                print(f"[Recruitment Suggestions] Gemini error: {e}")
        
        # AI ä¸å¯ç”¨æ—¶è¿”å›åŸºäºè¡Œä¸šçš„é»˜è®¤å»ºè®®
        default_suggestions = _get_default_suggestions(industry, business_scope)
        return {
            "company_name": company_name,
            "company_summary": f"{industry or 'ä¼ä¸š'}æ‹›è˜",
            "suggestions": default_suggestions,
            "enterprise_context": enterprise_context
        }
        
    except Exception as e:
        print(f"[Recruitment Suggestions] Error: {e}")
        return {
            "company_name": "è´µå…¬å¸",
            "company_summary": "",
            "suggestions": [
                "æ‹›ä¸€ä¸ªé«˜çº§å‰ç«¯å·¥ç¨‹å¸ˆ",
                "éœ€è¦äº§å“ç»ç†ï¼Œ3å¹´ç»éªŒ",
                "æŠ€æœ¯å›¢é˜Ÿæ‰©æ‹›5ä¸ªäºº"
            ],
            "enterprise_context": ""
        }


def _get_default_suggestions(industry: str, business_scope: str) -> list:
    """æ ¹æ®è¡Œä¸šè¿”å›é»˜è®¤æ‹›è˜å»ºè®®"""
    industry_lower = (industry or '').lower()
    scope_lower = (business_scope or '').lower()
    
    combined = industry_lower + scope_lower
    
    if any(kw in combined for kw in ['äº’è”ç½‘', 'ç§‘æŠ€', 'è½¯ä»¶', 'ä¿¡æ¯æŠ€æœ¯', 'it', 'äººå·¥æ™ºèƒ½', 'ai', 'æ•°æ®']):
        return [
            "æ‹›é«˜çº§å‰ç«¯å·¥ç¨‹å¸ˆï¼Œç†Ÿæ‚‰React",
            "éœ€è¦Javaåç«¯ï¼Œ3å¹´ä»¥ä¸Šç»éªŒ",
            "æ‹›äº§å“ç»ç†ï¼Œè´Ÿè´£æ ¸å¿ƒäº§å“çº¿",
            "æ•°æ®åˆ†æå¸ˆï¼Œç†Ÿæ‚‰Python",
        ]
    elif any(kw in combined for kw in ['ç”µå•†', 'é›¶å”®', 'è´¸æ˜“', 'å•†åŠ¡']):
        return [
            "æ‹›è¿è¥ç»ç†ï¼Œæœ‰ç”µå•†å¹³å°ç»éªŒ",
            "éœ€è¦ä¾›åº”é“¾ä¸“å‘˜ï¼Œç†Ÿæ‚‰ç‰©æµ",
            "æ‹›ç¾å·¥è®¾è®¡ï¼Œä¼šè§†é¢‘å‰ªè¾‘ä¼˜å…ˆ",
            "å®¢æœä¸»ç®¡ï¼Œç®¡ç†10äººå›¢é˜Ÿ",
        ]
    elif any(kw in combined for kw in ['é‡‘è', 'æŠ•èµ„', 'é“¶è¡Œ', 'ä¿é™©', 'è¯åˆ¸']):
        return [
            "æ‹›é£æ§ç»ç†ï¼Œ5å¹´ä»¥ä¸Šç»éªŒ",
            "éœ€è¦æŠ•èµ„åˆ†æå¸ˆï¼ŒCFAä¼˜å…ˆ",
            "æ‹›åˆè§„ä¸“å‘˜ï¼Œç†Ÿæ‚‰é‡‘èæ³•è§„",
            "å®¢æˆ·ç»ç†ï¼Œæœ‰é«˜å‡€å€¼å®¢æˆ·èµ„æº",
        ]
    elif any(kw in combined for kw in ['æ•™è‚²', 'åŸ¹è®­', 'å­¦æ ¡']):
        return [
            "æ‹›è¯¾ç¨‹ç ”å‘ä¸“å®¶ï¼ŒK12æ–¹å‘",
            "éœ€è¦è‹±è¯­è€å¸ˆï¼Œæœ‰æ•™å¸ˆèµ„æ ¼è¯",
            "æ‹›å¸‚åœºæ¨å¹¿ç»ç†ï¼Œæ‡‚æ•™è‚²è¡Œä¸š",
            "æ•™åŠ¡ç®¡ç†ï¼Œæœ‰3å¹´ä»¥ä¸Šç»éªŒ",
        ]
    elif any(kw in combined for kw in ['åŒ»ç–—', 'å¥åº·', 'åŒ»è¯', 'ç”Ÿç‰©']):
        return [
            "æ‹›ä¸´åºŠç ”ç©¶å‘˜ï¼ŒåŒ»å­¦èƒŒæ™¯",
            "éœ€è¦åŒ»è¯ä»£è¡¨ï¼Œè¦†ç›–åä¸œåŒºåŸŸ",
            "æ‹›æ³¨å†Œä¸“å‘˜ï¼Œç†Ÿæ‚‰NMPAæµç¨‹",
            "è´¨é‡ç®¡ç†å·¥ç¨‹å¸ˆï¼ŒGMPç»éªŒ",
        ]
    elif any(kw in combined for kw in ['åˆ¶é€ ', 'ç”Ÿäº§', 'å·¥å‚', 'å·¥ç¨‹']):
        return [
            "æ‹›ç”Ÿäº§ç»ç†ï¼Œç²¾ç›Šç®¡ç†ç»éªŒ",
            "éœ€è¦å“è´¨å·¥ç¨‹å¸ˆï¼Œç†Ÿæ‚‰ISO",
            "æ‹›æœºæ¢°è®¾è®¡å¸ˆï¼ŒSolidWorks",
            "ä»“å‚¨ç‰©æµä¸»ç®¡ï¼Œç®¡ç†ç»éªŒä¼˜å…ˆ",
        ]
    else:
        return [
            "æ‹›ä¸€ä¸ªé«˜çº§å·¥ç¨‹å¸ˆ",
            "éœ€è¦é”€å”®ç»ç†ï¼Œè¡Œä¸šç»éªŒä¸°å¯Œ",
            "æ‹›è¡Œæ”¿äººäº‹ï¼Œç»¼åˆç®¡ç†èƒ½åŠ›å¼º",
            "å¸‚åœºä¸“å‘˜ï¼Œæœ‰ç­–åˆ’æ‰§è¡Œç»éªŒ",
        ]


# ============ ç”¨æˆ·èµ„æ–™æ¥å£ ============

from app.models.profile import UserProfile, ProfileType
from pydantic import BaseModel

class ProfileUpdate(BaseModel):
    """èµ„æ–™æ›´æ–°è¯·æ±‚"""
    display_name: Optional[str] = None
    title: Optional[str] = None
    summary: Optional[str] = None
    avatar_url: Optional[str] = None
    cover_url: Optional[str] = None
    candidate_data: Optional[dict] = None
    employer_data: Optional[dict] = None


@router.get("/profile")
async def get_user_profile(
    user_id: int = Query(..., description="ç”¨æˆ·ID"),
    profile_type: str = Query("candidate", description="èµ„æ–™ç±»å‹: candidate æˆ– employer"),
    db: AsyncSession = Depends(get_db)
):
    """è·å–ç”¨æˆ·èµ„æ–™"""
    try:
        ptype = ProfileType(profile_type.lower())
    except ValueError:
        ptype = ProfileType.CANDIDATE
    
    result = await db.execute(
        select(UserProfile)
        .where(UserProfile.user_id == user_id)
        .where(UserProfile.profile_type == ptype)
    )
    profile = result.scalar_one_or_none()
    
    if not profile:
        # è¿”å›ç©ºèµ„æ–™æ¨¡æ¿
        return {
            "id": None,
            "user_id": user_id,
            "profile_type": profile_type,
            "display_name": None,
            "title": None,
            "summary": None,
            "avatar_url": None,
            "cover_url": None,
            "candidate_data": {
                "skills": [],
                "experience_years": 0,
                "career_path": [],
                "certifications": [],
                "awards": [],
                "radar_data": [],
                "ideal_job": ""
            } if ptype == ProfileType.CANDIDATE else None,
            "employer_data": {
                "company_name": "",
                "industry": "",
                "size": "",
                "location": "",
                "founded": "",
                "website": "",
                "culture": "",
                "benefits": [],
                "tech_stack": [],
                "open_positions": [],
                "mission": "",
                "vision": ""
            } if ptype == ProfileType.EMPLOYER else None,
            "is_empty": True
        }
    
    return {
        "id": profile.id,
        "user_id": profile.user_id,
        "profile_type": profile.profile_type.value,
        "display_name": profile.display_name,
        "title": profile.title,
        "summary": profile.summary,
        "avatar_url": profile.avatar_url,
        "cover_url": profile.cover_url,
        "candidate_data": profile.candidate_data,
        "employer_data": profile.employer_data,
        "is_empty": False,
        "created_at": profile.created_at.isoformat() if profile.created_at else None,
        "updated_at": profile.updated_at.isoformat() if profile.updated_at else None
    }


@router.post("/profile")
async def update_user_profile(
    profile_data: ProfileUpdate,
    user_id: int = Query(..., description="ç”¨æˆ·ID"),
    profile_type: str = Query("candidate", description="èµ„æ–™ç±»å‹: candidate æˆ– employer"),
    db: AsyncSession = Depends(get_db)
):
    """åˆ›å»ºæˆ–æ›´æ–°ç”¨æˆ·èµ„æ–™"""
    try:
        ptype = ProfileType(profile_type.lower())
    except ValueError:
        ptype = ProfileType.CANDIDATE
    
    result = await db.execute(
        select(UserProfile)
        .where(UserProfile.user_id == user_id)
        .where(UserProfile.profile_type == ptype)
    )
    profile = result.scalar_one_or_none()
    
    if not profile:
        # åˆ›å»ºæ–°èµ„æ–™
        profile = UserProfile(
            user_id=user_id,
            profile_type=ptype
        )
        db.add(profile)
    
    # æ›´æ–°å­—æ®µ
    if profile_data.display_name is not None:
        profile.display_name = profile_data.display_name
    if profile_data.title is not None:
        profile.title = profile_data.title
    if profile_data.summary is not None:
        profile.summary = profile_data.summary
    if profile_data.avatar_url is not None:
        profile.avatar_url = profile_data.avatar_url
    if profile_data.cover_url is not None:
        profile.cover_url = profile_data.cover_url
    if profile_data.candidate_data is not None:
        # åˆå¹¶ç°æœ‰æ•°æ®
        existing = profile.candidate_data or {}
        if isinstance(existing, str):
            existing = json.loads(existing)
        existing.update(profile_data.candidate_data)
        profile.candidate_data = existing
    if profile_data.employer_data is not None:
        existing = profile.employer_data or {}
        if isinstance(existing, str):
            existing = json.loads(existing)
        existing.update(profile_data.employer_data)
        profile.employer_data = existing
    
    await db.commit()
    await db.refresh(profile)
    
    return {
        "success": True,
        "message": "èµ„æ–™æ›´æ–°æˆåŠŸ",
        "profile": {
            "id": profile.id,
            "user_id": profile.user_id,
            "profile_type": profile.profile_type.value,
            "display_name": profile.display_name,
            "title": profile.title,
            "summary": profile.summary,
            "candidate_data": profile.candidate_data,
            "employer_data": profile.employer_data
        }
    }


@router.patch("/profile/field")
async def update_profile_field(
    field: str = Query(..., description="å­—æ®µå"),
    value: str = Query(..., description="å­—æ®µå€¼"),
    user_id: int = Query(..., description="ç”¨æˆ·ID"),
    profile_type: str = Query("candidate", description="èµ„æ–™ç±»å‹"),
    force_update: bool = Query(False, description="æ˜¯å¦å¼ºåˆ¶è¦†ç›–å·²æœ‰å€¼"),
    db: AsyncSession = Depends(get_db)
):
    """æ›´æ–°ç”¨æˆ·èµ„æ–™çš„å•ä¸ªå­—æ®µï¼ˆç”¨äº AI åŠ©æ‰‹ç¼–è¾‘ï¼‰
    
    é»˜è®¤åªåœ¨å­—æ®µä¸ºç©ºæ—¶ä¿å­˜ï¼ˆé¦–æ¬¡è¾“å…¥ï¼‰ï¼Œå¦‚æœå·²æœ‰å€¼éœ€è¦ force_update=true æ‰ä¼šè¦†ç›–
    """
    try:
        ptype = ProfileType(profile_type.lower())
    except ValueError:
        ptype = ProfileType.CANDIDATE
    
    result = await db.execute(
        select(UserProfile)
        .where(UserProfile.user_id == user_id)
        .where(UserProfile.profile_type == ptype)
    )
    profile = result.scalar_one_or_none()
    
    if not profile:
        profile = UserProfile(user_id=user_id, profile_type=ptype)
        db.add(profile)
    
    # è¾…åŠ©å‡½æ•°ï¼šæ£€æŸ¥å­—æ®µæ˜¯å¦å·²æœ‰å€¼
    def has_existing_value(field_name: str) -> tuple:
        """è¿”å› (æ˜¯å¦æœ‰å€¼, ç°æœ‰å€¼)"""
        if field_name in ['display_name', 'title', 'summary', 'avatar_url', 'cover_url']:
            existing = getattr(profile, field_name, None)
            if existing and str(existing).strip():
                return True, existing
            return False, None
        
        if ptype == ProfileType.CANDIDATE:
            data = profile.candidate_data or {}
            if isinstance(data, str):
                data = json.loads(data)
            existing = data.get(field_name)
            if existing:
                if isinstance(existing, list) and len(existing) > 0:
                    return True, existing
                if isinstance(existing, str) and existing.strip():
                    return True, existing
            return False, None
        
        if ptype == ProfileType.EMPLOYER:
            data = profile.employer_data or {}
            if isinstance(data, str):
                data = json.loads(data)
            existing = data.get(field_name)
            if existing:
                if isinstance(existing, list) and len(existing) > 0:
                    return True, existing
                if isinstance(existing, str) and existing.strip():
                    return True, existing
            return False, None
        
        return False, None
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰å€¼
    has_value, existing_value = has_existing_value(field)
    if has_value and not force_update:
        # å·²æœ‰å€¼ä¸”æœªå¼ºåˆ¶è¦†ç›–ï¼Œè¿”å›æç¤º
        return {
            "success": False,
            "has_existing": True,
            "existing_value": existing_value if isinstance(existing_value, str) else str(existing_value),
            "message": f"å­—æ®µ {field} å·²æœ‰å€¼ï¼Œå¦‚éœ€ä¿®æ”¹è¯·ç¡®è®¤è¦†ç›–",
            "field": field,
            "new_value": value
        }
    
    # æ ¹æ®å­—æ®µåæ›´æ–°
    if field in ['display_name', 'title', 'summary', 'avatar_url', 'cover_url']:
        setattr(profile, field, value)
    elif ptype == ProfileType.CANDIDATE:
        # æ›´æ–° candidate_data ä¸­çš„å­—æ®µ
        # åˆ›å»ºæ–°å­—å…¸å‰¯æœ¬ä»¥ç¡®ä¿ SQLAlchemy æ£€æµ‹åˆ°å˜åŒ–
        old_data = profile.candidate_data or {}
        if isinstance(old_data, str):
            old_data = json.loads(old_data)
        data = dict(old_data)  # åˆ›å»ºå‰¯æœ¬
        
        if field == 'skills':
            data['skills'] = [s.strip() for s in value.split(',') if s.strip()]
        elif field == 'experience':
            # å·¥ä½œç»å†ä¿å­˜ä¸ºæ•°ç»„
            data['experience'] = [value.strip()] if value.strip() else []
        elif field == 'education':
            # æ•™è‚²èƒŒæ™¯ä¿å­˜ä¸ºæ•°ç»„
            data['education'] = [value.strip()] if value.strip() else []
        elif field == 'projects':
            # é¡¹ç›®ç»å†ä¿å­˜ä¸ºæ•°ç»„
            data['projects'] = [value.strip()] if value.strip() else []
        elif field == 'experience_years':
            data['experience_years'] = int(value) if value.isdigit() else 0
        elif field == 'ideal_job':
            data['ideal_job'] = value
        else:
            data[field] = value
        
        # æ˜¾å¼èµ‹å€¼æ–°å­—å…¸ä»¥è§¦å‘ SQLAlchemy å˜æ›´æ£€æµ‹
        profile.candidate_data = data
        # å¼ºåˆ¶æ ‡è®°å­—æ®µä¸ºå·²ä¿®æ”¹
        from sqlalchemy.orm.attributes import flag_modified
        flag_modified(profile, 'candidate_data')
    elif ptype == ProfileType.EMPLOYER:
        # æ›´æ–° employer_data ä¸­çš„å­—æ®µ
        old_data = profile.employer_data or {}
        if isinstance(old_data, str):
            old_data = json.loads(old_data)
        data = dict(old_data)  # åˆ›å»ºå‰¯æœ¬
        
        if field in ['benefits', 'tech_stack']:
            data[field] = [s.strip() for s in value.split(',') if s.strip()]
        else:
            data[field] = value
        
        profile.employer_data = data
        # å¼ºåˆ¶æ ‡è®°å­—æ®µä¸ºå·²ä¿®æ”¹
        from sqlalchemy.orm.attributes import flag_modified
        flag_modified(profile, 'employer_data')
    
    await db.commit()
    await db.refresh(profile)
    
    return {
        "success": True,
        "message": f"å­—æ®µ {field} æ›´æ–°æˆåŠŸ",
        "field": field,
        "value": value,
        "was_overwrite": has_value
    }


# ============ Token èµ„é‡‘è´¦æˆ·æ¥å£ ============

from app.models.token import TokenUsage, TokenPackage, TokenAction, PackageType

# Token æ¶ˆè€—ç±»å‹ä¸­æ–‡åæ˜ å°„
TOKEN_ACTION_NAMES = {
    TokenAction.RESUME_PARSE: "ç®€å†è§£æ",
    TokenAction.PROFILE_BUILD: "ç”»åƒè°ƒä¼˜",
    TokenAction.JOB_MATCH: "èŒä½åŒ¹é…",
    TokenAction.INTERVIEW: "å¤šæ™ºèƒ½ä½“é¢è¯•",
    TokenAction.MARKET_ANALYSIS: "å¸‚åœºåˆ†æ",
    TokenAction.ROUTE_DISPATCH: "å…¨å±€è·¯ç”±",
    TokenAction.CHAT: "AI å¯¹è¯",
    TokenAction.OTHER: "å…¶ä»–",
}


@router.get("/tokens/stats")
async def get_token_stats(
    user_id: int = Query(..., description="ç”¨æˆ·ID"),
    db: AsyncSession = Depends(get_db)
):
    """è·å–ç”¨æˆ· Token ç»Ÿè®¡ä¿¡æ¯"""
    from sqlalchemy import func
    from datetime import timedelta
    
    # è·å–ç”¨æˆ·å½“å‰å¥—é¤å’Œä½™é¢
    result = await db.execute(
        select(TokenPackage)
        .where(TokenPackage.user_id == user_id)
        .where(TokenPackage.is_active == True)
        .order_by(TokenPackage.purchased_at.desc())
    )
    packages = result.scalars().all()
    
    # è®¡ç®—æ€»ä½™é¢
    total_remaining = sum(p.remaining_tokens for p in packages) if packages else 100000  # é»˜è®¤é€10ä¸‡
    total_used = sum(p.used_tokens for p in packages) if packages else 0
    total_purchased = sum(p.price for p in packages) if packages else 0
    
    # è·å–ä»Šæ—¥æ¶ˆè€—
    today = datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0)
    result = await db.execute(
        select(func.sum(TokenUsage.tokens_used))
        .where(TokenUsage.user_id == user_id)
        .where(TokenUsage.created_at >= today)
    )
    today_usage = result.scalar() or 0
    
    # è·å–æ˜¨æ—¥æ¶ˆè€—ï¼ˆç”¨äºè®¡ç®—ç¯æ¯”ï¼‰
    yesterday = today - timedelta(days=1)
    result = await db.execute(
        select(func.sum(TokenUsage.tokens_used))
        .where(TokenUsage.user_id == user_id)
        .where(TokenUsage.created_at >= yesterday)
        .where(TokenUsage.created_at < today)
    )
    yesterday_usage = result.scalar() or 1  # é¿å…é™¤é›¶
    
    # è®¡ç®—ç¯æ¯”å¢é•¿
    change_rate = ((today_usage - yesterday_usage) / yesterday_usage * 100) if yesterday_usage > 0 else 0
    
    # è®¡ç®—é¢„ä¼°ç»­èˆªå¤©æ•°ï¼ˆåŸºäºè¿‘7å¤©å¹³å‡æ¶ˆè€—ï¼‰
    week_ago = today - timedelta(days=7)
    result = await db.execute(
        select(func.sum(TokenUsage.tokens_used))
        .where(TokenUsage.user_id == user_id)
        .where(TokenUsage.created_at >= week_ago)
    )
    week_usage = result.scalar() or 1
    daily_avg = week_usage / 7
    estimated_days = int(total_remaining / daily_avg) if daily_avg > 0 else 999
    
    return {
        "balance": total_remaining,
        "balance_display": f"{total_remaining/1000000:.2f}M" if total_remaining >= 100000 else f"{total_remaining/1000:.1f}K",
        "today_usage": today_usage,
        "today_usage_display": f"{today_usage:,}",
        "change_rate": round(change_rate, 1),
        "change_direction": "up" if change_rate > 0 else "down" if change_rate < 0 else "stable",
        "total_purchased": total_purchased,
        "total_purchased_display": f"Â¥ {total_purchased:,.2f}",
        "estimated_days": estimated_days,
        "packages": [{
            "id": p.id,
            "type": p.package_type.value,
            "total": p.total_tokens,
            "used": p.used_tokens,
            "remaining": p.remaining_tokens,
            "expires_at": p.expires_at.isoformat() if p.expires_at else None
        } for p in packages]
    }


@router.get("/tokens/history")
async def get_token_history(
    user_id: int = Query(..., description="ç”¨æˆ·ID"),
    limit: int = Query(20, description="è¿”å›æ¡æ•°"),
    offset: int = Query(0, description="åç§»é‡"),
    db: AsyncSession = Depends(get_db)
):
    """è·å– Token æ¶ˆè€—å†å²"""
    result = await db.execute(
        select(TokenUsage)
        .where(TokenUsage.user_id == user_id)
        .order_by(TokenUsage.created_at.desc())
        .limit(limit)
        .offset(offset)
    )
    records = result.scalars().all()
    
    # å¦‚æœæ²¡æœ‰è®°å½•ï¼Œè¿”å›ä¸€äº›ç¤ºä¾‹æ•°æ®
    if not records:
        # ç”Ÿæˆæœ€è¿‘7å¤©çš„æ¨¡æ‹Ÿæ•°æ®
        sample_data = []
        for i in range(7):
            date = datetime.utcnow() - timedelta(days=i)
            actions = [TokenAction.RESUME_PARSE, TokenAction.INTERVIEW, TokenAction.PROFILE_BUILD, TokenAction.ROUTE_DISPATCH]
            action = actions[i % len(actions)]
            tokens = [42500, 89000, 12400, 56000, 92000, 15000, 34000][i]
            sample_data.append({
                "id": i + 1,
                "date": date.strftime("%Y-%m-%d"),
                "action": action.value,
                "type": TOKEN_ACTION_NAMES.get(action, "å…¶ä»–"),
                "tokens": tokens,
                "cost": f"Â¥{tokens/10000:.2f}",
                "description": f"AI {TOKEN_ACTION_NAMES.get(action, 'ä»»åŠ¡')}"
            })
        return {
            "items": sample_data,
            "total": len(sample_data),
            "has_more": False
        }
    
    return {
        "items": [{
            "id": r.id,
            "date": r.created_at.strftime("%Y-%m-%d"),
            "action": r.action.value,
            "type": TOKEN_ACTION_NAMES.get(r.action, "å…¶ä»–"),
            "tokens": r.tokens_used,
            "cost": f"Â¥{r.tokens_used/10000:.2f}",
            "description": r.description or f"AI {TOKEN_ACTION_NAMES.get(r.action, 'ä»»åŠ¡')}"
        } for r in records],
        "total": len(records),
        "has_more": len(records) >= limit
    }


@router.get("/tokens/chart")
async def get_token_chart(
    user_id: int = Query(..., description="ç”¨æˆ·ID"),
    days: int = Query(7, description="å¤©æ•°"),
    db: AsyncSession = Depends(get_db)
):
    """è·å– Token æ¶ˆè€—è¶‹åŠ¿å›¾æ•°æ®"""
    from sqlalchemy import func, cast, Date
    
    start_date = datetime.utcnow() - timedelta(days=days)
    
    # æŒ‰æ—¥æœŸåˆ†ç»„ç»Ÿè®¡
    result = await db.execute(
        select(
            func.date(TokenUsage.created_at).label('date'),
            func.sum(TokenUsage.tokens_used).label('total')
        )
        .where(TokenUsage.user_id == user_id)
        .where(TokenUsage.created_at >= start_date)
        .group_by(func.date(TokenUsage.created_at))
        .order_by(func.date(TokenUsage.created_at))
    )
    rows = result.all()
    
    # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    if not rows:
        chart_data = []
        for i in range(days):
            date = datetime.utcnow() - timedelta(days=days-1-i)
            values = [42500, 89000, 12400, 56000, 92000, 15000, 34000]
            chart_data.append({
                "name": date.strftime("%m-%d"),
                "value": values[i % len(values)]
            })
        return {
            "data": chart_data,
            "peak": max(v["value"] for v in chart_data),
            "average": sum(v["value"] for v in chart_data) // len(chart_data)
        }
    
    # æ„å»ºå®Œæ•´çš„æ—¥æœŸåºåˆ—ï¼ˆå¡«å……æ— æ•°æ®çš„æ—¥æœŸï¼‰
    date_map = {str(row.date): row.total for row in rows}
    chart_data = []
    for i in range(days):
        date = datetime.utcnow() - timedelta(days=days-1-i)
        date_str = date.strftime("%Y-%m-%d")
        chart_data.append({
            "name": date.strftime("%m-%d"),
            "value": date_map.get(date_str, 0)
        })
    
    values = [d["value"] for d in chart_data]
    return {
        "data": chart_data,
        "peak": max(values) if values else 0,
        "average": sum(values) // len(values) if values else 0
    }


@router.get("/tokens/packages")
async def get_available_packages():
    """è·å–å¯è´­ä¹°çš„ Token å¥—é¤"""
    return {
        "packages": [
            {
                "id": "starter",
                "name": "å…¥é—¨ä½“éªŒ",
                "tokens": 100000,
                "tokens_display": "100,000",
                "price": 99,
                "discount": None,
                "accent": "bg-indigo-50"
            },
            {
                "id": "pro",
                "name": "ç²¾è‹±çŒè˜",
                "tokens": 1000000,
                "tokens_display": "1,000,000",
                "price": 799,
                "discount": "æ€§ä»·æ¯”æœ€é«˜",
                "accent": "bg-amber-50"
            },
            {
                "id": "enterprise",
                "name": "ä¼ä¸šæ——èˆ°",
                "tokens": 10000000,
                "tokens_display": "10,000,000",
                "price": 6999,
                "discount": "-20%",
                "accent": "bg-rose-50"
            }
        ]
    }


@router.post("/tokens/record")
async def record_token_usage(
    user_id: int = Query(..., description="ç”¨æˆ·ID"),
    action: str = Query(..., description="æ¶ˆè€—ç±»å‹"),
    tokens: int = Query(..., description="æ¶ˆè€—æ•°é‡"),
    description: Optional[str] = Query(None, description="æè¿°"),
    db: AsyncSession = Depends(get_db)
):
    """è®°å½• Token æ¶ˆè€—"""
    try:
        token_action = TokenAction(action)
    except ValueError:
        token_action = TokenAction.OTHER
    
    # åˆ›å»ºæ¶ˆè€—è®°å½•
    usage = TokenUsage(
        user_id=user_id,
        action=token_action,
        tokens_used=tokens,
        description=description
    )
    db.add(usage)
    
    # æ›´æ–°ç”¨æˆ·å¥—é¤ä½™é¢
    result = await db.execute(
        select(TokenPackage)
        .where(TokenPackage.user_id == user_id)
        .where(TokenPackage.is_active == True)
        .where(TokenPackage.remaining_tokens > 0)
        .order_by(TokenPackage.expires_at.asc().nullslast())
    )
    package = result.scalar_one_or_none()
    
    if package:
        package.used_tokens += tokens
        package.remaining_tokens = max(0, package.remaining_tokens - tokens)
    
    await db.commit()
    
    return {
        "success": True,
        "message": "Token æ¶ˆè€—å·²è®°å½•",
        "tokens_used": tokens,
        "action": token_action.value
    }


# ============ æ¶ˆæ¯é€šçŸ¥ç›¸å…³æ¥å£ ============

@router.get("/notifications")
async def get_notifications(
    user_id: int = Query(..., description="ç”¨æˆ·ID"),
    notification_type: Optional[str] = Query(None, description="é€šçŸ¥ç±»å‹: system/match/interview/message"),
    unread_only: bool = Query(False, description="ä»…æœªè¯»"),
    page: int = Query(1, ge=1),
    page_size: int = Query(20, ge=1, le=100),
    db: AsyncSession = Depends(get_db)
):
    """è·å–ç”¨æˆ·é€šçŸ¥åˆ—è¡¨"""
    import random
    
    # åŸºç¡€é€šçŸ¥æ¨¡æ¿
    notification_templates = [
        {
            "type": "match",
            "title": "æ–°çš„èŒä½åŒ¹é…",
            "content_template": "æ‚¨çš„ç®€å†ä¸ã€Œ{job_title} - {company}ã€åŒ¹é…åº¦è¾¾åˆ° {match_rate}%",
            "icon": "Target",
            "color": "text-indigo-600",
            "bgColor": "bg-indigo-50",
            "link": "/jobs"
        },
        {
            "type": "interview",
            "title": "é¢è¯•é‚€è¯·",
            "content_template": "{company}é‚€è¯·æ‚¨å‚åŠ ã€Œ{job_title}ã€å²—ä½çš„{interview_type}",
            "icon": "Calendar",
            "color": "text-emerald-600",
            "bgColor": "bg-emerald-50",
            "link": "/workbench"
        },
        {
            "type": "system",
            "title": "ç³»ç»Ÿé€šçŸ¥",
            "content_template": "{message}",
            "icon": "Bell",
            "color": "text-amber-600",
            "bgColor": "bg-amber-50",
            "link": "/settings"
        },
        {
            "type": "message",
            "title": "æ–°æ¶ˆæ¯",
            "content_template": "{sender}å›å¤äº†æ‚¨ï¼šã€Œ{preview}ã€",
            "icon": "MessageSquare",
            "color": "text-violet-600",
            "bgColor": "bg-violet-50",
            "link": "/ai-assistant"
        },
        {
            "type": "match",
            "title": "ç®€å†è¢«æŸ¥çœ‹",
            "content_template": "{company}çš„æ‹›è˜å®˜æŸ¥çœ‹äº†æ‚¨çš„ç®€å†",
            "icon": "Eye",
            "color": "text-cyan-600",
            "bgColor": "bg-cyan-50",
            "link": "/candidate/profile"
        },
        {
            "type": "system",
            "title": "Token ä½™é¢æé†’",
            "content_template": "æ‚¨çš„ Token ä½™é¢ä¸è¶³ {threshold}ï¼Œå»ºè®®åŠæ—¶å……å€¼",
            "icon": "AlertCircle",
            "color": "text-rose-600",
            "bgColor": "bg-rose-50",
            "link": "/tokens"
        },
        {
            "type": "interview",
            "title": "é¢è¯•ç»“æœé€šçŸ¥",
            "content_template": "æ­å–œï¼æ‚¨é€šè¿‡äº†ã€Œ{company} - {job_title}ã€çš„{stage}",
            "icon": "CheckCircle2",
            "color": "text-emerald-600",
            "bgColor": "bg-emerald-50",
            "link": "/workbench"
        },
        {
            "type": "match",
            "title": "äººæ‰æ¨è",
            "content_template": "ç³»ç»Ÿä¸ºæ‚¨æ¨èäº† {count} ä½é«˜åŒ¹é…åº¦å€™é€‰äºº",
            "icon": "Users",
            "color": "text-indigo-600",
            "bgColor": "bg-indigo-50",
            "link": "/employer/talent-pool"
        },
        {
            "type": "system",
            "title": "è´¦æˆ·å‡çº§æˆåŠŸ",
            "content_template": "æ‚¨çš„è´¦æˆ·å·²å‡çº§ä¸º {plan} ç‰ˆæœ¬ï¼Œè§£é”æ›´å¤šé«˜çº§åŠŸèƒ½",
            "icon": "Zap",
            "color": "text-amber-600",
            "bgColor": "bg-amber-50",
            "link": "/settings"
        },
        {
            "type": "match",
            "title": "èŒä½æ›´æ–°æé†’",
            "content_template": "æ‚¨å…³æ³¨çš„ã€Œ{company}ã€å‘å¸ƒäº†æ–°èŒä½ï¼š{job_title}",
            "icon": "Briefcase",
            "color": "text-indigo-600",
            "bgColor": "bg-indigo-50",
            "link": "/jobs"
        }
    ]
    
    # ç¤ºä¾‹æ•°æ®
    companies = ["å­—èŠ‚è·³åŠ¨", "è…¾è®¯ç§‘æŠ€", "é˜¿é‡Œå·´å·´", "ç¾å›¢", "äº¬ä¸œ", "ç™¾åº¦", "åä¸º", "å°ç±³", "æ»´æ»´", "èš‚èšé›†å›¢"]
    job_titles = ["é«˜çº§å‰ç«¯å·¥ç¨‹å¸ˆ", "èµ„æ·±åç«¯å·¥ç¨‹å¸ˆ", "ç®—æ³•å·¥ç¨‹å¸ˆ", "äº§å“ç»ç†", "æŠ€æœ¯è´Ÿè´£äºº", "å…¨æ ˆå¼€å‘", "æ•°æ®åˆ†æå¸ˆ", "AI å·¥ç¨‹å¸ˆ"]
    interview_types = ["è§†é¢‘é¢è¯•", "ç”µè¯é¢è¯•", "ç°åœºé¢è¯•", "æŠ€æœ¯é¢è¯•", "HR é¢è¯•"]
    stages = ["ä¸€é¢", "äºŒé¢", "ç»ˆé¢", "HR é¢"]
    senders = ["HR ææ˜", "æ‹›è˜ç»ç†å¼ æ€»", "æŠ€æœ¯é¢è¯•å®˜ç‹å·¥", "çŒå¤´é¡¾é—®é™ˆç»ç†"]
    plans = ["Pro", "Ultra", "Enterprise"]
    messages = [
        "æ‚¨çš„ç®€å†å·²é€šè¿‡åˆç­›",
        "æ–°ç‰ˆæœ¬å·²å‘å¸ƒï¼Œè¯·åŠæ—¶æ›´æ–°",
        "æ‚¨çš„è´¦æˆ·å®‰å…¨è®¾ç½®å·²æ›´æ–°",
        "æ„Ÿè°¢æ‚¨çš„åé¦ˆï¼Œæˆ‘ä»¬ä¼šæŒç»­æ”¹è¿›"
    ]
    previews = [
        "å…³äºè–ªèµ„èŒƒå›´å¯ä»¥é¢è°ˆ...",
        "æœŸå¾…ä¸æ‚¨è¿›ä¸€æ­¥æ²Ÿé€š...",
        "æ‚¨çš„æŠ€æœ¯èƒŒæ™¯éå¸¸åŒ¹é…æˆ‘ä»¬çš„éœ€æ±‚...",
        "è¯·é—®æ‚¨æ–¹ä¾¿å‚åŠ ä¸‹å‘¨çš„é¢è¯•å—..."
    ]
    
    # æ—¶é—´ç”Ÿæˆå‡½æ•°
    def generate_time(minutes_ago: int) -> str:
        if minutes_ago < 60:
            return f"{minutes_ago}åˆ†é’Ÿå‰"
        elif minutes_ago < 1440:
            return f"{minutes_ago // 60}å°æ—¶å‰"
        elif minutes_ago < 2880:
            return "æ˜¨å¤©"
        else:
            return f"{minutes_ago // 1440}å¤©å‰"
    
    # ç”Ÿæˆé€šçŸ¥
    notifications = []
    for i in range(15):
        template = random.choice(notification_templates)
        
        # å¡«å……æ¨¡æ¿
        content = template["content_template"]
        content = content.replace("{company}", random.choice(companies))
        content = content.replace("{job_title}", random.choice(job_titles))
        content = content.replace("{match_rate}", str(random.randint(80, 98)))
        content = content.replace("{interview_type}", random.choice(interview_types))
        content = content.replace("{stage}", random.choice(stages))
        content = content.replace("{sender}", random.choice(senders))
        content = content.replace("{preview}", random.choice(previews))
        content = content.replace("{threshold}", "10,000")
        content = content.replace("{count}", str(random.randint(3, 8)))
        content = content.replace("{plan}", random.choice(plans))
        content = content.replace("{message}", random.choice(messages))
        
        # æ—¶é—´é€’å¢
        minutes_ago = i * random.randint(30, 180)
        
        notifications.append({
            "id": i + 1,
            "type": template["type"],
            "title": template["title"],
            "content": content,
            "time": generate_time(minutes_ago),
            "timestamp": (datetime.utcnow() - timedelta(minutes=minutes_ago)).isoformat(),
            "read": i >= 3,  # å‰ 3 æ¡æœªè¯»
            "icon": template["icon"],
            "color": template["color"],
            "bgColor": template["bgColor"],
            "link": template["link"]
        })
    
    # ç­›é€‰
    if notification_type:
        notifications = [n for n in notifications if n["type"] == notification_type]
    
    if unread_only:
        notifications = [n for n in notifications if not n["read"]]
    
    # åˆ†é¡µ
    total = len(notifications)
    start = (page - 1) * page_size
    end = start + page_size
    paginated = notifications[start:end]
    
    # ç»Ÿè®¡
    unread_count = len([n for n in notifications if not n["read"]])
    
    return {
        "notifications": paginated,
        "total": total,
        "unread_count": unread_count,
        "page": page,
        "page_size": page_size
    }


@router.post("/notifications/read")
async def mark_notification_read(
    user_id: int = Query(..., description="ç”¨æˆ·ID"),
    notification_id: Optional[int] = Query(None, description="é€šçŸ¥IDï¼Œä¸ä¼ åˆ™æ ‡è®°å…¨éƒ¨å·²è¯»"),
    db: AsyncSession = Depends(get_db)
):
    """æ ‡è®°é€šçŸ¥å·²è¯»"""
    # è¿™é‡Œåº”è¯¥æ›´æ–°æ•°æ®åº“ä¸­çš„é€šçŸ¥çŠ¶æ€
    # ç”±äºæ²¡æœ‰é€šçŸ¥è¡¨ï¼Œè¿”å›æ¨¡æ‹Ÿå“åº”
    if notification_id:
        return {
            "success": True,
            "message": f"é€šçŸ¥ {notification_id} å·²æ ‡è®°ä¸ºå·²è¯»"
        }
    else:
        return {
            "success": True,
            "message": "æ‰€æœ‰é€šçŸ¥å·²æ ‡è®°ä¸ºå·²è¯»"
        }


@router.delete("/notifications/{notification_id}")
async def delete_notification(
    notification_id: int,
    user_id: int = Query(..., description="ç”¨æˆ·ID"),
    db: AsyncSession = Depends(get_db)
):
    """åˆ é™¤é€šçŸ¥"""
    return {
        "success": True,
        "message": f"é€šçŸ¥ {notification_id} å·²åˆ é™¤"
    }


@router.get("/notifications/unread-count")
async def get_unread_count(
    user_id: int = Query(..., description="ç”¨æˆ·ID"),
    db: AsyncSession = Depends(get_db)
):
    """è·å–æœªè¯»é€šçŸ¥æ•°é‡"""
    import random
    # è¿”å›éšæœºæœªè¯»æ•°é‡ï¼ˆå®é™…åº”ä»æ•°æ®åº“æŸ¥è¯¢ï¼‰
    return {
        "unread_count": random.randint(1, 5)
    }


# ============ æ–‡ä»¶è§£ææ¥å£ ============

from fastapi import UploadFile, File, HTTPException
import tempfile
import os

@router.post("/parse-resume")
async def parse_resume_file(
    file: UploadFile = File(..., description="ç®€å†æ–‡ä»¶ (PDF/Word/TXT)")
):
    """
    è§£æä¸Šä¼ çš„ç®€å†æ–‡ä»¶ï¼Œè¿”å›æ–‡æœ¬å†…å®¹
    æ”¯æŒæ ¼å¼: PDF, Word (.doc/.docx), æ–‡æœ¬æ–‡ä»¶ (.txt/.md)
    """
    if not file.filename:
        raise HTTPException(status_code=400, detail="æœªé€‰æ‹©æ–‡ä»¶")
    
    # è·å–æ–‡ä»¶æ‰©å±•å
    file_ext = file.filename.split('.')[-1].lower() if '.' in file.filename else ''
    
    # æ£€æŸ¥æ–‡ä»¶ç±»å‹
    allowed_extensions = ['pdf', 'doc', 'docx', 'txt', 'md']
    if file_ext not in allowed_extensions:
        raise HTTPException(
            status_code=400, 
            detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}ã€‚æ”¯æŒçš„æ ¼å¼: PDF, Word, TXT"
        )
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å° (æœ€å¤§ 10MB)
    content = await file.read()
    if len(content) > 10 * 1024 * 1024:
        raise HTTPException(status_code=400, detail="æ–‡ä»¶è¿‡å¤§ï¼Œæœ€å¤§æ”¯æŒ 10MB")
    
    try:
        extracted_text = ""
        
        if file_ext == 'pdf':
            # è§£æ PDF
            import pdfplumber
            import io
            
            with pdfplumber.open(io.BytesIO(content)) as pdf:
                pages_text = []
                for page in pdf.pages:
                    text = page.extract_text()
                    if text:
                        pages_text.append(text)
                extracted_text = '\n\n'.join(pages_text)
        
        elif file_ext in ['doc', 'docx']:
            # è§£æ Word
            from docx import Document
            import io
            
            doc = Document(io.BytesIO(content))
            paragraphs = [p.text for p in doc.paragraphs if p.text.strip()]
            extracted_text = '\n'.join(paragraphs)
        
        elif file_ext in ['txt', 'md']:
            # æ–‡æœ¬æ–‡ä»¶ç›´æ¥è§£ç 
            try:
                extracted_text = content.decode('utf-8')
            except UnicodeDecodeError:
                extracted_text = content.decode('gbk', errors='ignore')
        
        if not extracted_text.strip():
            raise HTTPException(status_code=400, detail="æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æ— æ³•è§£æ")
        
        return {
            "success": True,
            "filename": file.filename,
            "file_type": file_ext,
            "content": extracted_text.strip(),
            "char_count": len(extracted_text.strip())
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"æ–‡ä»¶è§£æé”™è¯¯: {e}")
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶è§£æå¤±è´¥: {str(e)}")


from pydantic import BaseModel

class AutoFillRequest(BaseModel):
    user_id: int
    resume_content: str

@router.post("/auto-fill-profile")
async def auto_fill_profile_from_resume(
    request: AutoFillRequest,
    db: AsyncSession = Depends(get_db)
):
    """
    æ™ºèƒ½è§£æç®€å†å†…å®¹ï¼Œè‡ªåŠ¨å¡«å……ç”¨æˆ·èµ„æ–™å’Œè®°å¿†
    """
    from app.agents.base_agent import BaseAgent
    from app.models.memory import Memory, MemoryType, MemoryImportance, MemoryScope
    from app.models.profile import UserProfile, ProfileType
    from sqlalchemy.orm.attributes import flag_modified
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„ AI Agent ç”¨äºè§£æç®€å†
    class ResumeExtractor(BaseAgent):
        def _get_fallback_response(self, prompt: str) -> str:
            return "{}"
        def _get_fallback_json(self, prompt: str) -> dict:
            return {}
    
    extractor = ResumeExtractor(
        system_instruction="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç®€å†è§£æåŠ©æ‰‹ï¼Œè¯·å‡†ç¡®æå–ç®€å†ä¸­çš„ä¿¡æ¯å¹¶è¿”å› JSON æ ¼å¼ã€‚ç¡®ä¿ JSON å®Œæ•´ä¸”æ ¼å¼æ­£ç¡®ã€‚"
    )
    
    user_id = request.user_id
    resume_content = request.resume_content
    
    # ä½¿ç”¨ AI è§£æç®€å†å†…å®¹
    parse_prompt = f"""è¯·ä»ä»¥ä¸‹ç®€å†å†…å®¹ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼Œè¿”å› JSON æ ¼å¼ï¼š

ç®€å†å†…å®¹ï¼š
{resume_content[:6000]}

è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼ˆå¦‚æœç®€å†ä¸­æ²¡æœ‰ï¼Œå¡« nullï¼‰ï¼š
{{
    "display_name": "å§“å",
    "title": "å½“å‰èŒä½/èŒç§°",
    "summary": "ä¸ªäººç®€ä»‹/è‡ªæˆ‘è¯„ä»·ï¼ˆ50-200å­—ï¼Œå¦‚æœæ²¡æœ‰è¯·æ ¹æ®ç®€å†å†…å®¹æ€»ç»“ï¼‰",
    "skills": ["æŠ€èƒ½1", "æŠ€èƒ½2"],
    "experience": [
        {{
            "company": "å…¬å¸åç§°",
            "position": "èŒä½",
            "period": "æ—¶é—´æ®µ",
            "description": "å·¥ä½œæè¿°"
        }}
    ],
    "education": [
        {{
            "school": "å­¦æ ¡åç§°",
            "degree": "å­¦ä½",
            "major": "ä¸“ä¸š",
            "period": "æ—¶é—´æ®µ"
        }}
    ],
    "projects": [
        {{
            "name": "é¡¹ç›®åç§°",
            "role": "è§’è‰²",
            "description": "é¡¹ç›®æè¿°"
        }}
    ],
    "expected_salary": "æœŸæœ›è–ªèµ„ï¼ˆå¦‚æœæ²¡æœ‰å¡« nullï¼‰",
    "expected_location": "æœŸæœ›å·¥ä½œåœ°ç‚¹ï¼ˆå¦‚æœæ²¡æœ‰å¡« nullï¼‰",
    "extra_info": ["ç®€å†ä¸­çš„å…¶ä»–é‡è¦ä¿¡æ¯ï¼Œå¦‚è¯ä¹¦ã€è·å¥–ç­‰"]
}}

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

    try:
        parsed_data = await extractor.generate_json(parse_prompt)
        
        if not parsed_data:
            raise HTTPException(status_code=500, detail="AI è§£æå¤±è´¥ï¼Œæ— æ³•æå–ç»“æ„åŒ–ä¿¡æ¯")
        
        # è·å–æˆ–åˆ›å»ºç”¨æˆ·èµ„æ–™
        profile_query = select(UserProfile).where(
            UserProfile.user_id == user_id,
            UserProfile.profile_type == ProfileType.CANDIDATE
        )
        result = await db.execute(profile_query)
        profile = result.scalar_one_or_none()
        
        if not profile:
            profile = UserProfile(
                user_id=user_id,
                profile_type=ProfileType.CANDIDATE,
                candidate_data={}
            )
            db.add(profile)
        
        # æ›´æ–°èµ„æ–™å­—æ®µ
        updates_made = []
        
        if parsed_data.get('display_name'):
            profile.display_name = parsed_data['display_name']
            updates_made.append('å§“å')
        
        if parsed_data.get('title'):
            profile.title = parsed_data['title']
            updates_made.append('èŒä½')
        
        if parsed_data.get('summary'):
            profile.summary = parsed_data['summary']
            updates_made.append('ä¸ªäººç®€ä»‹')
        
        # æ›´æ–° candidate_data
        candidate_data = dict(profile.candidate_data or {})
        
        if parsed_data.get('skills'):
            candidate_data['skills'] = parsed_data['skills']
            updates_made.append('æŠ€èƒ½')
        
        if parsed_data.get('experience'):
            candidate_data['experience'] = parsed_data['experience']
            updates_made.append('å·¥ä½œç»å†')
        
        if parsed_data.get('education'):
            candidate_data['education'] = parsed_data['education']
            updates_made.append('æ•™è‚²èƒŒæ™¯')
        
        if parsed_data.get('projects'):
            candidate_data['projects'] = parsed_data['projects']
            updates_made.append('é¡¹ç›®ç»å†')
        
        if parsed_data.get('expected_salary'):
            candidate_data['expected_salary'] = parsed_data['expected_salary']
            updates_made.append('æœŸæœ›è–ªèµ„')
        
        if parsed_data.get('expected_location'):
            candidate_data['expected_location'] = parsed_data['expected_location']
            updates_made.append('æœŸæœ›åœ°ç‚¹')
        
        profile.candidate_data = candidate_data
        flag_modified(profile, 'candidate_data')
        
        # ä¿å­˜é¢å¤–ä¿¡æ¯åˆ°è®°å¿†
        memories_created = []
        extra_info = parsed_data.get('extra_info', [])
        
        if extra_info:
            for info in extra_info[:5]:  # æœ€å¤šä¿å­˜5æ¡é¢å¤–ä¿¡æ¯
                if info and len(info) > 10:
                    memory = Memory(
                        user_id=user_id,
                        scope=MemoryScope.CANDIDATE,
                        type=MemoryType.PREFERENCE,
                        content=info,
                        importance=MemoryImportance.MEDIUM,
                        ai_reasoning="ä»ç®€å†ä¸­è‡ªåŠ¨æå–çš„é¢å¤–ä¿¡æ¯"
                    )
                    db.add(memory)
                    memories_created.append(info[:50] + '...' if len(info) > 50 else info)
        
        await db.commit()
        
        # è®¡ç®—å®Œå–„åº¦
        total_fields = 9
        filled_fields = 0
        if profile.display_name: filled_fields += 1
        if profile.title: filled_fields += 1
        if profile.summary and len(profile.summary) >= 20: filled_fields += 1
        if candidate_data.get('skills'): filled_fields += 1
        if candidate_data.get('experience'): filled_fields += 1
        if candidate_data.get('education'): filled_fields += 1
        if candidate_data.get('projects'): filled_fields += 1
        if candidate_data.get('expected_salary'): filled_fields += 1
        if candidate_data.get('expected_location'): filled_fields += 1
        
        completeness = round((filled_fields / total_fields) * 100)
        
        return {
            "success": True,
            "parsed_data": parsed_data,
            "updates_made": updates_made,
            "memories_created": memories_created,
            "completeness": completeness,
            "message": f"å·²è‡ªåŠ¨å¡«å…… {len(updates_made)} ä¸ªå­—æ®µï¼Œåˆ›å»º {len(memories_created)} æ¡è®°å¿†ï¼Œç®€å†å®Œå–„åº¦ï¼š{completeness}%"
        }
        
    except json.JSONDecodeError as e:
        print(f"JSON è§£æé”™è¯¯: {e}")
        raise HTTPException(status_code=500, detail="AI è¿”å›æ ¼å¼é”™è¯¯ï¼Œè¯·é‡è¯•")
    except Exception as e:
        print(f"è‡ªåŠ¨å¡«å……å¤±è´¥: {e}")
        await db.rollback()
        raise HTTPException(status_code=500, detail=f"è‡ªåŠ¨å¡«å……å¤±è´¥: {str(e)}")


# ============ å²—ä½å‘å¸ƒï¼ˆå…¬å¼€æ¥å£ï¼‰ ============

from pydantic import BaseModel as PydanticBaseModel, Field as PydanticField
from typing import List as TypingList

class PublicJobCreate(PydanticBaseModel):
    """å…¬å¼€åˆ›å»ºå²—ä½è¯·æ±‚"""
    user_id: int
    title: str
    company: str
    location: str
    description: str
    salary_min: Optional[int] = None
    salary_max: Optional[int] = None
    tags: TypingList[str] = []

@router.post("/jobs")
async def create_public_job(
    job_in: PublicJobCreate,
    db: AsyncSession = Depends(get_db)
):
    """
    å…¬å¼€æ¥å£åˆ›å»ºå²—ä½ï¼ˆé€šè¿‡ user_id é‰´æƒï¼‰
    """
    from app.models.user import User
    
    # éªŒè¯ç”¨æˆ·
    result = await db.execute(select(User).where(User.id == job_in.user_id))
    user = result.scalar_one_or_none()
    if not user:
        raise HTTPException(status_code=404, detail="ç”¨æˆ·ä¸å­˜åœ¨")
    
    # åˆ›å»ºå²—ä½
    job = Job(
        title=job_in.title,
        company=job_in.company,
        location=job_in.location,
        description=job_in.description,
        salary_min=job_in.salary_min,
        salary_max=job_in.salary_max,
        owner_id=user.id,
        status=JobStatus.ACTIVE
    )
    
    # å¤„ç†æ ‡ç­¾
    if job_in.tags:
        for tag_name in job_in.tags:
            tag_result = await db.execute(select(JobTag).where(JobTag.name == tag_name))
            tag = tag_result.scalar_one_or_none()
            if not tag:
                tag = JobTag(name=tag_name)
                db.add(tag)
            job.tags.append(tag)
    
    db.add(job)
    await db.commit()
    await db.refresh(job)
    
    # è‡ªåŠ¨å†™å…¥å²—ä½å‘å¸ƒæ—¥å¿—
    try:
        from app.models.job import JobLog, JobLogAction
        import json as json_mod
        publish_log = JobLog(
            job_id=job.id,
            actor_id=user.id,
            actor_type="user",
            action=JobLogAction.PUBLISH,
            title="å²—ä½å‘å¸ƒæˆåŠŸ",
            content=f"å²—ä½ã€Œ{job.title}ã€å·²æˆåŠŸå‘å¸ƒï¼Œåœ°ç‚¹ï¼š{job.location}ï¼Œè–ªèµ„ï¼š{job.salary_min or 'é¢è®®'}-{job.salary_max or 'é¢è®®'}",
            extra_data=json_mod.dumps({
                "job_title": job.title,
                "company": job.company,
                "location": job.location,
                "salary_min": job.salary_min,
                "salary_max": job.salary_max,
                "tags": job_in.tags,
            }),
        )
        db.add(publish_log)
        await db.commit()
    except Exception as e:
        print(f"[JobLog] å†™å…¥å‘å¸ƒæ—¥å¿—å¤±è´¥: {e}")
    
    return {
        "id": job.id,
        "title": job.title,
        "company": job.company,
        "location": job.location,
        "description": job.description,
        "salary_min": job.salary_min,
        "salary_max": job.salary_max,
        "status": job.status.value if job.status else "active",
        "created_at": job.created_at.isoformat() if job.created_at else None
    }


@router.get("/my-jobs")
async def list_my_jobs(
    user_id: int = Query(...),
    db: AsyncSession = Depends(get_db)
):
    """è·å–ç”¨æˆ·å‘å¸ƒçš„å²—ä½åˆ—è¡¨"""
    result = await db.execute(
        select(Job).options(selectinload(Job.tags))
        .where(Job.owner_id == user_id)
        .order_by(Job.created_at.desc())
    )
    jobs = result.scalars().all()
    
    return [
        {
            "id": j.id,
            "title": j.title,
            "company": j.company,
            "location": j.location,
            "description": j.description,
            "salary_min": j.salary_min,
            "salary_max": j.salary_max,
            "status": j.status.value if j.status else "active",
            "tags": [t.name for t in j.tags] if j.tags else [],
            "view_count": j.view_count or 0,
            "apply_count": j.apply_count or 0,
            "created_at": j.created_at.isoformat() if j.created_at else None,
        }
        for j in jobs
    ]


class PublicJobUpdate(PydanticBaseModel):
    """å…¬å¼€æ›´æ–°å²—ä½è¯·æ±‚"""
    user_id: int
    title: Optional[str] = None
    company: Optional[str] = None
    location: Optional[str] = None
    description: Optional[str] = None
    salary_min: Optional[int] = None
    salary_max: Optional[int] = None
    status: Optional[str] = None
    tags: Optional[TypingList[str]] = None


@router.put("/jobs/{job_id}")
async def update_public_job(
    job_id: int,
    job_in: PublicJobUpdate,
    db: AsyncSession = Depends(get_db)
):
    """æ›´æ–°å²—ä½"""
    result = await db.execute(
        select(Job).options(selectinload(Job.tags)).where(Job.id == job_id)
    )
    job = result.scalar_one_or_none()
    if not job:
        raise HTTPException(status_code=404, detail="å²—ä½ä¸å­˜åœ¨")
    if job.owner_id != job_in.user_id:
        raise HTTPException(status_code=403, detail="æ— æƒä¿®æ”¹æ­¤å²—ä½")
    
    if job_in.title is not None: job.title = job_in.title
    if job_in.company is not None: job.company = job_in.company
    if job_in.location is not None: job.location = job_in.location
    if job_in.description is not None: job.description = job_in.description
    if job_in.salary_min is not None: job.salary_min = job_in.salary_min
    if job_in.salary_max is not None: job.salary_max = job_in.salary_max
    if job_in.status is not None:
        job.status = JobStatus(job_in.status) if job_in.status in [s.value for s in JobStatus] else job.status
    if job_in.tags is not None:
        job.tags.clear()
        for tag_name in job_in.tags:
            tag_result = await db.execute(select(JobTag).where(JobTag.name == tag_name))
            tag = tag_result.scalar_one_or_none()
            if not tag:
                tag = JobTag(name=tag_name)
                db.add(tag)
            job.tags.append(tag)
    
    await db.commit()
    await db.refresh(job)
    return {"ok": True, "id": job.id}


@router.delete("/jobs/{job_id}")
async def delete_public_job(
    job_id: int,
    user_id: int = Query(...),
    db: AsyncSession = Depends(get_db)
):
    """åˆ é™¤å²—ä½"""
    result = await db.execute(select(Job).where(Job.id == job_id))
    job = result.scalar_one_or_none()
    if not job:
        raise HTTPException(status_code=404, detail="å²—ä½ä¸å­˜åœ¨")
    if job.owner_id != user_id:
        raise HTTPException(status_code=403, detail="æ— æƒåˆ é™¤æ­¤å²—ä½")
    
    await db.delete(job)
    await db.commit()
    return {"ok": True}


@router.get("/job-detail/{job_id}")
async def get_job_detail_with_applications(
    job_id: int,
    user_id: int = Query(...),
    db: AsyncSession = Depends(get_db)
):
    """è·å–å²—ä½è¯¦æƒ…åŠæŠ•é€’åˆ—è¡¨ â€” åˆå¹¶ Flow è®°å½• + JobLog ä¸­çš„å€™é€‰äººæ•°æ®"""
    from app.models.user import User
    
    # è·å–å²—ä½ä¿¡æ¯
    result = await db.execute(
        select(Job).options(selectinload(Job.tags)).where(Job.id == job_id)
    )
    job = result.scalar_one_or_none()
    if not job:
        raise HTTPException(status_code=404, detail="å²—ä½ä¸å­˜åœ¨")
    if job.owner_id != user_id:
        raise HTTPException(status_code=403, detail="æ— æƒæŸ¥çœ‹æ­¤å²—ä½")
    
    # ========== 1. ä» Flow è¡¨è·å–çœŸå®å€™é€‰äºº ==========
    flows_result = await db.execute(
        select(Flow)
        .options(selectinload(Flow.steps), selectinload(Flow.timeline))
        .where(Flow.job_id == job_id)
        .order_by(Flow.created_at.desc())
    )
    flows = flows_result.scalars().all()
    
    applications = []
    flow_candidate_ids = set()  # è®°å½•å·²åœ¨ Flow ä¸­çš„å€™é€‰äºº ID
    
    # é¢„è¯»å– screen_result æ—¥å¿—ï¼ˆç”¨äºä¸º Flow å€™é€‰äººè¡¥å……ç­›é€‰è¯¦æƒ…ï¼‰
    flow_screen_logs_result = await db.execute(
        select(JobLog)
        .where(JobLog.job_id == job_id, JobLog.action == JobLogAction.SCREEN_RESULT)
        .order_by(JobLog.created_at.desc())
        .limit(5)
    )
    flow_screen_logs = flow_screen_logs_result.scalars().all()
    
    # æ„å»º å€™é€‰äººåå­— -> ç­›é€‰ç»“æœ æ˜ å°„
    flow_screen_map = {}
    for fsl in flow_screen_logs:
        try:
            fsl_extra = json.loads(fsl.extra_data) if isinstance(fsl.extra_data, str) else (fsl.extra_data or {})
            for fr in fsl_extra.get("results", []):
                flow_screen_map[fr.get("name", "")] = fr
        except Exception:
            pass
    
    for flow in flows:
        cand_result = await db.execute(
            select(Candidate).where(Candidate.id == flow.candidate_id)
        )
        candidate = cand_result.scalar_one_or_none()
        
        profile = None
        user_info = None
        if candidate:
            prof_result = await db.execute(
                select(CandidateProfile).where(CandidateProfile.candidate_id == candidate.id)
            )
            profile = prof_result.scalar_one_or_none()
            user_result = await db.execute(
                select(User).where(User.id == candidate.user_id)
            )
            user_info = user_result.scalar_one_or_none()
        
        flow_candidate_ids.add(flow.candidate_id)
        
        # ä»ç­›é€‰æ—¥å¿—ä¸­æŸ¥æ‰¾è¯¥å€™é€‰äººçš„ç­›é€‰ç»“æœ
        cand_name = profile.display_name if profile else (user_info.name if user_info else f"å€™é€‰äºº#{flow.candidate_id}")
        fsr = flow_screen_map.get(cand_name, {})
        
        # æ ¹æ® Flow çŠ¶æ€æ¨æ–­ both_passï¼ˆACCEPTED/EVALUATING ä¸”åœ¨ FINAL é˜¶æ®µè§†ä¸ºåŒæ–¹é€šè¿‡ï¼‰
        is_both_pass = fsr.get("both_pass", False)
        if not is_both_pass and flow.status and flow.status.value in ("accepted", "evaluating") and flow.current_stage and flow.current_stage.value == "final":
            is_both_pass = True
        
        screen_result_data = None
        if fsr:
            screen_result_data = {
                "employer_pass": fsr.get("employer_pass") or is_both_pass,
                "employer_score": fsr.get("employer_score"),
                "candidate_pass": fsr.get("candidate_pass") or is_both_pass,
                "candidate_interest": fsr.get("candidate_interest"),
                "both_pass": is_both_pass,  # Flow çŠ¶æ€ä¼˜å…ˆï¼šaccepted+final è§†ä¸ºåŒæ–¹é€šè¿‡
                "final_status": "åŒæ–¹é€šè¿‡" if is_both_pass else fsr.get("final_status"),
                "strengths": fsr.get("strengths", []),
                "concerns": fsr.get("concerns", []),
            }
        elif is_both_pass:
            # Flow çŠ¶æ€è¡¨æ˜é€šè¿‡ä½†æ²¡æœ‰ç­›é€‰æ—¥å¿— â†’ æ ¹æ® Flow details æ„å»º
            screen_result_data = {
                "employer_pass": True,
                "candidate_pass": True,
                "both_pass": True,
                "final_status": "åŒæ–¹é€šè¿‡",
                "employer_score": 0,
                "candidate_interest": 0,
                "strengths": [],
                "concerns": [],
            }
            # å°è¯•ä» flow.details è§£æåˆ†æ•°
            if flow.details:
                import re
                es_match = re.search(r"ä¼ä¸šè¯„åˆ†\s*(\d+)", flow.details)
                ci_match = re.search(r"å€™é€‰äººæ„å‘\s*(\d+)", flow.details)
                if es_match:
                    screen_result_data["employer_score"] = int(es_match.group(1))
                if ci_match:
                    screen_result_data["candidate_interest"] = int(ci_match.group(1))
        
        applications.append({
            "flow_id": flow.id,
            "candidate_id": flow.candidate_id,
            "source": "real",
            "status": flow.status.value if flow.status else "unknown",
            "current_stage": flow.current_stage.value if flow.current_stage else "unknown",
            "current_step": flow.current_step,
            "match_score": flow.match_score or 0,
            "tokens_consumed": flow.tokens_consumed or 0,
            "next_action": flow.next_action,
            "details": flow.details,
            "last_action": flow.last_action,
            "created_at": flow.created_at.isoformat() if flow.created_at else None,
            "updated_at": flow.updated_at.isoformat() if flow.updated_at else None,
            "candidate_name": cand_name,
            "candidate_role": profile.current_role if profile else None,
            "candidate_avatar": user_info.avatar_url if user_info else None,
            "candidate_experience": profile.experience_years if profile else None,
            "candidate_summary": profile.summary if profile else None,
            "candidate_email": user_info.email if user_info else None,
            "candidate_phone": user_info.phone if user_info else None,
            "candidate_wechat": user_info.phone if user_info else None,
            "screen_result": screen_result_data,
        })
    
    # ========== 2. ä» JobLog ä¸­è¡¥å…… AI æ¨¡æ‹Ÿå€™é€‰äººï¼ˆæ²¡æœ‰ Flow è®°å½•çš„ï¼‰ ==========
    # è¯»å– invite_match æ—¥å¿—è·å–é‚€è¯·é˜¶æ®µçš„å€™é€‰äºº
    invite_logs_result = await db.execute(
        select(JobLog)
        .where(JobLog.job_id == job_id, JobLog.action == JobLogAction.INVITE_MATCH)
        .order_by(JobLog.created_at.desc())
        .limit(5)
    )
    invite_logs = invite_logs_result.scalars().all()
    
    # è¯»å– screen_result æ—¥å¿—è·å–ç­›é€‰ç»“æœ
    screen_logs_result = await db.execute(
        select(JobLog)
        .where(JobLog.job_id == job_id, JobLog.action == JobLogAction.SCREEN_RESULT)
        .order_by(JobLog.created_at.desc())
        .limit(5)
    )
    screen_logs = screen_logs_result.scalars().all()
    
    # æ„å»ºç­›é€‰ç»“æœæ˜ å°„ { name -> result }
    screen_map = {}
    for sl in screen_logs:
        try:
            extra = json.loads(sl.extra_data) if isinstance(sl.extra_data, str) else (sl.extra_data or {})
            for r in extra.get("results", []):
                screen_map[r.get("name", "")] = r
        except Exception:
            pass
    
    # é¢„åŠ è½½æ‰€æœ‰å€™é€‰äººçš„è”ç³»æ–¹å¼ï¼ˆç”¨äº AI æ¨¡æ‹Ÿå€™é€‰äººæŸ¥è¯¢ï¼‰
    from app.models.user import User as UserModel
    candidate_contact_cache = {}  # candidate_id -> {email, phone}
    
    async def get_candidate_contact(cand_id):
        """ä»æ•°æ®åº“æŸ¥è¯¢å€™é€‰äººè”ç³»æ–¹å¼ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        if not cand_id:
            return None, None
        if cand_id in candidate_contact_cache:
            return candidate_contact_cache[cand_id]["email"], candidate_contact_cache[cand_id]["phone"]
        try:
            cand_res = await db.execute(select(Candidate).where(Candidate.id == cand_id))
            cand = cand_res.scalar_one_or_none()
            if cand:
                u_res = await db.execute(select(UserModel).where(UserModel.id == cand.user_id))
                u = u_res.scalar_one_or_none()
                if u:
                    candidate_contact_cache[cand_id] = {"email": u.email, "phone": u.phone}
                    return u.email, u.phone
        except Exception:
            pass
        candidate_contact_cache[cand_id] = {"email": None, "phone": None}
        return None, None
    
    async def ensure_ai_candidate_exists(c_name: str, c_data: dict, sr_data: dict) -> tuple:
        """ç¡®ä¿ AI æ¨¡æ‹Ÿå€™é€‰äººåœ¨æ•°æ®åº“ä¸­å­˜åœ¨ï¼Œè¿”å› (candidate_id, email, phone)"""
        from app.models.user import User as UModel, UserRole as URole
        from app.models.candidate import Candidate as CModel, CandidateProfile as PModel
        from app.models.profile import UserProfile as UPModel, ProfileType as PType
        from app.utils.security import get_password_hash
        
        if not c_name:
            return None, None, None
        
        # æŒ‰åå­—æ„å»º email æŸ¥è¯¢
        email_slug = c_name.replace(" ", "").lower()
        mock_email = f"{email_slug}@ai-mock.dev"
        
        exists_res = await db.execute(select(UModel).where(UModel.email == mock_email))
        existing = exists_res.scalar_one_or_none()
        if existing:
            cand_res = await db.execute(select(CModel).where(CModel.user_id == existing.id))
            cand = cand_res.scalar_one_or_none()
            cand_id = cand.id if cand else None
            return cand_id, existing.email, existing.phone
        
        # åˆ›å»ºæ–°çš„ mock ç”¨æˆ·å’Œå€™é€‰äºº
        try:
            mock_phone = f"138{hash(c_name) % 100000000:08d}"
            role_title = c_data.get("role") or c_data.get("current_role") or c_data.get("title") or "AIæ¨èå€™é€‰äºº"
            
            exp_raw = c_data.get("experience_years") or c_data.get("experience", "3")
            try:
                exp_val = float(str(exp_raw).replace("å¹´", "").strip())
            except (ValueError, TypeError):
                exp_val = 3.0
            
            summary_text = sr_data.get("employer_analysis", "") or c_data.get("highlight", "") or c_data.get("match_reason", "") or f"{c_name}ï¼ŒAIæ¨èçš„ä¼˜è´¨å€™é€‰äººã€‚"
            match_score = c_data.get("match_score", 80)
            skills = c_data.get("skills", [])
            if not skills:
                skills = ["æ²Ÿé€šèƒ½åŠ›", "å›¢é˜Ÿåä½œ", "é—®é¢˜è§£å†³", "é¡¹ç›®ç®¡ç†"]
            
            user = UModel(
                email=mock_email,
                hashed_password=get_password_hash("ai_mock_pwd"),
                name=c_name,
                phone=mock_phone,
                role=URole.CANDIDATE,
                is_active=True, is_verified=True,
            )
            db.add(user)
            await db.flush()
            
            cand = CModel(
                user_id=user.id,
                resume_text=summary_text,
                is_profile_complete=True,
            )
            db.add(cand)
            await db.flush()
            
            profile = PModel(
                candidate_id=cand.id,
                display_name=c_name,
                current_role=role_title,
                experience_years=exp_val,
                summary=summary_text,
                ideal_job_persona=c_data.get("highlight", ""),
                salary_range="é¢è®®",
                market_demand=f"AI æ™ºèƒ½æ¨èå€™é€‰äººï¼ŒåŒ¹é…åˆ† {match_score}%",
                radar_data={
                    "æŠ€æœ¯æ·±åº¦": min(95, match_score),
                    "é¡¹ç›®ç»éªŒ": min(90, match_score - 5),
                    "æ²Ÿé€šåä½œ": 75, "å­¦ä¹ èƒ½åŠ›": 80, "è¡Œä¸šè®¤çŸ¥": 70,
                },
                interview_questions=[
                    "è¯·ä»‹ç»ä½ æœ€æœ‰æŒ‘æˆ˜æ€§çš„é¡¹ç›®ç»å†ã€‚",
                    "ä½ å¦‚ä½•çœ‹å¾…å½“å‰è¡Œä¸šçš„æŠ€æœ¯å‘å±•è¶‹åŠ¿ï¼Ÿ",
                    "æè¿°ä¸€æ¬¡ä½ è§£å†³å¤æ‚æŠ€æœ¯é—®é¢˜çš„è¿‡ç¨‹ã€‚",
                ],
                optimization_suggestions=[
                    "å»ºè®®ä¸°å¯Œé¡¹ç›®æ¡ˆä¾‹æè¿°",
                    "å¯ä»¥è¡¥å……è¡Œä¸šè®¤è¯ä¿¡æ¯",
                    "å¢åŠ æ•°æ®é‡åŒ–çš„æˆæœå±•ç¤º",
                ],
                certifications=[{"name": "è¡Œä¸šä»ä¸šèµ„æ ¼", "issuer": "äººç¤¾éƒ¨", "date": "2024-01"}],
                awards=[],
            )
            db.add(profile)
            
            # åˆ›å»ºæŠ€èƒ½è®°å½•
            from app.models.candidate import Skill as SkillModel
            for sk in skills[:6]:
                db.add(SkillModel(candidate_id=cand.id, name=sk, level=70 + hash(sk) % 25, category="æŠ€æœ¯"))
            
            # åˆ›å»º UserProfile è®°å½•ï¼ˆå«æ•™è‚²/å·¥ä½œ/é¡¹ç›®ï¼‰
            user_profile = UPModel(
                user_id=user.id,
                profile_type=PType.CANDIDATE,
                display_name=c_name,
                title=role_title,
                summary=summary_text,
                candidate_data={
                    "skills": skills,
                    "experience_years": exp_val,
                    "current_role": role_title,
                    "summary": summary_text,
                    "ideal_job": c_data.get("highlight", ""),
                    "expected_salary": "é¢è®®",
                    "expected_location": "ä¸é™",
                    "radar_data": [
                        {"subject": "æŠ€æœ¯æ·±åº¦", "value": min(95, match_score)},
                        {"subject": "é¡¹ç›®ç»éªŒ", "value": min(90, match_score - 5)},
                        {"subject": "æ²Ÿé€šåä½œ", "value": 75},
                        {"subject": "å­¦ä¹ èƒ½åŠ›", "value": 80},
                        {"subject": "è¡Œä¸šè®¤çŸ¥", "value": 70},
                    ],
                    "education": [
                        {"school": "åŒ—äº¬ç†å·¥å¤§å­¦", "major": "è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯", "degree": "æœ¬ç§‘", "period": "2016 - 2020"},
                    ],
                    "experience": [
                        {"company": "æŸäº’è”ç½‘å…¬å¸", "position": role_title, "period": f"2020 - è‡³ä»Š", "description": summary_text[:100]},
                    ],
                    "projects": [
                        {"name": "æ ¸å¿ƒä¸šåŠ¡ç³»ç»Ÿé‡æ„", "role": role_title, "description": f"è´Ÿè´£æ ¸å¿ƒä¸šåŠ¡æ¨¡å—è®¾è®¡ä¸å¼€å‘ï¼Œè¿ç”¨{'/'.join(skills[:3])}ç­‰æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡ç³»ç»Ÿæ€§èƒ½å’Œç”¨æˆ·ä½“éªŒã€‚"},
                    ],
                    "career_path": [],
                    "certifications": [],
                    "awards": [],
                },
            )
            db.add(user_profile)
            
            await db.commit()
            
            return cand.id, mock_email, mock_phone
        except Exception:
            await db.rollback()
            return None, None, None
    
    # ä»é‚€è¯·æ—¥å¿—ä¸­æå– AI æ¨¡æ‹Ÿå€™é€‰äººï¼ˆsource != real ä¸”ä¸åœ¨ flow_candidate_ids ä¸­ï¼‰
    seen_names = {a["candidate_name"] for a in applications}
    for il in invite_logs:
        try:
            extra = json.loads(il.extra_data) if isinstance(il.extra_data, str) else (il.extra_data or {})
            for c in extra.get("candidates", []):
                c_name = c.get("name", "")
                c_id = c.get("id")
                # è·³è¿‡å·²æœ‰ Flow è®°å½•çš„
                if c_id and c_id in flow_candidate_ids:
                    continue
                # è·³è¿‡å·²æ·»åŠ çš„åŒåå€™é€‰äºº
                if c_name in seen_names:
                    continue
                
                # ä»ç­›é€‰ç»“æœä¸­æŸ¥æ‰¾è¯¥å€™é€‰äººçš„çŠ¶æ€
                sr = screen_map.get(c_name, {})
                
                # æ¨æ–­çŠ¶æ€
                if sr.get("both_pass"):
                    sim_status = "accepted"
                    sim_stage = "final"
                    sim_last_action = "æ™ºèƒ½ç­›é€‰ - åŒæ–¹é€šè¿‡"
                elif sr.get("employer_pass"):
                    sim_status = "screening"
                    sim_stage = "benchmark"
                    sim_last_action = "æ™ºèƒ½ç­›é€‰ - ä¼ä¸šé€šè¿‡/å€™é€‰äººæœªç¡®è®¤"
                elif sr:
                    sim_status = "rejected"
                    sim_stage = "benchmark"
                    sim_last_action = f"æ™ºèƒ½ç­›é€‰ - {sr.get('final_status', 'æœªé€šè¿‡')}"
                else:
                    sim_status = "screening"
                    sim_stage = "parse"
                    sim_last_action = "æ™ºèƒ½é‚€è¯· - å·²å‘é€"
                
                # æŸ¥è¯¢æˆ–è‡ªåŠ¨åˆ›å»ºå€™é€‰äººè®°å½•å¹¶è·å–è”ç³»æ–¹å¼
                if c_id:
                    c_email, c_phone = await get_candidate_contact(c_id)
                else:
                    # AI æ¨¡æ‹Ÿå€™é€‰äººæ—  IDï¼Œè‡ªåŠ¨åˆ›å»ºæ•°æ®åº“è®°å½•
                    c_id, c_email, c_phone = await ensure_ai_candidate_exists(c_name, c, sr)
                
                applications.append({
                    "flow_id": None,
                    "candidate_id": c_id,
                    "source": c.get("source", "ai_simulated"),
                    "status": sim_status,
                    "current_stage": sim_stage,
                    "current_step": 1,
                    "match_score": c.get("match_score", 0),
                    "tokens_consumed": 0,
                    "next_action": None,
                    "details": sr.get("employer_analysis", "") or c.get("match_reason", ""),
                    "last_action": sim_last_action,
                    "created_at": il.created_at.isoformat() if il.created_at else None,
                    "updated_at": il.created_at.isoformat() if il.created_at else None,
                    "candidate_name": c_name,
                    "candidate_role": c.get("role") or c.get("current_role"),
                    "candidate_avatar": None,
                    "candidate_experience": c.get("experience_years"),
                    "candidate_summary": sr.get("employer_analysis") or c.get("match_reason"),
                    "candidate_email": c_email,
                    "candidate_phone": c_phone,
                    "candidate_wechat": c_phone,
                    # é¢å¤–å­—æ®µï¼šç­›é€‰è¯¦æƒ…
                    "screen_result": {
                        "employer_pass": sr.get("employer_pass"),
                        "employer_score": sr.get("employer_score"),
                        "candidate_pass": sr.get("candidate_pass"),
                        "candidate_interest": sr.get("candidate_interest"),
                        "both_pass": sr.get("both_pass"),
                        "final_status": sr.get("final_status"),
                        "strengths": sr.get("strengths", []),
                        "concerns": sr.get("concerns", []),
                    } if sr else None,
                })
                seen_names.add(c_name)
        except Exception:
            pass
    
    # ========== 3. ä» screen_result æ—¥å¿—è¡¥å……é—æ¼å€™é€‰äºº ==========
    for sl in screen_logs:
        try:
            extra = json.loads(sl.extra_data) if isinstance(sl.extra_data, str) else (sl.extra_data or {})
            for r in extra.get("results", []):
                c_name = r.get("name", "")
                if c_name in seen_names:
                    continue
                
                if r.get("both_pass"):
                    sim_status = "accepted"
                    sim_stage = "final"
                    sim_last_action = "æ™ºèƒ½ç­›é€‰ - åŒæ–¹é€šè¿‡"
                elif r.get("employer_pass"):
                    sim_status = "screening"
                    sim_stage = "benchmark"
                    sim_last_action = "æ™ºèƒ½ç­›é€‰ - ä¼ä¸šé€šè¿‡/å€™é€‰äººæœªç¡®è®¤"
                else:
                    sim_status = "rejected"
                    sim_stage = "benchmark"
                    sim_last_action = f"æ™ºèƒ½ç­›é€‰ - {r.get('final_status', 'æœªé€šè¿‡')}"
                
                # æŸ¥è¯¢æˆ–è‡ªåŠ¨åˆ›å»ºå€™é€‰äººè®°å½•å¹¶è·å–è”ç³»æ–¹å¼
                sr_c_id = r.get("id")
                if sr_c_id:
                    sr_email, sr_phone = await get_candidate_contact(sr_c_id)
                else:
                    sr_c_id, sr_email, sr_phone = await ensure_ai_candidate_exists(c_name, r, r)
                
                applications.append({
                    "flow_id": None,
                    "candidate_id": sr_c_id,
                    "source": r.get("source", "ai_simulated"),
                    "status": sim_status,
                    "current_stage": sim_stage,
                    "current_step": 1,
                    "match_score": r.get("match_score", 0),
                    "tokens_consumed": 0,
                    "next_action": None,
                    "details": r.get("employer_analysis", ""),
                    "last_action": sim_last_action,
                    "created_at": sl.created_at.isoformat() if sl.created_at else None,
                    "updated_at": sl.created_at.isoformat() if sl.created_at else None,
                    "candidate_name": c_name,
                    "candidate_role": None,
                    "candidate_avatar": None,
                    "candidate_experience": None,
                    "candidate_summary": r.get("employer_analysis"),
                    "candidate_email": sr_email,
                    "candidate_phone": sr_phone,
                    "candidate_wechat": sr_phone,
                    "screen_result": {
                        "employer_pass": r.get("employer_pass"),
                        "employer_score": r.get("employer_score"),
                        "candidate_pass": r.get("candidate_pass"),
                        "candidate_interest": r.get("candidate_interest"),
                        "both_pass": r.get("both_pass"),
                        "final_status": r.get("final_status"),
                        "strengths": r.get("strengths", []),
                        "concerns": r.get("concerns", []),
                    },
                })
                seen_names.add(c_name)
        except Exception:
            pass
    
    # ç»Ÿè®¡æ•°æ®
    status_counts = {}
    for app in applications:
        s = app["status"]
        status_counts[s] = status_counts.get(s, 0) + 1
    
    return {
        "job": {
            "id": job.id,
            "title": job.title,
            "company": job.company,
            "location": job.location,
            "description": job.description,
            "salary_min": job.salary_min,
            "salary_max": job.salary_max,
            "status": job.status.value if job.status else "active",
            "tags": [t.name for t in job.tags] if job.tags else [],
            "view_count": job.view_count or 0,
            "apply_count": job.apply_count or 0,
            "created_at": job.created_at.isoformat() if job.created_at else None,
        },
        "applications": applications,
        "stats": {
            "total": len(applications),
            "status_counts": status_counts,
        }
    }


# ============ æ™ºèƒ½åŒ¹é… API ============

from app.models.job import JobLog, JobLogAction
from app.models.candidate import Skill


class SmartMatchRequest(BaseModel):
    """æ™ºèƒ½åŒ¹é…è¯·æ±‚"""
    job_ids: List[int]
    user_id: int
    extra_requirements: str = ""


@router.post("/smart-match")
async def smart_match_candidates(
    req: SmartMatchRequest,
    db: AsyncSession = Depends(get_db)
):
    """æ™ºèƒ½å€™é€‰äººåŒ¹é… â€” ç»“åˆæ•°æ®åº“çœŸå®äººæ‰ + AI æ¨¡æ‹Ÿï¼Œèå…¥ä¼ä¸šè®°å¿†"""
    import httpx
    from app.config import settings
    
    # 1. æŸ¥è¯¢å·²å‘å¸ƒçš„å²—ä½è¯¦æƒ…
    job_result = await db.execute(
        select(Job).options(selectinload(Job.tags)).where(Job.id.in_(req.job_ids))
    )
    jobs = job_result.scalars().all()
    if not jobs:
        return {"matches": [], "total_real": 0, "total_simulated": 0, "job_titles": [], "memory_context": ""}
    
    job_titles = [j.title for j in jobs]
    job_descriptions = []
    for j in jobs:
        tags_str = ", ".join([t.name for t in (j.tags or [])]) if j.tags else ""
        job_descriptions.append(
            f"å²—ä½: {j.title}\nåœ°ç‚¹: {j.location}\nè–ªèµ„: {j.salary_min or 'é¢è®®'}K-{j.salary_max or 'é¢è®®'}K\næè¿°: {(j.description or '')[:500]}\næ ‡ç­¾: {tags_str}"
        )
    jobs_context = "\n---\n".join(job_descriptions)
    
    # 2. æŸ¥è¯¢ä¼ä¸šè®°å¿†ï¼ˆrequirement ç±»å‹ + é«˜å¼ºè°ƒè®°å¿†ï¼‰
    memory_result = await db.execute(
        select(Memory).where(
            Memory.user_id == req.user_id,
            Memory.scope == MemoryScope.EMPLOYER
        )
    )
    all_memories = memory_result.scalars().all()
    # ç­›é€‰é«˜ä¼˜å…ˆè®°å¿†ï¼štype=requirement æˆ– emphasis_count >= 2
    important_memories = [
        m for m in all_memories
        if m.type == MemoryType.REQUIREMENT or m.emphasis_count >= 2
    ]
    memory_context = ""
    if important_memories:
        memory_lines = [f"- [{m.type.value}] {m.content}" for m in important_memories[:10]]
        memory_context = "ä¼ä¸šåå¥½/è¦æ±‚ï¼š\n" + "\n".join(memory_lines)
    
    # 3. æŸ¥è¯¢æ•°æ®åº“ä¸­çœŸå®çš„å€™é€‰äºº
    candidate_result = await db.execute(
        select(Candidate)
        .options(selectinload(Candidate.profile), selectinload(Candidate.skills))
        .where(Candidate.is_profile_complete == True)
        .limit(20)
    )
    db_candidates = candidate_result.scalars().all()
    
    real_candidates_info = []
    for c in db_candidates:
        if not c.profile:
            continue
        skills_list = [s.name for s in (c.skills or [])][:8]
        real_candidates_info.append({
            "id": c.id,
            "name": c.profile.display_name or "æœªçŸ¥",
            "title": c.profile.current_role or "æœªçŸ¥èŒä½",
            "experience_years": c.profile.experience_years or 0,
            "skills": skills_list,
            "summary": (c.profile.summary or "")[:200],
        })
    
    # 4. æ„å»º AI prompt â€” å¯¹çœŸå®å€™é€‰äººè¯„åˆ† + ç”Ÿæˆæ¨¡æ‹Ÿå€™é€‰äºº
    real_section = ""
    if real_candidates_info:
        real_lines = []
        for rc in real_candidates_info:
            real_lines.append(
                f'  {{"id": {rc["id"]}, "name": "{rc["name"]}", "title": "{rc["title"]}", '
                f'"experience": "{rc["experience_years"]}å¹´", "skills": {json.dumps(rc["skills"], ensure_ascii=False)}, '
                f'"summary": "{rc["summary"][:100]}"}}'
            )
        real_section = f"""
ä»¥ä¸‹æ˜¯æ•°æ®åº“ä¸­çš„çœŸå®å€™é€‰äººï¼ˆè¯·è¯„ä¼°æ¯ä½å€™é€‰äººä¸å²—ä½çš„åŒ¹é…åº¦ 0-100 åˆ†ï¼Œå¹¶ç»™å‡ºåŒ¹é…ç†ç”±å’Œä¸€å¥è¯äº®ç‚¹ï¼‰ï¼š
[
{chr(10).join(real_lines)}
]
"""
    
    extra_req_section = f"\nç”¨æˆ·é¢å¤–ç­›é€‰è¦æ±‚ï¼š{req.extra_requirements}" if req.extra_requirements else ""
    
    match_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ HR æ™ºèƒ½åŒ¹é…å¼•æ“ã€‚

ã€å²—ä½ä¿¡æ¯ã€‘
{jobs_context}

{memory_context}
{extra_req_section}
{real_section}

è¯·å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
1. å¯¹ä¸Šé¢çš„çœŸå®å€™é€‰äººé€ä¸€æ‰“åˆ†è¯„ä¼°ï¼ˆå¦‚æœ‰çš„è¯ï¼‰
2. å¦å¤–æ¨¡æ‹Ÿç”Ÿæˆ 3 ä¸ªé¢å¤–çš„ä¼˜è´¨å€™é€‰äººä½œä¸º AI æ¨è

ä¸¥æ ¼æŒ‰ä»¥ä¸‹ JSON æ ¼å¼è¿”å›ï¼ˆç›´æ¥è¿”å›JSONæ•°ç»„ï¼Œä¸è¦åŒ…å«markdownä»£ç å—æ ‡è®°ï¼‰ï¼š
[
  {{
    "id": null,
    "name": "å§“å",
    "title": "å½“å‰èŒä½",
    "experience": "Xå¹´ç»éªŒ",
    "match_score": 85,
    "highlight": "ä¸€å¥è¯äº®ç‚¹",
    "skills": ["æŠ€èƒ½1", "æŠ€èƒ½2"],
    "source": "database æˆ– ai_simulated",
    "match_reason": "åŒ¹é…ç†ç”±ï¼ˆ1-2å¥è¯ï¼‰"
  }}
]

è§„åˆ™ï¼š
- çœŸå®å€™é€‰äººçš„ id å¡«å†™å…¶å®é™… id æ•°å­—ï¼Œsource å¡« "database"
- AI æ¨¡æ‹Ÿå€™é€‰äººçš„ id å¡« nullï¼Œsource å¡« "ai_simulated"
- æŒ‰ match_score ä»é«˜åˆ°ä½æ’åº
- match_score è¦åˆç†ï¼Œä¸è¦å…¨ç»™é«˜åˆ†"""

    # 5. è°ƒç”¨ LLM
    matches = []
    minimax_api_key = settings.minimax_api_key or ""
    gemini_api_key = settings.gemini_api_key or ""
    
    ai_response_text = ""
    
    if minimax_api_key:
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    "https://api.minimax.chat/v1/text/chatcompletion_v2",
                    headers={
                        "Authorization": f"Bearer {minimax_api_key}",
                        "Content-Type": "application/json",
                    },
                    json={
                        "model": "abab6.5s-chat",
                        "messages": [
                            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ HR æ™ºèƒ½åŒ¹é…å¼•æ“ï¼Œåªè¿”å› JSON æ•°æ®ã€‚"},
                            {"role": "user", "content": match_prompt},
                        ],
                        "max_tokens": 2048,
                        "temperature": 0.7,
                    }
                )
                result = response.json()
                if result.get("base_resp", {}).get("status_code", 0) == 0:
                    if "choices" in result and len(result["choices"]) > 0:
                        ai_response_text = result["choices"][0].get("message", {}).get("content", "")
        except Exception as e:
            print(f"[smart-match] MiniMax error: {e}")
    
    if not ai_response_text and gemini_api_key:
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={gemini_api_key}",
                    json={
                        "contents": [{"parts": [{"text": match_prompt}]}],
                        "generationConfig": {"temperature": 0.7, "maxOutputTokens": 2048},
                    }
                )
                result = response.json()
                if "candidates" in result:
                    ai_response_text = result["candidates"][0].get("content", {}).get("parts", [{}])[0].get("text", "")
        except Exception as e:
            print(f"[smart-match] Gemini error: {e}")
    
    # 6. è§£æ AI è¿”å›
    if ai_response_text:
        try:
            cleaned = ai_response_text.strip()
            cleaned = cleaned.replace("```json", "").replace("```", "").strip()
            # æå– JSON æ•°ç»„
            import re
            json_match = re.search(r'\[[\s\S]*\]', cleaned)
            if json_match:
                matches = json.loads(json_match.group(0))
        except Exception as e:
            print(f"[smart-match] JSON parse error: {e}")
    
    # 7. å¦‚æœ AI è¿”å›ä¸ºç©ºï¼Œä½¿ç”¨ fallbackï¼ˆçœŸå®å€™é€‰äºº + é»˜è®¤æ¨¡æ‹Ÿï¼‰
    if not matches:
        # åŠ å…¥çœŸå®å€™é€‰äººï¼ˆé»˜è®¤ç»™ä¸ªåˆç†åˆ†æ•°ï¼‰
        for rc in real_candidates_info[:5]:
            matches.append({
                "id": rc["id"],
                "name": rc["name"],
                "title": rc["title"],
                "experience": f'{rc["experience_years"]}å¹´',
                "match_score": 70 + (rc["id"] % 20),
                "highlight": rc["summary"][:50] if rc["summary"] else "æ•°æ®åº“å€™é€‰äºº",
                "skills": rc["skills"][:5],
                "source": "database",
                "match_reason": "æŠ€èƒ½å’Œç»éªŒä¸å²—ä½éœ€æ±‚åŸºæœ¬åŒ¹é…",
            })
        # åŠ å…¥æ¨¡æ‹Ÿå€™é€‰äºº
        simulated_names = [
            {"name": "å¼ æŸæŸ", "title": "é«˜çº§å‰ç«¯å·¥ç¨‹å¸ˆ", "exp": "5å¹´", "score": 92, "hl": "å¤§å‚èƒŒæ™¯ï¼ŒReact/TS ä¸“å®¶", "skills": ["React", "TypeScript", "Node.js"]},
            {"name": "ææŸæŸ", "title": "å…¨æ ˆå·¥ç¨‹å¸ˆ", "exp": "4å¹´", "score": 87, "hl": "å¤šä¸ªç‹¬ç«‹é¡¹ç›®ç»éªŒ", "skills": ["Node.js", "React", "Python"]},
            {"name": "ç‹æŸæŸ", "title": "å‰ç«¯å¼€å‘å·¥ç¨‹å¸ˆ", "exp": "3å¹´", "score": 81, "hl": "æœ‰å›¢é˜Ÿç®¡ç†ç»éªŒ", "skills": ["Vue", "React", "CSS"]},
        ]
        for s in simulated_names:
            matches.append({
                "id": None,
                "name": s["name"],
                "title": s["title"],
                "experience": s["exp"],
                "match_score": s["score"],
                "highlight": s["hl"],
                "skills": s["skills"],
                "source": "ai_simulated",
                "match_reason": "AI æ ¹æ®å²—ä½è¦æ±‚æ¨èçš„ä¼˜è´¨å€™é€‰äºº",
            })
    
    # æŒ‰åŒ¹é…åº¦æ’åº
    matches.sort(key=lambda x: x.get("match_score", 0), reverse=True)
    
    total_real = sum(1 for m in matches if m.get("source") == "database")
    total_simulated = sum(1 for m in matches if m.get("source") == "ai_simulated")
    
    return {
        "matches": matches,
        "total_real": total_real,
        "total_simulated": total_simulated,
        "job_titles": job_titles,
        "memory_context": memory_context[:200] if memory_context else "",
    }


# ============ å¼‚æ­¥ä»»åŠ¡ API ============

# å†…å­˜ä¸­çš„å¼‚æ­¥ä»»åŠ¡çŠ¶æ€å­˜å‚¨ï¼ˆç”Ÿäº§ç¯å¢ƒå»ºè®®ç”¨ Redisï¼‰
_async_tasks: dict = {}

import asyncio
import uuid
from fastapi import BackgroundTasks


class AsyncTaskRequest(BaseModel):
    """å¼‚æ­¥ä»»åŠ¡è¯·æ±‚"""
    task_type: str  # "smart_invite" | "smart_screen"
    job_ids: List[int]
    user_id: int
    todo_id: Optional[int] = None
    extra_requirements: str = ""


def _get_task(task_id: str) -> dict:
    return _async_tasks.get(task_id, {})


def _set_task(task_id: str, data: dict):
    _async_tasks[task_id] = data


async def _run_smart_invite(task_id: str, job_ids: List[int], user_id: int, todo_id: Optional[int], extra_requirements: str):
    """åå°æ‰§è¡Œæ™ºèƒ½é‚€è¯·åŒ¹é…ï¼ˆç»†ç²’åº¦å®æ—¶è¿›åº¦å›ä¼ ï¼‰"""
    import httpx
    import re
    from app.config import settings
    
    _set_task(task_id, {"status": "running", "progress": 2, "stage": "init", "message": "æ­£åœ¨å¯åŠ¨æ™ºèƒ½åŒ¹é…å¼•æ“..."})
    
    async with AsyncSessionLocal() as db:
        try:
            # ===== é˜¶æ®µ 1ï¼šåŠ è½½å²—ä½æ•°æ® =====
            _set_task(task_id, {"status": "running", "progress": 5, "stage": "loading", "message": "æ­£åœ¨æŸ¥è¯¢å…³è”å²—ä½ä¿¡æ¯..."})
            
            job_result = await db.execute(
                select(Job).options(selectinload(Job.tags)).where(Job.id.in_(job_ids))
            )
            jobs = job_result.scalars().all()
            if not jobs:
                _set_task(task_id, {"status": "failed", "progress": 0, "message": "æœªæ‰¾åˆ°å…³è”çš„å²—ä½"})
                return
            
            job_titles = [j.title for j in jobs]
            _set_task(task_id, {"status": "running", "progress": 8, "stage": "loading",
                "message": f"å·²åŠ è½½ {len(jobs)} ä¸ªå²—ä½ï¼š{'ã€'.join(job_titles)}"})
            
            # å†™å…¥ invite_start æ—¥å¿—
            for j in jobs:
                log = JobLog(
                    job_id=j.id, actor_type="system", action=JobLogAction.INVITE_START,
                    title="å¼€å§‹æ™ºèƒ½å€™é€‰äººåŒ¹é…",
                    content=f"ç³»ç»Ÿå¯åŠ¨äº‘ç«¯å¼‚æ­¥æ™ºèƒ½åŒ¹é…ï¼Œå²—ä½ã€Œ{j.title}ã€{f'ï¼Œé¢å¤–è¦æ±‚ï¼š{extra_requirements}' if extra_requirements else ''}",
                    extra_data=json.dumps({"task_id": task_id, "extra_requirements": extra_requirements}),
                    todo_id=todo_id,
                )
                db.add(log)
            await db.commit()
            
            # ===== é˜¶æ®µ 2ï¼šåŠ è½½ä¼ä¸šè®°å¿† =====
            _set_task(task_id, {"status": "running", "progress": 12, "stage": "loading",
                "message": "æ­£åœ¨è¯»å–ä¼ä¸šè®°å¿†ä¸æ‹›è˜åå¥½..."})
            
            # æ„å»ºå²—ä½æè¿°
            job_descriptions = []
            for j in jobs:
                tags_str = ", ".join([t.name for t in (j.tags or [])]) if j.tags else ""
                job_descriptions.append(f"å²—ä½: {j.title}\nåœ°ç‚¹: {j.location}\nè–ªèµ„: {j.salary_min or 'é¢è®®'}K-{j.salary_max or 'é¢è®®'}K\næè¿°: {(j.description or '')[:500]}\næ ‡ç­¾: {tags_str}")
            jobs_context = "\n---\n".join(job_descriptions)
            
            memory_result = await db.execute(
                select(Memory).where(Memory.user_id == user_id, Memory.scope == MemoryScope.EMPLOYER)
            )
            all_memories = memory_result.scalars().all()
            important_memories = [m for m in all_memories if m.type == MemoryType.REQUIREMENT or m.emphasis_count >= 2]
            memory_context = ""
            mem_summary = "æ— ç‰¹æ®Šåå¥½"
            if important_memories:
                memory_lines = [f"- [{m.type.value}] {m.content}" for m in important_memories[:10]]
                memory_context = "ä¼ä¸šåå¥½/è¦æ±‚ï¼š\n" + "\n".join(memory_lines)
                mem_summary = f"{len(important_memories)} æ¡åå¥½/è¦æ±‚å·²æ³¨å…¥"
            
            _set_task(task_id, {"status": "running", "progress": 16, "stage": "loading",
                "message": f"ä¼ä¸šè®°å¿†ï¼š{mem_summary}\n\næ­£åœ¨æ‰«æäººæ‰åº“..."})
            
            # ===== é˜¶æ®µ 3ï¼šæ‰«æäººæ‰åº“ =====
            candidate_result = await db.execute(
                select(Candidate).options(selectinload(Candidate.profile), selectinload(Candidate.skills))
                .where(Candidate.is_profile_complete == True).limit(20)
            )
            db_candidates = candidate_result.scalars().all()
            
            real_candidates_info = []
            for c in db_candidates:
                if not c.profile:
                    continue
                real_candidates_info.append({
                    "id": c.id, "name": c.profile.display_name or "æœªçŸ¥",
                    "title": c.profile.current_role or "æœªçŸ¥èŒä½",
                    "experience_years": c.profile.experience_years or 0,
                    "skills": [s.name for s in (c.skills or [])][:8],
                    "summary": (c.profile.summary or "")[:200],
                })
            
            # é€ä¸€å±•ç¤ºå‘ç°çš„å€™é€‰äºº
            scan_lines = []
            for idx, rc in enumerate(real_candidates_info):
                skill_tags = "ã€".join(rc["skills"][:4])
                scan_lines.append(f"Â· {rc['name']} â€” {rc['title']}ï¼ˆ{rc['experience_years']}å¹´ | {skill_tags}ï¼‰")
                _set_task(task_id, {"status": "running", "progress": 18 + int((idx + 1) / max(len(real_candidates_info), 1) * 12),
                    "stage": "scanning",
                    "message": f"ğŸ” **äººæ‰åº“æ‰«æä¸­** ({idx+1}/{len(real_candidates_info)})\n\n{chr(10).join(scan_lines)}"})
                await asyncio.sleep(0.3)
            
            _set_task(task_id, {"status": "running", "progress": 32, "stage": "scanning",
                "message": f"ğŸ” äººæ‰åº“æ‰«æå®Œæˆ âœ“\n\nå‘ç° **{len(real_candidates_info)}** åå€™é€‰äºº\n\n{chr(10).join(scan_lines)}\n\næ­£åœ¨æäº¤ AI æ™ºèƒ½åŒ¹é…åˆ†æ..."})
            
            await asyncio.sleep(1.5)
            
            # ===== é˜¶æ®µ 4ï¼šAI åŒ¹é…åˆ†æ =====
            real_section = ""
            if real_candidates_info:
                real_lines = [
                    f'  {{"id": {rc["id"]}, "name": "{rc["name"]}", "title": "{rc["title"]}", "experience": "{rc["experience_years"]}å¹´", "skills": {json.dumps(rc["skills"], ensure_ascii=False)}}}'
                    for rc in real_candidates_info
                ]
                real_section = f"\nä»¥ä¸‹æ˜¯æ•°æ®åº“ä¸­çš„çœŸå®å€™é€‰äººï¼š\n[\n{chr(10).join(real_lines)}\n]"
            
            extra_req_section = f"\nç”¨æˆ·é¢å¤–è¦æ±‚ï¼š{extra_requirements}" if extra_requirements else ""
            match_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ HR æ™ºèƒ½åŒ¹é…å¼•æ“ã€‚
ã€å²—ä½ä¿¡æ¯ã€‘
{jobs_context}
{memory_context}
{extra_req_section}
{real_section}

è¯·å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
1. å¯¹ä¸Šé¢çš„çœŸå®å€™é€‰äººé€ä¸€æ‰“åˆ†è¯„ä¼°ï¼ˆå¦‚æœ‰çš„è¯ï¼‰
2. å¦å¤–æ¨¡æ‹Ÿç”Ÿæˆ 3 ä¸ªé¢å¤–çš„ä¼˜è´¨å€™é€‰äººä½œä¸º AI æ¨è

ä¸¥æ ¼æŒ‰ä»¥ä¸‹ JSON æ ¼å¼è¿”å›ï¼ˆç›´æ¥è¿”å›JSONæ•°ç»„ï¼Œä¸è¦åŒ…å«markdownä»£ç å—æ ‡è®°ï¼‰ï¼š
[
  {{"id": null, "name": "å§“å", "title": "å½“å‰èŒä½", "experience": "Xå¹´ç»éªŒ", "match_score": 85, "highlight": "ä¸€å¥è¯äº®ç‚¹", "skills": ["æŠ€èƒ½1", "æŠ€èƒ½2"], "source": "database æˆ– ai_simulated", "match_reason": "åŒ¹é…ç†ç”±"}}
]
è§„åˆ™ï¼šçœŸå®å€™é€‰äºº id å¡«å®é™…æ•°å­— source å¡« databaseï¼ŒAI æ¨¡æ‹Ÿ id å¡« null source å¡« ai_simulatedï¼ŒæŒ‰ match_score ä»é«˜åˆ°ä½æ’åºã€‚"""
            
            _set_task(task_id, {"status": "running", "progress": 35, "stage": "ai_analyzing",
                "message": f"ğŸ¤– **AI åŒ¹é…åˆ†æå¯åŠ¨**\n\nAI æ­£åœ¨å°† {len(real_candidates_info)} åå€™é€‰äººä¸å²—ä½è¦æ±‚é€ä¸€æ¯”å¯¹...\n\nåˆ†æç»´åº¦ï¼šæŠ€èƒ½åŒ¹é… Â· ç»éªŒå¥‘åˆ Â· å‘å±•æ½œåŠ› Â· ç»¼åˆç«äº‰åŠ›"})
            
            # æ„å»º AI æ€è€ƒæœŸé—´çš„åŠ¨æ€æ¶ˆæ¯
            candidate_names = [rc["name"] for rc in real_candidates_info]
            ai_thinking = [
                f"ğŸ¤– æ­£åœ¨åˆ†æ **{candidate_names[i % max(len(candidate_names), 1)]}** çš„æŠ€èƒ½çŸ©é˜µä¸å²—ä½åŒ¹é…åº¦..."
                for i in range(len(candidate_names))
            ] + [
                f"ğŸ¤– æ­£åœ¨è¯„ä¼° **{candidate_names[i % max(len(candidate_names), 1)]}** çš„ç»éªŒæ·±åº¦å’Œé¡¹ç›®èƒŒæ™¯..."
                for i in range(len(candidate_names))
            ] + [
                "ğŸ¤– æ­£åœ¨äº¤å‰å¯¹æ¯”å„å€™é€‰äººçš„ç»¼åˆç«äº‰åŠ›...",
                "ğŸ¤– æ­£åœ¨ç»“åˆä¼ä¸šåå¥½ç”Ÿæˆæ™ºèƒ½æ¨èè¯„åˆ†...",
                "ğŸ¤– æ­£åœ¨æ¨¡æ‹Ÿç”Ÿæˆ AI æ¨èå€™é€‰äºº...",
                "ğŸ¤– AI åˆ†æå³å°†å®Œæˆï¼Œæ­£åœ¨ç”ŸæˆåŒ¹é…æŠ¥å‘Š...",
            ]
            
            # åœ¨åå°æ¸è¿›æ¨è¿›è¿›åº¦ï¼ˆä½¿ç”¨ screen ä¸­å®šä¹‰çš„ _tick_progressï¼‰
            ticker = asyncio.create_task(
                _run_smart_screen.__code__.co_consts[1] if False else  # placeholder
                _smart_invite_tick(task_id, 35, 72, ai_thinking)
            )
            
            minimax_api_key = settings.minimax_api_key or ""
            gemini_api_key = settings.gemini_api_key or ""
            ai_response_text = ""
            
            try:
                if minimax_api_key:
                    try:
                        async with httpx.AsyncClient(timeout=60.0) as client:
                            response = await client.post(
                                "https://api.minimax.chat/v1/text/chatcompletion_v2",
                                headers={"Authorization": f"Bearer {minimax_api_key}", "Content-Type": "application/json"},
                                json={"model": "abab6.5s-chat", "messages": [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ HR æ™ºèƒ½åŒ¹é…å¼•æ“ï¼Œåªè¿”å› JSON æ•°æ®ã€‚"}, {"role": "user", "content": match_prompt}], "max_tokens": 2048, "temperature": 0.7}
                            )
                            result = response.json()
                            if result.get("base_resp", {}).get("status_code", 0) == 0 and "choices" in result and len(result["choices"]) > 0:
                                ai_response_text = result["choices"][0].get("message", {}).get("content", "")
                    except Exception as e:
                        print(f"[async-invite] MiniMax error: {e}")
                
                if not ai_response_text and gemini_api_key:
                    try:
                        async with httpx.AsyncClient(timeout=60.0) as client:
                            response = await client.post(
                                f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={gemini_api_key}",
                                json={"contents": [{"parts": [{"text": match_prompt}]}], "generationConfig": {"temperature": 0.7, "maxOutputTokens": 2048}}
                            )
                            result = response.json()
                            if "candidates" in result:
                                ai_response_text = result["candidates"][0].get("content", {}).get("parts", [{}])[0].get("text", "")
                    except Exception as e:
                        print(f"[async-invite] Gemini error: {e}")
            finally:
                ticker.cancel()
                try: await ticker
                except asyncio.CancelledError: pass
            
            # è§£æç»“æœ
            matches = []
            if ai_response_text:
                try:
                    cleaned = ai_response_text.replace("```json", "").replace("```", "").strip()
                    json_match = re.search(r'\[[\s\S]*\]', cleaned)
                    if json_match:
                        matches = json.loads(json_match.group(0))
                except Exception as e:
                    print(f"[async-invite] JSON parse error: {e}")
            
            if not matches:
                for rc in real_candidates_info[:5]:
                    matches.append({"id": rc["id"], "name": rc["name"], "title": rc["title"], "experience": f'{rc["experience_years"]}å¹´', "match_score": 70 + (rc["id"] % 20), "highlight": rc["summary"][:50] or "æ•°æ®åº“å€™é€‰äºº", "skills": rc["skills"][:5], "source": "database", "match_reason": "æŠ€èƒ½å’Œç»éªŒä¸å²—ä½éœ€æ±‚åŸºæœ¬åŒ¹é…"})
                for s in [{"name": "å¼ æŸæŸ", "title": "é«˜çº§å‰ç«¯å·¥ç¨‹å¸ˆ", "exp": "5å¹´", "score": 92, "hl": "å¤§å‚èƒŒæ™¯", "skills": ["React", "TypeScript"]}, {"name": "ææŸæŸ", "title": "å…¨æ ˆå·¥ç¨‹å¸ˆ", "exp": "4å¹´", "score": 87, "hl": "ç‹¬ç«‹é¡¹ç›®ç»éªŒ", "skills": ["Node.js", "React"]}, {"name": "ç‹æŸæŸ", "title": "å‰ç«¯å¼€å‘", "exp": "3å¹´", "score": 81, "hl": "å›¢é˜Ÿç®¡ç†ç»éªŒ", "skills": ["Vue", "React"]}]:
                    matches.append({"id": None, "name": s["name"], "title": s["title"], "experience": s["exp"], "match_score": s["score"], "highlight": s["hl"], "skills": s["skills"], "source": "ai_simulated", "match_reason": "AIæ¨è"})
            
            matches.sort(key=lambda x: x.get("match_score", 0), reverse=True)
            total_real = sum(1 for m in matches if m.get("source") == "database")
            total_simulated = sum(1 for m in matches if m.get("source") == "ai_simulated")
            
            # å±•ç¤ºåŒ¹é…ç»“æœæ‘˜è¦
            result_lines = []
            for idx, m in enumerate(matches[:10]):
                badge = "[DB]" if m.get("source") == "database" else "[AI]"
                result_lines.append(f"Â· {badge} {m['name']} â€” {m.get('title', '')} | {m.get('match_score', 0)}% åŒ¹é…")
            
            _set_task(task_id, {"status": "running", "progress": 75, "stage": "ai_done",
                "message": f"ğŸ¤– **AI åŒ¹é…åˆ†æå®Œæˆ** âœ“\n\nåŒ¹é…åˆ° {len(matches)} åå€™é€‰äººï¼ˆæ•°æ®åº“ {total_real} + AIæ¨è {total_simulated}ï¼‰ï¼š\n\n{chr(10).join(result_lines)}\n\næ­£åœ¨å‘é€æŠ•é€’é‚€è¯·..."})
            
            await asyncio.sleep(1.5)
            
            # ===== é˜¶æ®µ 5ï¼šå‘é€é‚€è¯· =====
            send_lines = []
            for idx, m in enumerate(matches):
                send_lines.append(f"âœ‰ï¸ å·²é‚€è¯· {m['name']}")
                _set_task(task_id, {"status": "running", "progress": 78 + int((idx + 1) / len(matches) * 15),
                    "stage": "sending",
                    "message": f"ğŸ“¨ **å‘é€æŠ•é€’é‚€è¯·** ({idx+1}/{len(matches)})\n\n{chr(10).join(send_lines)}"})
                await asyncio.sleep(0.4)
            
            # å†™å…¥ invite_match + invite_send æ—¥å¿—
            for j in jobs:
                log_match = JobLog(
                    job_id=j.id, actor_type="ai", action=JobLogAction.INVITE_MATCH,
                    title="æ™ºèƒ½å€™é€‰äººåŒ¹é…å®Œæˆ",
                    content=f"AI ä¸ºå²—ä½ã€Œ{j.title}ã€åŒ¹é…äº† {len(matches)} åå€™é€‰äººï¼ˆæ•°æ®åº“ {total_real} äºº + AIæ¨è {total_simulated} äººï¼‰",
                    extra_data=json.dumps({"candidates_count": len(matches), "real_count": total_real, "simulated_count": total_simulated, "candidates": [{"id": c.get("id"), "name": c["name"], "match_score": c.get("match_score"), "source": c.get("source")} for c in matches]}, ensure_ascii=False),
                    todo_id=todo_id,
                )
                log_send = JobLog(
                    job_id=j.id, actor_type="system", action=JobLogAction.INVITE_SEND,
                    title="æŠ•é€’é‚€è¯·å·²å‘é€",
                    content=f"ç³»ç»Ÿå‘ {len(matches)} åå€™é€‰äººå‘é€äº†å²—ä½ã€Œ{j.title}ã€çš„æŠ•é€’é‚€è¯·",
                    extra_data=json.dumps({"invited_count": len(matches), "invited_candidates": [c["name"] for c in matches]}, ensure_ascii=False),
                    todo_id=todo_id,
                )
                db.add(log_match)
                db.add(log_send)
            await db.commit()
            
            # ===== ä¸ºåŒ¹é…åˆ°çš„çœŸå®å€™é€‰äººåˆ›å»º Flow è®°å½•ï¼ˆæ¨¡æ‹ŸæŠ•é€’ï¼‰ =====
            from app.models.flow import FlowStatus, FlowStage, FlowTimeline
            flow_created = 0
            for j in jobs:
                for m in matches:
                    c_id = m.get("id")
                    if not c_id:
                        continue  # AI æ¨¡æ‹Ÿå€™é€‰äººæ²¡æœ‰çœŸå® IDï¼Œè·³è¿‡
                    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ Flow
                    existing = await db.execute(
                        select(Flow).where(Flow.job_id == j.id, Flow.candidate_id == c_id)
                    )
                    if existing.scalar_one_or_none():
                        continue
                    flow = Flow(
                        candidate_id=c_id,
                        job_id=j.id,
                        recruiter_id=user_id,
                        status=FlowStatus.SCREENING,
                        current_stage=FlowStage.PARSE,
                        current_step=1,
                        match_score=m.get("match_score", 0),
                        details=f"æ™ºèƒ½é‚€è¯·åŒ¹é…ï¼š{m.get('match_reason', '')}",
                        last_action="æ™ºèƒ½é‚€è¯· - å€™é€‰äººåŒ¹é…",
                        agents_used=["smart_invite"],
                    )
                    db.add(flow)
                    # æ·»åŠ æ—¶é—´çº¿
                    db.add(FlowTimeline(
                        flow=flow,
                        action=f"æ™ºèƒ½é‚€è¯·åŒ¹é…ï¼ˆåŒ¹é…åº¦ {m.get('match_score', 0)}%ï¼‰",
                        agent_name="smart_invite",
                    ))
                    flow_created += 1
            if flow_created > 0:
                await db.commit()
            
            _set_task(task_id, {"status": "running", "progress": 97, "stage": "sending",
                "message": f"ğŸ“¨ æŠ•é€’é‚€è¯·å…¨éƒ¨å‘é€å®Œæˆ âœ“\n\n{chr(10).join(send_lines)}\n\nå·²åˆ›å»º {flow_created} æ¡æŠ•é€’æµç¨‹è®°å½•\næ­£åœ¨ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š..."})
            await asyncio.sleep(1)
            
            _set_task(task_id, {
                "status": "completed", "progress": 100, "stage": "done",
                "message": f"æ™ºèƒ½é‚€è¯·å®Œæˆï¼åŒ¹é… {len(matches)} åå€™é€‰äººå¹¶å‘é€é‚€è¯·",
                "result": {
                    "matches": matches, "total_real": total_real, "total_simulated": total_simulated,
                    "job_titles": job_titles, "memory_context": memory_context[:200] if memory_context else "",
                }
            })
        except Exception as e:
            print(f"[async-invite] Error: {e}")
            import traceback; traceback.print_exc()
            _set_task(task_id, {"status": "failed", "progress": 0, "message": f"æ™ºèƒ½é‚€è¯·å¤±è´¥ï¼š{str(e)}"})


async def _smart_invite_tick(task_id: str, start_pct: int, end_pct: int, thinking_msgs: list, interval: float = 1.5):
    """æ™ºèƒ½é‚€è¯· LLM è°ƒç”¨æœŸé—´çš„è¿›åº¦æ¨è¿›"""
    pct = start_pct
    msg_idx = 0
    while pct < end_pct:
        await asyncio.sleep(interval)
        pct = min(pct + 1, end_pct)
        msg = thinking_msgs[msg_idx % len(thinking_msgs)]
        _set_task(task_id, {
            "status": "running", "progress": pct,
            "stage": _get_task(task_id).get("stage", "ai_analyzing"),
            "message": msg,
        })
        msg_idx += 1


async def _run_smart_screen(task_id: str, job_ids: List[int], user_id: int, todo_id: Optional[int], extra_requirements: str):
    """åå°æ‰§è¡Œæ™ºèƒ½ç­›é€‰ â€” å¤šç»´åº¦ AI ç‹¬ç«‹å®¡æ ¸åˆ†æï¼ˆç»†ç²’åº¦å®æ—¶è¿›åº¦å›ä¼ ï¼‰"""
    import httpx
    import re
    import time
    from app.config import settings
    
    # --- è¾…åŠ©ï¼šåœ¨ LLM è°ƒç”¨æœŸé—´æ¸è¿›æ¨è¿›è¿›åº¦ + æ›´æ–°åŠ¨æ€æ€è€ƒæ¶ˆæ¯ ---
    async def _tick_progress(task_id: str, start_pct: int, end_pct: int, thinking_msgs: list, interval: float = 1.2):
        """åœ¨ LLM API ç­‰å¾…æœŸé—´æ¯ interval ç§’æ¨è¿› 1% è¿›åº¦å¹¶è½®æ¢æ€è€ƒæ¶ˆæ¯"""
        pct = start_pct
        msg_idx = 0
        while pct < end_pct:
            await asyncio.sleep(interval)
            pct = min(pct + 1, end_pct)
            msg = thinking_msgs[msg_idx % len(thinking_msgs)]
            _set_task(task_id, {
                "status": "running", "progress": pct,
                "stage": _get_task(task_id).get("stage", ""),
                "message": msg,
            })
            msg_idx += 1
    
    _set_task(task_id, {"status": "running", "progress": 2, "stage": "init", "message": "æ­£åœ¨å¯åŠ¨æ™ºèƒ½ç­›é€‰å¼•æ“..."})
    
    async with AsyncSessionLocal() as db:
        try:
            # ===== é˜¶æ®µ 1ï¼šåŠ è½½æ•°æ® =====
            _set_task(task_id, {"status": "running", "progress": 3, "stage": "init", "message": "æ­£åœ¨è¿æ¥æ•°æ®åº“ï¼ŒæŸ¥è¯¢å…³è”å²—ä½..."})
            
            job_result = await db.execute(
                select(Job).options(selectinload(Job.tags)).where(Job.id.in_(job_ids))
            )
            jobs = job_result.scalars().all()
            if not jobs:
                _set_task(task_id, {"status": "failed", "progress": 0, "message": "æœªæ‰¾åˆ°å…³è”çš„å²—ä½"})
                return
            
            job_titles = [j.title for j in jobs]
            candidate_names_str = ""
            
            _set_task(task_id, {"status": "running", "progress": 5, "stage": "init",
                "message": f"å·²åŠ è½½ {len(jobs)} ä¸ªå²—ä½ï¼š{'ã€'.join(job_titles)}"})
            
            # å†™å…¥ screen_start æ—¥å¿—
            for j in jobs:
                log = JobLog(
                    job_id=j.id, actor_type="system", action=JobLogAction.SCREEN_START,
                    title="å¼€å§‹æ™ºèƒ½ç­›é€‰",
                    content=f"ç³»ç»Ÿå¯åŠ¨äº‘ç«¯å¼‚æ­¥æ™ºèƒ½ç­›é€‰ï¼Œå²—ä½ã€Œ{j.title}ã€{f'ï¼Œç­›é€‰è¦æ±‚ï¼š{extra_requirements}' if extra_requirements else ''}",
                    extra_data=json.dumps({"task_id": task_id, "extra_requirements": extra_requirements}, ensure_ascii=False),
                    todo_id=todo_id,
                )
                db.add(log)
            await db.commit()
            
            _set_task(task_id, {"status": "running", "progress": 8, "stage": "loading_invites",
                "message": "æ­£åœ¨ä»å²—ä½æ—¥å¿—ä¸­è¯»å–é‚€è¯·é˜¶æ®µçš„å€™é€‰äººæ•°æ®..."})
            
            # è·å–ä¹‹å‰ invite é˜¶æ®µåŒ¹é…çš„å€™é€‰äºº
            invite_candidates = []
            for j in jobs:
                log_result = await db.execute(
                    select(JobLog).where(
                        JobLog.job_id == j.id,
                        JobLog.action == JobLogAction.INVITE_MATCH
                    ).order_by(JobLog.created_at.desc()).limit(1)
                )
                invite_log = log_result.scalar_one_or_none()
                if invite_log and invite_log.extra_data:
                    try:
                        data = json.loads(invite_log.extra_data) if isinstance(invite_log.extra_data, str) else (invite_log.extra_data or {})
                        invite_candidates = data.get("candidates", [])
                    except:
                        pass
            
            if not invite_candidates:
                invite_candidates = [
                    {"id": None, "name": "å¼ æŸæŸ", "match_score": 92, "source": "ai_simulated"},
                    {"id": None, "name": "ææŸæŸ", "match_score": 87, "source": "ai_simulated"},
                    {"id": None, "name": "ç‹æŸæŸ", "match_score": 81, "source": "ai_simulated"},
                ]
            
            candidate_names = [c.get("name", "æœªçŸ¥") for c in invite_candidates]
            candidate_names_str = "ã€".join(candidate_names)
            
            _set_task(task_id, {"status": "running", "progress": 12, "stage": "loading_invites",
                "message": f"å·²åŠ è½½ {len(invite_candidates)} åå€™é€‰äººï¼š{candidate_names_str}\n\næ­£åœ¨è¯»å–ä¼ä¸šè®°å¿†ä¸åå¥½..."})
            
            # æŸ¥è¯¢ä¼ä¸šè®°å¿†
            memory_result = await db.execute(
                select(Memory).where(Memory.user_id == user_id, Memory.scope == MemoryScope.EMPLOYER)
            )
            all_memories = memory_result.scalars().all()
            important_memories = [m for m in all_memories if m.type == MemoryType.REQUIREMENT or m.emphasis_count >= 2]
            memory_context = ""
            mem_summary = "æ— ç‰¹æ®Šåå¥½"
            if important_memories:
                memory_lines = [f"- [{m.type.value}] {m.content}" for m in important_memories[:10]]
                memory_context = "\n".join(memory_lines)
                mem_summary = f"{len(important_memories)} æ¡ä¼ä¸šåå¥½/è¦æ±‚å·²æ³¨å…¥"
            
            _set_task(task_id, {"status": "running", "progress": 15, "stage": "loading_invites",
                "message": f"æ•°æ®å‡†å¤‡å®Œæˆ âœ“\n\nÂ· å²—ä½ï¼š{'ã€'.join(job_titles)}\nÂ· å€™é€‰äººï¼š{candidate_names_str}ï¼ˆ{len(invite_candidates)} äººï¼‰\nÂ· ä¼ä¸šè®°å¿†ï¼š{mem_summary}\n\nå³å°†å¼€å§‹ AI å®¡æ ¸åˆ†æ..."})
            
            await asyncio.sleep(1.5)  # ç»™å‰ç«¯ä¸€ç‚¹æ—¶é—´å±•ç¤ºå‡†å¤‡å®Œæˆ
            
            # ===== é˜¶æ®µ 2ï¼šä¼ä¸šæ–¹ AI å®¡æ ¸ =====
            jobs_context = "\n".join([f"å²—ä½: {j.title}, æè¿°: {(j.description or '')[:300]}" for j in jobs])
            candidates_info = json.dumps(invite_candidates, ensure_ascii=False)
            
            _set_task(task_id, {"status": "running", "progress": 18, "stage": "employer_review",
                "message": f"ğŸ¢ **ä¼ä¸šæ–¹ AI å®¡æ ¸å¯åŠ¨**\n\nAI æ­£åœ¨ä»¥ä¼ä¸šè§†è§’é€ä¸€å®¡æ ¸ {len(invite_candidates)} åå€™é€‰äºº...\n\nå®¡æ ¸ç»´åº¦ï¼šæŠ€èƒ½åŒ¹é…åº¦ Â· ç»éªŒé€‚é…æ€§ Â· æ–‡åŒ–å¥‘åˆåº¦ Â· é£é™©è¯„ä¼°"})
            
            employer_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ AI æ‹›è˜å®¡æ ¸ä¸“å®¶ï¼Œä»£è¡¨ä¼ä¸šæ–¹å¯¹å€™é€‰äººè¿›è¡Œç‹¬ç«‹å®¡æ ¸ã€‚

ã€å²—ä½ä¿¡æ¯ã€‘
{jobs_context}

ã€ä¼ä¸šåå¥½/è¦æ±‚ã€‘
{memory_context or 'æ— ç‰¹æ®Šè¦æ±‚'}

{f'ã€é¢å¤–ç­›é€‰è¦æ±‚ã€‘{extra_requirements}' if extra_requirements else ''}

ã€å¾…å®¡æ ¸å€™é€‰äººï¼ˆæ¥è‡ªæ™ºèƒ½é‚€è¯·åŒ¹é…ï¼‰ã€‘
{candidates_info}

è¯·å¯¹æ¯ä½å€™é€‰äººè¿›è¡Œç‹¬ç«‹ã€ä¸¥è°¨çš„å®¡æ ¸åˆ†æã€‚æ³¨æ„ï¼šä¸æ˜¯æ‰€æœ‰äººéƒ½åº”è¯¥é€šè¿‡ï¼Œè¯·çœŸå®è¯„ä¼°ï¼

ä¸¥æ ¼æŒ‰ä»¥ä¸‹ JSON æ ¼å¼è¿”å›ï¼ˆç›´æ¥è¿”å›JSONæ•°ç»„ï¼‰ï¼š
[
  {{
    "name": "å€™é€‰äººå§“å",
    "employer_pass": true,
    "employer_score": 90,
    "employer_analysis": "ä¼ä¸šæ–¹å®¡æ ¸æ„è§ï¼ˆ2-3å¥è¯ï¼ŒåŒ…å«ä¼˜åŠ¿å’Œé£é™©è¯„ä¼°ï¼‰",
    "strengths": ["ä¼˜åŠ¿1", "ä¼˜åŠ¿2"],
    "concerns": ["å…³æ³¨ç‚¹/é£é™©"]
  }}
]
è§„åˆ™ï¼šemployer_pass ä¸º true è¡¨ç¤ºä¼ä¸šæ–¹é€šè¿‡ï¼Œfalse è¡¨ç¤ºä¸é€šè¿‡ã€‚è¦å®äº‹æ±‚æ˜¯ï¼Œè‡³å°‘æ·˜æ±°1äººã€‚"""
            
            minimax_api_key = settings.minimax_api_key or ""
            gemini_api_key = settings.gemini_api_key or ""
            
            # æ„å»º LLM è°ƒç”¨æœŸé—´çš„åŠ¨æ€æ€è€ƒæ¶ˆæ¯
            employer_thinking = [
                f"ğŸ¢ æ­£åœ¨åˆ†æ **{candidate_names[i % len(candidate_names)]}** çš„æŠ€èƒ½çŸ©é˜µä¸å²—ä½è¦æ±‚çš„åŒ¹é…åº¦..."
                for i in range(len(candidate_names))
            ] + [
                f"ğŸ¢ æ­£åœ¨è¯„ä¼° **{candidate_names[i % len(candidate_names)]}** çš„é¡¹ç›®ç»éªŒæ·±åº¦ä¸è¡Œä¸šç›¸å…³æ€§..."
                for i in range(len(candidate_names))
            ] + [
                "ğŸ¢ æ­£åœ¨äº¤å‰å¯¹æ¯”å€™é€‰äººä¹‹é—´çš„ç«äº‰åŠ›å·®å¼‚...",
                "ğŸ¢ æ­£åœ¨ç»“åˆä¼ä¸šè®°å¿†åå¥½è¿›è¡Œç»¼åˆè¯„åˆ†...",
                f"ğŸ¢ æ­£åœ¨å¯¹ {len(candidate_names)} åå€™é€‰äººç”Ÿæˆé£é™©è¯„ä¼°æŠ¥å‘Š...",
                "ğŸ¢ AI æ·±åº¦åˆ†æä¸­ï¼Œæ­£åœ¨ç”Ÿæˆå®¡æ ¸ç»“è®º...",
            ]
            
            # å¯åŠ¨åå°è¿›åº¦æ¨è¿›ä»»åŠ¡
            ticker = asyncio.create_task(_tick_progress(task_id, 18, 45, employer_thinking, interval=1.5))
            
            employer_reviews = []
            ai_text = ""
            try:
                if minimax_api_key:
                    try:
                        async with httpx.AsyncClient(timeout=60.0) as client:
                            resp = await client.post("https://api.minimax.chat/v1/text/chatcompletion_v2", headers={"Authorization": f"Bearer {minimax_api_key}", "Content-Type": "application/json"}, json={"model": "abab6.5s-chat", "messages": [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ AI æ‹›è˜å®¡æ ¸ä¸“å®¶ï¼Œåªè¿”å› JSONã€‚"}, {"role": "user", "content": employer_prompt}], "max_tokens": 2048, "temperature": 0.5})
                            r = resp.json()
                            if r.get("base_resp", {}).get("status_code", 0) == 0 and "choices" in r:
                                ai_text = r["choices"][0].get("message", {}).get("content", "")
                    except Exception as e:
                        print(f"[screen-employer] MiniMax error: {e}")
                if not ai_text and gemini_api_key:
                    try:
                        async with httpx.AsyncClient(timeout=60.0) as client:
                            resp = await client.post(f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={gemini_api_key}", json={"contents": [{"parts": [{"text": employer_prompt}]}], "generationConfig": {"temperature": 0.5, "maxOutputTokens": 2048}})
                            r = resp.json()
                            if "candidates" in r:
                                ai_text = r["candidates"][0].get("content", {}).get("parts", [{}])[0].get("text", "")
                    except Exception as e:
                        print(f"[screen-employer] Gemini error: {e}")
            finally:
                ticker.cancel()
                try: await ticker
                except asyncio.CancelledError: pass
            
            if ai_text:
                try:
                    cleaned = ai_text.replace("```json", "").replace("```", "").strip()
                    m = re.search(r'\[[\s\S]*\]', cleaned)
                    if m:
                        employer_reviews = json.loads(m.group(0))
                except:
                    pass
            
            if not employer_reviews:
                employer_reviews = [
                    {"name": c.get("name", "æœªçŸ¥"), "employer_pass": c.get("match_score", 80) >= 85, "employer_score": c.get("match_score", 80), "employer_analysis": "AI å®¡æ ¸è¯„ä¼°ç»“æœ", "strengths": ["å¾…è¡¥å……"], "concerns": ["å¾…è¡¥å……"]}
                    for c in invite_candidates
                ]
            
            # å±•ç¤ºä¼ä¸šå®¡æ ¸ç»“æœæ‘˜è¦
            ep_count = sum(1 for r in employer_reviews if r.get("employer_pass"))
            ef_count = len(employer_reviews) - ep_count
            employer_summary_lines = []
            for r in employer_reviews:
                status = "âœ… é€šè¿‡" if r.get("employer_pass") else "âŒ æœªé€šè¿‡"
                employer_summary_lines.append(f"Â· {r.get('name', '?')} â†’ {status}ï¼ˆ{r.get('employer_score', 0)}åˆ†ï¼‰")
            
            _set_task(task_id, {"status": "running", "progress": 47, "stage": "employer_review_done",
                "message": f"ğŸ¢ **ä¼ä¸šæ–¹å®¡æ ¸å®Œæˆ** âœ“\n\n{chr(10).join(employer_summary_lines)}\n\nå…± {len(employer_reviews)} äººå®¡æ ¸ â†’ é€šè¿‡ {ep_count} äººï¼Œæœªé€šè¿‡ {ef_count} äºº\n\nå³å°†å¼€å§‹å€™é€‰äººæ„æ„¿è¯„ä¼°..."})
            
            # å†™å…¥ AI åˆ†ææ—¥å¿— â€” ä¼ä¸šæ–¹å®¡æ ¸ï¼ˆåŒ…å«æ¯äººå®¡æ ¸æ„è§ï¼‰
            employer_log_lines = []
            for r in employer_reviews:
                pass_str = "âœ“é€šè¿‡" if r.get("employer_pass") else "âœ—æœªé€šè¿‡"
                employer_log_lines.append(f"Â· {r.get('name', '?')}ï¼š{pass_str}ï¼ˆ{r.get('employer_score', 0)}åˆ†ï¼‰â€” {r.get('employer_analysis', '')}")
            
            for j in jobs:
                log = JobLog(
                    job_id=j.id, actor_type="ai", action=JobLogAction.AI_ANALYSIS,
                    title="ä¼ä¸šæ–¹ AI ç‹¬ç«‹å®¡æ ¸å®Œæˆ",
                    content=(
                        f"AI å¯¹å²—ä½ã€Œ{j.title}ã€çš„ {len(employer_reviews)} åå€™é€‰äººå®Œæˆä¼ä¸šæ–¹ç‹¬ç«‹å®¡æ ¸\n\n"
                        + "\n".join(employer_log_lines)
                    ),
                    extra_data=json.dumps({"review_type": "employer", "reviews": employer_reviews, "pass_count": ep_count, "fail_count": ef_count}, ensure_ascii=False),
                    todo_id=todo_id,
                )
                db.add(log)
            await db.commit()
            
            await asyncio.sleep(2)  # ç»™å‰ç«¯æ—¶é—´å±•ç¤ºä¼ä¸šå®¡æ ¸ç»“æœ
            
            # ===== é˜¶æ®µ 3ï¼šå€™é€‰äººæ„æ„¿ AI è¯„ä¼° =====
            _set_task(task_id, {"status": "running", "progress": 50, "stage": "candidate_review",
                "message": f"ğŸ‘¤ **å€™é€‰äººæ„æ„¿è¯„ä¼°å¯åŠ¨**\n\nAI æ­£åœ¨æ¨¡æ‹Ÿ {len(invite_candidates)} åå€™é€‰äººçš„è§†è§’...\n\nè¯„ä¼°ç»´åº¦ï¼šå²—ä½å¸å¼•åŠ› Â· è–ªèµ„ç«äº‰åŠ› Â· å‘å±•ç©ºé—´ Â· ä¼ä¸šæ–‡åŒ–"})
            
            candidate_prompt = f"""ä½ æ˜¯ä¸€ä¸ª AI æ±‚èŒé¡¾é—®ï¼Œç°åœ¨éœ€è¦æ¨¡æ‹Ÿå€™é€‰äººçš„è§†è§’ï¼Œè¯„ä¼°ä»–ä»¬æ˜¯å¦æ„¿æ„æ¥å—è¿™äº›å²—ä½çš„é‚€è¯·ã€‚

ã€å²—ä½ä¿¡æ¯ã€‘
{jobs_context}

ã€å€™é€‰äººåˆ—è¡¨ã€‘
{candidates_info}

è¯·ç«™åœ¨æ¯ä½å€™é€‰äººçš„è§’åº¦ï¼Œåˆ†æè¿™ä¸ªå²—ä½å¯¹ä»–ä»¬çš„å¸å¼•åŠ›ï¼Œæ¨¡æ‹Ÿä»–ä»¬æ˜¯å¦ä¼šæ¥å—é‚€è¯·æŠ•é€’ç®€å†ã€‚

ä¸¥æ ¼æŒ‰ä»¥ä¸‹ JSON æ ¼å¼è¿”å›ï¼ˆç›´æ¥è¿”å›JSONæ•°ç»„ï¼‰ï¼š
[
  {{
    "name": "å€™é€‰äººå§“å",
    "candidate_pass": true,
    "candidate_interest": 85,
    "candidate_analysis": "å€™é€‰äººè§†è§’åˆ†æï¼ˆä¸ºä»€ä¹ˆæ¥å—/æ‹’ç»ï¼Œ1-2å¥è¯ï¼‰",
    "response_type": "ç§¯æå“åº”/è€ƒè™‘ä¸­/å©‰æ‹’"
  }}
]
è§„åˆ™ï¼šcandidate_pass ä¸º true è¡¨ç¤ºå€™é€‰äººæ„¿æ„æŠ•é€’/æ¥å—é‚€è¯·ï¼Œfalse è¡¨ç¤ºæ‹’ç»ã€‚è¦åˆç†æ¨¡æ‹Ÿï¼Œä¸æ˜¯æ‰€æœ‰äººéƒ½ä¼šæ¥å—ã€‚"""
            
            # æ„å»ºå€™é€‰äººè¯„ä¼°æœŸé—´çš„åŠ¨æ€æ€è€ƒæ¶ˆæ¯
            candidate_thinking = [
                f"ğŸ‘¤ æ­£åœ¨æ¨¡æ‹Ÿ **{candidate_names[i % len(candidate_names)]}** çš„æ±‚èŒå†³ç­–è¿‡ç¨‹..."
                for i in range(len(candidate_names))
            ] + [
                f"ğŸ‘¤ æ­£åœ¨è¯„ä¼°å²—ä½è–ªèµ„ vs **{candidate_names[i % len(candidate_names)]}** çš„å¸‚åœºæœŸæœ›..."
                for i in range(len(candidate_names))
            ] + [
                "ğŸ‘¤ æ­£åœ¨åˆ†æå²—ä½å‘å±•ç©ºé—´å¯¹å€™é€‰äººçš„å¸å¼•åŠ›...",
                "ğŸ‘¤ æ­£åœ¨æ¨¡æ‹Ÿå€™é€‰äººå¯¹ä¼ä¸šæ–‡åŒ–çš„é€‚é…åº¦æ„Ÿå—...",
                f"ğŸ‘¤ æ­£åœ¨å¯¹ {len(candidate_names)} åå€™é€‰äººç”Ÿæˆæ„æ„¿è¯„ä¼°ç»“è®º...",
                "ğŸ‘¤ AI æ„æ„¿æ¨¡æ‹Ÿå®Œæˆä¸­...",
            ]
            
            ticker2 = asyncio.create_task(_tick_progress(task_id, 50, 75, candidate_thinking, interval=1.5))
            
            candidate_reviews = []
            ai_text2 = ""
            try:
                if minimax_api_key:
                    try:
                        async with httpx.AsyncClient(timeout=60.0) as client:
                            resp = await client.post("https://api.minimax.chat/v1/text/chatcompletion_v2", headers={"Authorization": f"Bearer {minimax_api_key}", "Content-Type": "application/json"}, json={"model": "abab6.5s-chat", "messages": [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ª AI æ±‚èŒé¡¾é—®ï¼Œåªè¿”å› JSONã€‚"}, {"role": "user", "content": candidate_prompt}], "max_tokens": 2048, "temperature": 0.6})
                            r = resp.json()
                            if r.get("base_resp", {}).get("status_code", 0) == 0 and "choices" in r:
                                ai_text2 = r["choices"][0].get("message", {}).get("content", "")
                    except Exception as e:
                        print(f"[screen-candidate] MiniMax error: {e}")
                if not ai_text2 and gemini_api_key:
                    try:
                        async with httpx.AsyncClient(timeout=60.0) as client:
                            resp = await client.post(f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={gemini_api_key}", json={"contents": [{"parts": [{"text": candidate_prompt}]}], "generationConfig": {"temperature": 0.6, "maxOutputTokens": 2048}})
                            r = resp.json()
                            if "candidates" in r:
                                ai_text2 = r["candidates"][0].get("content", {}).get("parts", [{}])[0].get("text", "")
                    except Exception as e:
                        print(f"[screen-candidate] Gemini error: {e}")
            finally:
                ticker2.cancel()
                try: await ticker2
                except asyncio.CancelledError: pass
            
            if ai_text2:
                try:
                    cleaned = ai_text2.replace("```json", "").replace("```", "").strip()
                    m = re.search(r'\[[\s\S]*\]', cleaned)
                    if m:
                        candidate_reviews = json.loads(m.group(0))
                except:
                    pass
            
            if not candidate_reviews:
                candidate_reviews = [
                    {"name": c.get("name", "æœªçŸ¥"), "candidate_pass": c.get("match_score", 80) >= 80, "candidate_interest": c.get("match_score", 80), "candidate_analysis": "å€™é€‰äººæ„æ„¿è¯„ä¼°", "response_type": "ç§¯æå“åº”" if c.get("match_score", 80) >= 85 else "è€ƒè™‘ä¸­"}
                    for c in invite_candidates
                ]
            
            # å±•ç¤ºå€™é€‰äººæ„æ„¿ç»“æœæ‘˜è¦
            cp_count = sum(1 for r in candidate_reviews if r.get("candidate_pass"))
            cf_count = len(candidate_reviews) - cp_count
            candidate_summary_lines = []
            for r in candidate_reviews:
                status = "âœ… æ„¿æ„" if r.get("candidate_pass") else "âŒ å©‰æ‹’"
                candidate_summary_lines.append(f"Â· {r.get('name', '?')} â†’ {status}ï¼ˆ{r.get('response_type', 'æœªçŸ¥')}ï¼Œå…´è¶£ {r.get('candidate_interest', 0)}%ï¼‰")
            
            _set_task(task_id, {"status": "running", "progress": 78, "stage": "candidate_review_done",
                "message": f"ğŸ‘¤ **å€™é€‰äººæ„æ„¿è¯„ä¼°å®Œæˆ** âœ“\n\n{chr(10).join(candidate_summary_lines)}\n\nå…± {len(candidate_reviews)} äººè¯„ä¼° â†’ æ„¿æ„ {cp_count} äººï¼Œå©‰æ‹’ {cf_count} äºº\n\næ­£åœ¨ç»¼åˆä¸¤è½®å®¡æ ¸ç»“æœ..."})
            
            # å†™å…¥ AI åˆ†ææ—¥å¿— â€” å€™é€‰äººæ–¹ï¼ˆåŒ…å«æ¯äººæ„æ„¿è¯„ä¼°ï¼‰
            candidate_log_lines = []
            for r in candidate_reviews:
                pass_str = "âœ“æ„¿æ„" if r.get("candidate_pass") else "âœ—å©‰æ‹’"
                candidate_log_lines.append(f"Â· {r.get('name', '?')}ï¼š{pass_str}ï¼ˆ{r.get('response_type', '')}ï¼Œå…´è¶£ {r.get('candidate_interest', 0)}%ï¼‰â€” {r.get('candidate_analysis', '')}")
            
            for j in jobs:
                log = JobLog(
                    job_id=j.id, actor_type="ai", action=JobLogAction.AI_ANALYSIS,
                    title="å€™é€‰äººæ„æ„¿ AI è¯„ä¼°å®Œæˆ",
                    content=(
                        f"AI æ¨¡æ‹Ÿäº† {len(candidate_reviews)} åå€™é€‰äººå¯¹å²—ä½ã€Œ{j.title}ã€çš„æŠ•é€’æ„æ„¿\n\n"
                        + "\n".join(candidate_log_lines)
                    ),
                    extra_data=json.dumps({"review_type": "candidate", "reviews": candidate_reviews, "accept_count": cp_count, "reject_count": cf_count}, ensure_ascii=False),
                    todo_id=todo_id,
                )
                db.add(log)
            await db.commit()
            
            await asyncio.sleep(2)  # ç»™å‰ç«¯æ—¶é—´å±•ç¤ºå€™é€‰äººå®¡æ ¸ç»“æœ
            
            # ===== é˜¶æ®µ 4ï¼šç»¼åˆåˆ†æ =====
            _set_task(task_id, {"status": "running", "progress": 82, "stage": "merging",
                "message": "ğŸ“Š æ­£åœ¨äº¤å‰å¯¹æ¯”ä¼ä¸šå®¡æ ¸ä¸å€™é€‰äººæ„æ„¿...\n\nåŒ¹é…è§„åˆ™ï¼šä¼ä¸šé€šè¿‡ + å€™é€‰äººæ„¿æ„ = é€šè¿‡ç­›é€‰"})
            
            employer_map = {r.get("name", ""): r for r in employer_reviews}
            candidate_map = {r.get("name", ""): r for r in candidate_reviews}
            
            final_results = []
            merge_lines = []
            for idx, ic in enumerate(invite_candidates):
                name = ic.get("name", "æœªçŸ¥")
                er = employer_map.get(name, {})
                cr = candidate_map.get(name, {})
                
                employer_pass = er.get("employer_pass", False)
                candidate_pass = cr.get("candidate_pass", False)
                both_pass = employer_pass and candidate_pass
                
                result_item = {
                    "name": name,
                    "id": ic.get("id"),
                    "match_score": ic.get("match_score", 80),
                    "source": ic.get("source", "ai_simulated"),
                    "employer_pass": employer_pass,
                    "employer_score": er.get("employer_score", 0),
                    "employer_analysis": er.get("employer_analysis", ""),
                    "strengths": er.get("strengths", []),
                    "concerns": er.get("concerns", []),
                    "candidate_pass": candidate_pass,
                    "candidate_interest": cr.get("candidate_interest", 0),
                    "candidate_analysis": cr.get("candidate_analysis", ""),
                    "response_type": cr.get("response_type", "æœªçŸ¥"),
                    "both_pass": both_pass,
                    "final_status": "é€šè¿‡" if both_pass else ("ä¼ä¸šé€šè¿‡" if employer_pass else ("å€™é€‰äººæ„å‘" if candidate_pass else "æœªé€šè¿‡")),
                }
                final_results.append(result_item)
                
                e_icon = "âœ…" if employer_pass else "âŒ"
                c_icon = "âœ…" if candidate_pass else "âŒ"
                f_icon = "ğŸ‰" if both_pass else "â€”"
                merge_lines.append(f"Â· {name}ï¼šä¼ä¸š{e_icon} + å€™é€‰äºº{c_icon} â†’ {f_icon}{result_item['final_status']}")
                
                _set_task(task_id, {"status": "running", "progress": 82 + int((idx + 1) / len(invite_candidates) * 10),
                    "stage": "merging",
                    "message": f"ğŸ“Š **ç»¼åˆåˆ†æä¸­** ({idx+1}/{len(invite_candidates)})\n\n{chr(10).join(merge_lines)}"})
                await asyncio.sleep(0.8)
            
            both_pass_list = [r for r in final_results if r["both_pass"]]
            employer_only = [r for r in final_results if r["employer_pass"] and not r["candidate_pass"]]
            candidate_only = [r for r in final_results if r["candidate_pass"] and not r["employer_pass"]]
            neither = [r for r in final_results if not r["employer_pass"] and not r["candidate_pass"]]
            
            _set_task(task_id, {"status": "running", "progress": 95, "stage": "merging",
                "message": f"ğŸ“Š **ç»¼åˆåˆ†æå®Œæˆ** âœ“\n\n{chr(10).join(merge_lines)}\n\n---\né€šè¿‡ **{len(both_pass_list)}** äºº | å¾…ç¡®è®¤ {len(employer_only)} äºº | æœªé€šè¿‡ {len(candidate_only) + len(neither)} äºº\n\næ­£åœ¨å†™å…¥å®¡æ ¸æŠ¥å‘Šå’Œå²—ä½æ—¥å¿—..."})
            
            # å†™å…¥ screen_result æ—¥å¿— â€” åŒ…å«å®Œæ•´çš„ç­›é€‰æŠ¥å‘Š
            # æ„å»ºè¯¦ç»†çš„çº¯æ–‡æœ¬ç­›é€‰æŠ¥å‘Š
            detail_lines = []
            for r in final_results:
                status_label = r.get("final_status", "æœªçŸ¥")
                e_score = r.get("employer_score", 0)
                c_interest = r.get("candidate_interest", 0)
                detail_lines.append(
                    f"ã€{r['name']}ã€‘ç»“æœï¼š{status_label} | ä¼ä¸šè¯„åˆ†ï¼š{e_score}åˆ† | å€™é€‰äººæ„æ„¿ï¼š{c_interest}%\n"
                    f"  ä¼ä¸šå®¡æ ¸ï¼š{r.get('employer_analysis', 'æ— ')}\n"
                    f"  å€™é€‰äººåé¦ˆï¼š{r.get('candidate_analysis', 'æ— ')} ({r.get('response_type', '')})\n"
                    f"  ä¼˜åŠ¿ï¼š{'ã€'.join(r.get('strengths', []))}\n"
                    f"  å…³æ³¨ï¼š{'ã€'.join(r.get('concerns', []))}"
                )
            detailed_report = "\n\n".join(detail_lines)
            
            for j in jobs:
                log = JobLog(
                    job_id=j.id, actor_type="ai", action=JobLogAction.SCREEN_RESULT,
                    title="æ™ºèƒ½ç­›é€‰å®Œæˆ",
                    content=(
                        f"å²—ä½ã€Œ{j.title}ã€æ™ºèƒ½ç­›é€‰å®Œæˆ\n"
                        f"â”â”â” æ±‡æ€» â”â”â”\n"
                        f"é€šè¿‡ {len(both_pass_list)} äºº | å¾…ç¡®è®¤ {len(employer_only)} äºº | ä»…å€™é€‰äººæ„å‘ {len(candidate_only)} äºº | æœªé€šè¿‡ {len(neither)} äºº\n\n"
                        f"â”â”â” è¯¦ç»†è¯„å®¡ â”â”â”\n{detailed_report}"
                    ),
                    extra_data=json.dumps({
                        "both_pass_count": len(both_pass_list),
                        "employer_only_count": len(employer_only),
                        "candidate_only_count": len(candidate_only),
                        "neither_count": len(neither),
                        "results": final_results,
                        "both_pass_names": [r["name"] for r in both_pass_list],
                        "employer_only_names": [r["name"] for r in employer_only],
                    }, ensure_ascii=False),
                    todo_id=todo_id,
                )
                db.add(log)
            await db.commit()
            
            # ===== æ ¹æ®ç­›é€‰ç»“æœæ›´æ–° Flow è®°å½• =====
            from app.models.flow import FlowStatus, FlowStage, FlowTimeline
            flow_updated = 0
            for j in jobs:
                for r in final_results:
                    c_id = r.get("id")
                    if not c_id:
                        continue
                    flow_result = await db.execute(
                        select(Flow).where(Flow.job_id == j.id, Flow.candidate_id == c_id)
                    )
                    flow = flow_result.scalar_one_or_none()
                    if not flow:
                        continue
                    
                    if r.get("both_pass"):
                        flow.status = FlowStatus.ACCEPTED
                        flow.current_stage = FlowStage.FINAL
                        flow.last_action = "æ™ºèƒ½ç­›é€‰é€šè¿‡ Â· è”ç³»æ–¹å¼å·²äº’æ¢"
                        flow.details = f"ä¼ä¸šè¯„åˆ† {r.get('employer_score', 0)}åˆ† | å€™é€‰äººæ„å‘ {r.get('candidate_interest', 0)}åˆ† | ç­›é€‰é€šè¿‡ï¼Œè”ç³»æ–¹å¼å·²è‡ªåŠ¨äº’æ¢"
                        flow.completed_at = datetime.utcnow()
                        db.add(FlowTimeline(
                            flow=flow,
                            action=f"æ™ºèƒ½ç­›é€‰é€šè¿‡ï¼ˆä¼ä¸š {r.get('employer_score', 0)}åˆ† / å€™é€‰äººæ„å‘ {r.get('candidate_interest', 0)}åˆ†ï¼‰Â· è”ç³»æ–¹å¼å·²è‡ªåŠ¨äº’æ¢",
                            agent_name="smart_screen",
                        ))
                    elif r.get("employer_pass"):
                        flow.status = FlowStatus.SCREENING
                        flow.current_stage = FlowStage.BENCHMARK
                        flow.last_action = "æ™ºèƒ½ç­›é€‰ - ä¼ä¸šé€šè¿‡/å€™é€‰äººæœªç¡®è®¤"
                        flow.details = f"ä¼ä¸šè¯„åˆ† {r.get('employer_score', 0)}åˆ† | å€™é€‰äººæ„å‘ä¸è¶³"
                        db.add(FlowTimeline(
                            flow=flow, action="ä¼ä¸šæ–¹é€šè¿‡ï¼Œå€™é€‰äººæš‚æœªç¡®è®¤",
                            agent_name="smart_screen",
                        ))
                    else:
                        flow.status = FlowStatus.REJECTED
                        flow.last_action = "æ™ºèƒ½ç­›é€‰ - æœªé€šè¿‡"
                        flow.details = f"ä¼ä¸šè¯„åˆ† {r.get('employer_score', 0)}åˆ† | {r.get('employer_analysis', '')}"
                        db.add(FlowTimeline(
                            flow=flow, action=f"ç­›é€‰æœªé€šè¿‡ï¼ˆ{r.get('final_status', 'æœªé€šè¿‡')}ï¼‰",
                            agent_name="smart_screen",
                        ))
                    flow_updated += 1
            if flow_updated > 0:
                await db.commit()
            
            # ===== ä¸º AI æ¨¡æ‹Ÿçš„ç­›é€‰é€šè¿‡å€™é€‰äººåˆ›å»ºæ•°æ®åº“è®°å½• =====
            from app.models.user import User as UserModelForSeed, UserRole as UserRoleForSeed
            from app.models.candidate import Candidate as CandidateForSeed, CandidateProfile as ProfileForSeed
            from app.utils.security import get_password_hash as hash_pw
            
            ai_created = 0
            for r in final_results:
                # ä»…å¤„ç†æ²¡æœ‰ id çš„ AI æ¨¡æ‹Ÿå€™é€‰äºº
                if r.get("id") or r.get("source") == "database":
                    continue
                c_name = r.get("name", "")
                if not c_name:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨åŒåçš„æ¨¡æ‹Ÿç”¨æˆ·
                email_slug = c_name.replace(" ", "").lower()
                mock_email = f"{email_slug}@ai-mock.dev"
                exists_check = await db.execute(select(UserModelForSeed).where(UserModelForSeed.email == mock_email))
                existing_mock = exists_check.scalar_one_or_none()
                
                if existing_mock:
                    # å·²å­˜åœ¨ï¼Œè·å–å…¶å€™é€‰äºº ID å¹¶æ›´æ–° final_results
                    cand_check = await db.execute(select(CandidateForSeed).where(CandidateForSeed.user_id == existing_mock.id))
                    cand_existing = cand_check.scalar_one_or_none()
                    if cand_existing:
                        r["id"] = cand_existing.id
                    continue
                
                # ç”Ÿæˆæ¨¡æ‹Ÿæ‰‹æœºå·å’Œåˆ›å»ºç”¨æˆ·
                mock_phone = f"138{hash(c_name) % 100000000:08d}"
                mock_user = UserModelForSeed(
                    email=mock_email,
                    hashed_password=hash_pw("ai_mock_pwd"),
                    name=c_name,
                    phone=mock_phone,
                    role=UserRoleForSeed.CANDIDATE,
                    is_active=True,
                    is_verified=True,
                )
                db.add(mock_user)
                await db.flush()
                
                mock_cand = CandidateForSeed(
                    user_id=mock_user.id,
                    resume_text=r.get("employer_analysis", "") or r.get("match_reason", "") or f"{c_name}çš„ç®€å†",
                    is_profile_complete=True,
                )
                db.add(mock_cand)
                await db.flush()
                
                mock_profile = ProfileForSeed(
                    candidate_id=mock_cand.id,
                    display_name=c_name,
                    current_role=r.get("title") or r.get("role") or "AIæ¨èå€™é€‰äºº",
                    experience_years=float(str(r.get("experience", "3")).replace("å¹´", "").strip() or "3"),
                    summary=r.get("employer_analysis", "") or r.get("highlight", "") or f"{c_name}ï¼ŒAIæ¨èçš„ä¼˜è´¨å€™é€‰äººã€‚",
                    ideal_job_persona=r.get("highlight", ""),
                    salary_range="é¢è®®",
                    market_demand=f"AI æ™ºèƒ½æ¨èå€™é€‰äººï¼ŒåŒ¹é…åˆ† {r.get('match_score', 80)}%",
                    radar_data={
                        "æŠ€æœ¯æ·±åº¦": min(95, r.get("match_score", 80)),
                        "é¡¹ç›®ç»éªŒ": min(90, r.get("match_score", 80) - 5),
                        "æ²Ÿé€šåä½œ": 75,
                        "å­¦ä¹ èƒ½åŠ›": 80,
                        "è¡Œä¸šè®¤çŸ¥": 70,
                    },
                    interview_questions=[
                        f"è¯·ä»‹ç»ä½ æœ€æœ‰æŒ‘æˆ˜æ€§çš„é¡¹ç›®ç»å†ã€‚",
                        f"ä½ å¦‚ä½•çœ‹å¾…å½“å‰è¡Œä¸šçš„æŠ€æœ¯å‘å±•è¶‹åŠ¿ï¼Ÿ",
                        f"æè¿°ä¸€æ¬¡ä½ è§£å†³å¤æ‚æŠ€æœ¯é—®é¢˜çš„è¿‡ç¨‹ã€‚",
                    ],
                )
                db.add(mock_profile)
                
                # æ›´æ–° final_results ä¸­çš„ id
                r["id"] = mock_cand.id
                ai_created += 1
            
            if ai_created > 0:
                await db.commit()
                
                # åŒæ—¶æ›´æ–° screen_result æ—¥å¿—ä¸­çš„ candidate id
                screen_update_result = await db.execute(
                    select(JobLog).where(JobLog.action == JobLogAction.SCREEN_RESULT)
                    .order_by(JobLog.created_at.desc()).limit(10)
                )
                screen_logs_to_update = screen_update_result.scalars().all()
                for j in jobs:
                    for sl in screen_logs_to_update:
                        try:
                            extra = json.loads(sl.extra_data) if isinstance(sl.extra_data, str) else (sl.extra_data or {})
                            results = extra.get("results", [])
                            updated = False
                            for res_item in results:
                                for fr in final_results:
                                    if res_item.get("name") == fr.get("name") and fr.get("id") and not res_item.get("id"):
                                        res_item["id"] = fr["id"]
                                        updated = True
                            if updated:
                                sl.extra_data = json.dumps(extra, ensure_ascii=False)
                        except Exception:
                            pass
                    
                    # åŒæ—¶æ›´æ–° invite_match æ—¥å¿—
                    invite_logs_update = await db.execute(
                        select(JobLog).where(
                            JobLog.job_id == j.id,
                            JobLog.action == JobLogAction.INVITE_MATCH
                        ).order_by(JobLog.created_at.desc()).limit(1)
                    )
                    il_update = invite_logs_update.scalar_one_or_none()
                    if il_update:
                        try:
                            il_extra = json.loads(il_update.extra_data) if isinstance(il_update.extra_data, str) else (il_update.extra_data or {})
                            il_candidates = il_extra.get("candidates", [])
                            for ic in il_candidates:
                                for fr in final_results:
                                    if ic.get("name") == fr.get("name") and fr.get("id") and not ic.get("id"):
                                        ic["id"] = fr["id"]
                            il_extra["candidates"] = il_candidates
                            il_update.extra_data = json.dumps(il_extra, ensure_ascii=False)
                        except Exception:
                            pass
                
                await db.commit()
            
            await asyncio.sleep(1)
            
            _set_task(task_id, {
                "status": "completed", "progress": 100, "stage": "done",
                "message": f"æ™ºèƒ½ç­›é€‰å®Œæˆï¼{len(both_pass_list)} äººé€šè¿‡ç­›é€‰",
                "result": {
                    "final_results": final_results,
                    "both_pass": both_pass_list,
                    "employer_only": employer_only,
                    "candidate_only": candidate_only,
                    "neither": neither,
                    "job_titles": job_titles,
                    "summary": {
                        "total": len(final_results),
                        "both_pass_count": len(both_pass_list),
                        "employer_only_count": len(employer_only),
                        "candidate_only_count": len(candidate_only),
                        "neither_count": len(neither),
                    }
                }
            })
        except Exception as e:
            print(f"[async-screen] Error: {e}")
            import traceback; traceback.print_exc()
            _set_task(task_id, {"status": "failed", "progress": 0, "message": f"æ™ºèƒ½ç­›é€‰å¤±è´¥ï¼š{str(e)}"})


@router.post("/async-task")
async def start_async_task(
    req: AsyncTaskRequest,
    background_tasks: BackgroundTasks,
):
    """å¯åŠ¨å¼‚æ­¥ä»»åŠ¡ï¼ˆæ™ºèƒ½é‚€è¯·/æ™ºèƒ½ç­›é€‰ï¼‰"""
    task_id = str(uuid.uuid4())[:12]
    
    _set_task(task_id, {"status": "pending", "progress": 0, "stage": "init", "message": "ä»»åŠ¡å·²åˆ›å»ºï¼Œç­‰å¾…æ‰§è¡Œ..."})
    
    if req.task_type == "smart_invite":
        background_tasks.add_task(_run_smart_invite, task_id, req.job_ids, req.user_id, req.todo_id, req.extra_requirements)
    elif req.task_type == "smart_screen":
        background_tasks.add_task(_run_smart_screen, task_id, req.job_ids, req.user_id, req.todo_id, req.extra_requirements)
    else:
        return {"error": f"æœªçŸ¥ä»»åŠ¡ç±»å‹: {req.task_type}"}
    
    return {"task_id": task_id, "status": "pending", "message": "å¼‚æ­¥ä»»åŠ¡å·²å¯åŠ¨"}


@router.get("/async-task/{task_id}")
async def get_async_task_status(task_id: str):
    """æŸ¥è¯¢å¼‚æ­¥ä»»åŠ¡çŠ¶æ€"""
    task = _get_task(task_id)
    if not task:
        return {"status": "not_found", "message": "ä»»åŠ¡ä¸å­˜åœ¨"}
    return {"task_id": task_id, **task}


# ============ å²—ä½æ—¥å¿— API ============

class JobLogCreate(BaseModel):
    """åˆ›å»ºå²—ä½æ—¥å¿—"""
    job_id: int
    actor_id: Optional[int] = None
    actor_type: str = "user"  # user / ai / system
    action: str  # JobLogAction æšä¸¾å€¼
    title: str
    content: str
    extra_data: Optional[dict] = None
    todo_id: Optional[int] = None


@router.post("/job-logs")
async def create_job_log(
    log_in: JobLogCreate,
    db: AsyncSession = Depends(get_db)
):
    """åˆ›å»ºå²—ä½æ—¥å¿—"""
    import json as json_mod
    
    # éªŒè¯å²—ä½å­˜åœ¨
    result = await db.execute(select(Job).where(Job.id == log_in.job_id))
    job = result.scalar_one_or_none()
    if not job:
        raise HTTPException(status_code=404, detail="å²—ä½ä¸å­˜åœ¨")
    
    try:
        action = JobLogAction(log_in.action)
    except ValueError:
        action = JobLogAction.USER_ACTION
    
    log = JobLog(
        job_id=log_in.job_id,
        actor_id=log_in.actor_id,
        actor_type=log_in.actor_type,
        action=action,
        title=log_in.title,
        content=log_in.content,
        extra_data=json_mod.dumps(log_in.extra_data) if log_in.extra_data else None,
        todo_id=log_in.todo_id,
    )
    db.add(log)
    await db.commit()
    await db.refresh(log)
    
    return {
        "id": log.id,
        "job_id": log.job_id,
        "action": log.action.value,
        "title": log.title,
        "created_at": log.created_at.isoformat() if log.created_at else None,
    }


@router.get("/job-logs/{job_id}")
async def get_job_logs(
    job_id: int,
    action: Optional[str] = Query(None, description="æŒ‰æ“ä½œç±»å‹è¿‡æ»¤"),
    limit: int = Query(50, description="æœ€å¤§è¿”å›æ•°é‡"),
    db: AsyncSession = Depends(get_db)
):
    """è·å–å²—ä½çš„äº¤äº’æ—¥å¿—"""
    import json as json_mod
    
    # éªŒè¯å²—ä½å­˜åœ¨
    result = await db.execute(select(Job).where(Job.id == job_id))
    job = result.scalar_one_or_none()
    if not job:
        raise HTTPException(status_code=404, detail="å²—ä½ä¸å­˜åœ¨")
    
    query = select(JobLog).where(JobLog.job_id == job_id)
    
    if action:
        try:
            action_enum = JobLogAction(action)
            query = query.where(JobLog.action == action_enum)
        except ValueError:
            pass
    
    query = query.order_by(JobLog.created_at.desc()).limit(limit)
    result = await db.execute(query)
    logs = result.scalars().all()
    
    return {
        "job": {
            "id": job.id,
            "title": job.title,
        },
        "logs": [{
            "id": log.id,
            "job_id": log.job_id,
            "actor_id": log.actor_id,
            "actor_type": log.actor_type,
            "action": log.action.value if log.action else "user_action",
            "title": log.title,
            "content": log.content,
            "extra_data": json_mod.loads(log.extra_data) if isinstance(log.extra_data, str) else (log.extra_data or {}),
            "todo_id": log.todo_id,
            "created_at": log.created_at.isoformat() if log.created_at else None,
        } for log in logs],
        "total": len(logs),
    }


@router.post("/job-logs/batch")
async def create_job_logs_batch(
    logs_in: list,
    db: AsyncSession = Depends(get_db)
):
    """æ‰¹é‡åˆ›å»ºå²—ä½æ—¥å¿—ï¼ˆå‰ç«¯ä¸€æ¬¡æ€§æäº¤å¤šæ¡ï¼‰"""
    import json as json_mod
    
    created = []
    for log_data in logs_in:
        try:
            action = JobLogAction(log_data.get("action", "user_action"))
        except ValueError:
            action = JobLogAction.USER_ACTION
        
        log = JobLog(
            job_id=log_data["job_id"],
            actor_id=log_data.get("actor_id"),
            actor_type=log_data.get("actor_type", "system"),
            action=action,
            title=log_data.get("title", ""),
            content=log_data.get("content", ""),
            extra_data=json_mod.dumps(log_data["extra_data"]) if log_data.get("extra_data") else None,
            todo_id=log_data.get("todo_id"),
        )
        db.add(log)
        created.append(log)
    
    await db.commit()
    
    return {"created": len(created), "message": f"æˆåŠŸåˆ›å»º {len(created)} æ¡æ—¥å¿—"}


# ============ æ¨¡æ‹Ÿå€™é€‰äºº Seed API ============

@router.post("/seed-candidates")
async def seed_mock_candidates(
    db: AsyncSession = Depends(get_db)
):
    """åˆ›å»ºæ¨¡æ‹Ÿæ±‚èŒè€…å’ŒæŠ•é€’æ•°æ®ï¼Œç”¨äºæ™ºèƒ½é‚€è¯·/ç­›é€‰æµ‹è¯•"""
    from app.models.user import User, UserRole
    from app.models.candidate import Candidate, CandidateProfile, Skill
    from app.utils.security import get_password_hash
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰è¶³å¤Ÿçš„å€™é€‰äºº
    existing = await db.execute(
        select(Candidate).where(Candidate.is_profile_complete == True)
    )
    existing_count = len(existing.scalars().all())
    
    # ä¸å†æå‰è¿”å›ï¼Œå³ä½¿å·²æœ‰è¶³å¤Ÿå€™é€‰äººä¹Ÿå…è®¸æ›´æ–°æ•°æ®
    
    # æ¨¡æ‹Ÿå€™é€‰äººæ•°æ®ï¼ˆå®Œæ•´ç®€å†ä¸»é¡µä¿¡æ¯ï¼‰
    mock_candidates = [
        {
            "email": "liuwei@mock.dev", "name": "åˆ˜ä¼Ÿ", "phone": "13812345601", "password": "mock123456",
            "profile": {
                "display_name": "åˆ˜ä¼Ÿ", "current_role": "é«˜çº§å‰ç«¯å·¥ç¨‹å¸ˆ",
                "experience_years": 6.0,
                "summary": "6å¹´å‰ç«¯å¼€å‘ç»éªŒï¼Œç²¾é€šReact/Vueç”Ÿæ€ï¼Œæ›¾åœ¨å­—èŠ‚è·³åŠ¨è´Ÿè´£æŠ–éŸ³åˆ›ä½œè€…å¹³å°å‰ç«¯æ¶æ„è®¾è®¡ã€‚å¯¹æ€§èƒ½ä¼˜åŒ–ã€å¾®å‰ç«¯å’Œå·¥ç¨‹åŒ–æœ‰æ·±å…¥ç ”ç©¶ï¼Œä¸»å¯¼è¿‡æ—¥æ´»åƒä¸‡çº§äº§å“çš„å‰ç«¯é‡æ„ã€‚",
                "ideal_job_persona": "æŠ€æœ¯Lead / å‰ç«¯æ¶æ„å¸ˆ",
                "salary_range": "30K-45K",
                "market_demand": "å‰ç«¯æ¶æ„å¸ˆå²—ä½éœ€æ±‚æ—ºç››ï¼Œå…·å¤‡å¾®å‰ç«¯å’Œæ€§èƒ½ä¼˜åŒ–ç»éªŒçš„å€™é€‰äººä¾›ä¸åº”æ±‚ã€‚",
                "interview_questions": ["å¦‚ä½•è®¾è®¡ä¸€ä¸ªæ”¯æŒç™¾ä¸‡æ—¥æ´»çš„å‰ç«¯æ€§èƒ½ç›‘æ§æ–¹æ¡ˆï¼Ÿ", "æè¿°ä½ ä¸»å¯¼å¾®å‰ç«¯æ¶æ„è½åœ°çš„å®Œæ•´è¿‡ç¨‹å’ŒæŒ‘æˆ˜ã€‚", "React 18 å¹¶å‘æ¨¡å¼åœ¨å®é™…é¡¹ç›®ä¸­çš„åº”ç”¨ç»éªŒï¼Ÿ"],
                "optimization_suggestions": ["å¯ä»¥è¡¥å…… Web3/åŒºå—é“¾å‰ç«¯çš„ç»éªŒ", "å»ºè®®è€ƒå– Google Web Professional è®¤è¯", "ä¸°å¯Œè·¨ç«¯ï¼ˆRN/Flutterï¼‰æŠ€æœ¯æ ˆ"],
                "certifications": [{"name": "Google Mobile Web Specialist", "issuer": "Google", "date": "2024-06"}, {"name": "AWS Cloud Practitioner", "issuer": "Amazon Web Services", "date": "2023-09"}],
                "awards": [{"name": "å­—èŠ‚è·³åŠ¨å¹´åº¦æŠ€æœ¯ä¹‹æ˜Ÿ", "org": "ByteDance", "year": "2024", "description": "æŠ–éŸ³åˆ›ä½œè€…å¹³å°å‰ç«¯æ¶æ„å“è¶Šè´¡çŒ®"}],
            },
            "skills": ["React", "TypeScript", "Vue.js", "Webpack", "Node.js", "GraphQL", "Micro-Frontend"]
        },
        {
            "email": "wangjing@mock.dev", "name": "ç‹é™", "phone": "13812345602", "password": "mock123456",
            "profile": {
                "display_name": "ç‹é™", "current_role": "å…¨æ ˆå·¥ç¨‹å¸ˆ",
                "experience_years": 4.0,
                "summary": "4å¹´å…¨æ ˆå¼€å‘ç»éªŒï¼Œæ“…é•¿Node.js+ReactæŠ€æœ¯æ ˆã€‚ç‹¬ç«‹å®Œæˆè¿‡ä¸¤ä¸ªSaaSäº§å“çš„ä»0åˆ°1å…¨æµç¨‹å¼€å‘ï¼ŒåŒ…æ‹¬æ¶æ„è®¾è®¡ã€æ•°æ®åº“å»ºæ¨¡ã€å‰åç«¯å®ç°å’Œéƒ¨ç½²ä¸Šçº¿ã€‚",
                "ideal_job_persona": "å…¨æ ˆæŠ€æœ¯è´Ÿè´£äºº",
                "salary_range": "25K-35K",
                "market_demand": "å…¨æ ˆå·¥ç¨‹å¸ˆåœ¨åˆ›ä¸šå…¬å¸å’Œä¸­å°ä¼ä¸šä¸­éœ€æ±‚ç¨³å®šï¼Œèƒ½ç‹¬ç«‹äº¤ä»˜é¡¹ç›®çš„å€™é€‰äººå°¤å—æ¬¢è¿ã€‚",
                "interview_questions": ["å¦‚ä½•ä»é›¶è®¾è®¡ä¸€ä¸ªæ”¯æŒå¤šç§Ÿæˆ·çš„SaaSç³»ç»Ÿï¼Ÿ", "å‰åç«¯åˆ†ç¦»æ¶æ„ä¸­ï¼Œä½ å¦‚ä½•å¤„ç†è®¤è¯å’Œé‰´æƒï¼Ÿ", "åˆ†äº«ä¸€ä¸ªä½ ä»0åˆ°1ç‹¬ç«‹å®Œæˆçš„é¡¹ç›®ç»å†ã€‚"],
                "optimization_suggestions": ["å¢åŠ äº‘åŸç”Ÿå’Œå®¹å™¨åŒ–éƒ¨ç½²çš„å®è·µç»éªŒ", "å­¦ä¹  Go è¯­è¨€æå‡åç«¯å¤šæ ·æ€§", "è¡¥å……ç³»ç»Ÿè®¾è®¡æ–¹é¢çš„ç†è®ºå‚¨å¤‡"],
                "certifications": [{"name": "AWS Solutions Architect Associate", "issuer": "Amazon Web Services", "date": "2024-02"}],
                "awards": [],
            },
            "skills": ["Node.js", "React", "PostgreSQL", "Docker", "AWS", "TypeScript"]
        },
        {
            "email": "zhangpeng@mock.dev", "name": "å¼ é¹", "phone": "13812345603", "password": "mock123456",
            "profile": {
                "display_name": "å¼ é¹", "current_role": "AIç®—æ³•å·¥ç¨‹å¸ˆ",
                "experience_years": 3.0,
                "summary": "3å¹´AI/MLå¼€å‘ç»éªŒï¼Œä¸“æ³¨NLPå’Œå¤§è¯­è¨€æ¨¡å‹åº”ç”¨ã€‚å‚ä¸è¿‡å¤šä¸ªä¼ä¸šçº§RAGç³»ç»Ÿå»ºè®¾ï¼Œç†Ÿæ‚‰LangChain/LlamaIndex/Difyæ¡†æ¶ã€‚æ‹¥æœ‰æ¸…åå¤§å­¦è®¡ç®—æœºç¡•å£«å­¦ä½ï¼Œåœ¨NeurIPSå‘è¡¨è¿‡è®ºæ–‡ã€‚",
                "ideal_job_persona": "AIåº”ç”¨æ¶æ„å¸ˆ",
                "salary_range": "35K-50K",
                "market_demand": "AIå·¥ç¨‹å¸ˆç›®å‰æ˜¯æœ€ç´§ç¼ºçš„æŠ€æœ¯å²—ä½ä¹‹ä¸€ï¼Œå…·å¤‡å¤§æ¨¡å‹è½åœ°ç»éªŒçš„å€™é€‰äººè–ªèµ„æ¶¨å¹…æ˜æ˜¾ã€‚",
                "interview_questions": ["å¦‚ä½•ä¼˜åŒ–RAGç³»ç»Ÿçš„æ£€ç´¢å‡†ç¡®ç‡å’Œå“åº”å»¶è¿Ÿï¼Ÿ", "å¯¹æ¯”ä¸åŒå‘é‡æ•°æ®åº“çš„ä¼˜åŠ£åŠé€‰å‹æ€è·¯ã€‚", "å¤§è¯­è¨€æ¨¡å‹çš„å¹»è§‰é—®é¢˜æœ‰å“ªäº›ç¼“è§£ç­–ç•¥ï¼Ÿ"],
                "optimization_suggestions": ["åŠ å¼ºå¤šæ¨¡æ€æ¨¡å‹çš„å®è·µç»éªŒ", "å¢åŠ AI Agent/å·¥å…·è°ƒç”¨æ–¹é¢çš„é¡¹ç›®", "è€ƒè™‘å‚ä¸å¼€æºLLMç¤¾åŒºæå‡å½±å“åŠ›"],
                "certifications": [{"name": "DeepLearning.AI Specialization", "issuer": "Coursera / Andrew Ng", "date": "2023-12"}, {"name": "Google TensorFlow Developer Certificate", "issuer": "Google", "date": "2023-06"}],
                "awards": [{"name": "NeurIPS 2024 Workshop Best Paper", "org": "NeurIPS", "year": "2024", "description": "å¤§è¯­è¨€æ¨¡å‹é«˜æ•ˆæ¨ç†ç ”ç©¶"}],
            },
            "skills": ["Python", "PyTorch", "LangChain", "RAG", "NLP", "FastAPI", "Docker"]
        },
        {
            "email": "chenxiao@mock.dev", "name": "é™ˆæ™“", "phone": "13812345604", "password": "mock123456",
            "profile": {
                "display_name": "é™ˆæ™“", "current_role": "åç«¯å¼€å‘å·¥ç¨‹å¸ˆ",
                "experience_years": 5.0,
                "summary": "5å¹´Java/Goåç«¯ç»éªŒï¼Œé˜¿é‡Œäº‘ACEè®¤è¯é«˜çº§å¼€å‘è€…ã€‚æ“…é•¿é«˜å¹¶å‘å¾®æœåŠ¡æ¶æ„è®¾è®¡ï¼Œä¸»å¯¼è¿‡æ—¥å‡è¯·æ±‚è¿‡äº¿çš„ç”µå•†ç³»ç»Ÿåç«¯é‡æ„ï¼Œæœ‰ç™¾ä¸‡çº§DAUç³»ç»Ÿè¿ç»´ç»éªŒã€‚",
                "ideal_job_persona": "åç«¯æ¶æ„å¸ˆ / æŠ€æœ¯ä¸“å®¶",
                "salary_range": "30K-40K",
                "market_demand": "åç«¯æ¶æ„å¸ˆåœ¨å„è§„æ¨¡ä¼ä¸šä¸­æŒç»­ç´§ç¼ºï¼Œå…·å¤‡é«˜å¹¶å‘ç³»ç»Ÿç»éªŒçš„å€™é€‰äººç«äº‰åŠ›æå¼ºã€‚",
                "interview_questions": ["å¦‚ä½•è®¾è®¡ä¸€ä¸ªæ”¯æ’‘ç™¾ä¸‡QPSçš„åˆ†å¸ƒå¼ç¼“å­˜æ–¹æ¡ˆï¼Ÿ", "å¾®æœåŠ¡æ‹†åˆ†çš„åŸåˆ™å’Œä½ åœ¨å®é™…é¡¹ç›®ä¸­çš„å–èˆï¼Ÿ", "æè¿°ä¸€æ¬¡çº¿ä¸Šæ•…éšœæ’æŸ¥å’Œå¤ç›˜çš„å®Œæ•´è¿‡ç¨‹ã€‚"],
                "optimization_suggestions": ["æ‹“å±•äº‘åŸç”Ÿå’ŒServerlessæ¶æ„å®è·µ", "è¡¥å……åˆ†å¸ƒå¼æ•°æ®åº“ï¼ˆTiDB/CockroachDBï¼‰ç»éªŒ", "å¢åŠ æŠ€æœ¯ç®¡ç†å’Œå›¢é˜Ÿåä½œæ–¹é¢çš„æ¡ˆä¾‹"],
                "certifications": [{"name": "é˜¿é‡Œäº‘ACEé«˜çº§å·¥ç¨‹å¸ˆ", "issuer": "é˜¿é‡Œäº‘", "date": "2024-01"}, {"name": "CKA (Kubernetes Administrator)", "issuer": "CNCF", "date": "2023-10"}],
                "awards": [{"name": "é˜¿é‡Œäº‘MVP", "org": "é˜¿é‡Œäº‘", "year": "2024", "description": "äº‘åŸç”Ÿæ¶æ„è®¾è®¡æ°å‡ºè´¡çŒ®è€…"}],
            },
            "skills": ["Java", "Go", "Kubernetes", "Redis", "Kafka", "Spring Boot", "MySQL"]
        },
        {
            "email": "huangmei@mock.dev", "name": "é»„æ¢…", "phone": "13812345605", "password": "mock123456",
            "profile": {
                "display_name": "é»„æ¢…", "current_role": "äº§å“è®¾è®¡å¸ˆ",
                "experience_years": 4.5,
                "summary": "4å¹´åŠUI/UXè®¾è®¡ç»éªŒï¼Œæ›¾åœ¨ç¾å›¢è´Ÿè´£å•†å®¶ç«¯äº§å“è®¾è®¡ã€‚ç²¾é€šFigmaå’Œè®¾è®¡ç³»ç»Ÿå»ºè®¾ï¼Œæ“…é•¿æ•°æ®é©±åŠ¨çš„è®¾è®¡å†³ç­–ã€‚æœ‰ä¸€å®šçš„å‰ç«¯å¼€å‘èƒ½åŠ›ï¼Œèƒ½ç‹¬ç«‹å®ç°è®¾è®¡ç¨¿ã€‚",
                "ideal_job_persona": "è®¾è®¡Leader / äº§å“è®¾è®¡ä¸“å®¶",
                "salary_range": "20K-30K",
                "market_demand": "å…¼å…·UI/UXè®¾è®¡å’Œå‰ç«¯å®ç°èƒ½åŠ›çš„å¤åˆå‹è®¾è®¡å¸ˆè¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œå°¤å…¶åœ¨åˆ›ä¸šå…¬å¸ä¸­éœ€æ±‚æ—ºç››ã€‚",
                "interview_questions": ["å¦‚ä½•ç”¨æ•°æ®éªŒè¯ä¸€ä¸ªè®¾è®¡æ–¹æ¡ˆçš„æœ‰æ•ˆæ€§ï¼Ÿ", "æè¿°ä¸€ä¸ªä½ ä»è°ƒç ”åˆ°ä¸Šçº¿çš„å®Œæ•´è®¾è®¡æ¡ˆä¾‹ã€‚", "è®¾è®¡ç³»ç»Ÿçš„å»ºè®¾å’Œç»´æŠ¤ä¸­æœ€å¤§çš„æŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿ"],
                "optimization_suggestions": ["åŠ å¼ºåŠ¨æ•ˆè®¾è®¡å’Œ Lottie/Rive åŠ¨ç”»èƒ½åŠ›", "å­¦ä¹ åŸºç¡€çš„æ•°æ®åˆ†ææŠ€èƒ½", "æ‹“å±• AI è¾…åŠ©è®¾è®¡å·¥å…·çš„ä½¿ç”¨ç»éªŒ"],
                "certifications": [{"name": "Google UX Design Professional Certificate", "issuer": "Google / Coursera", "date": "2024-03"}],
                "awards": [{"name": "ç¾å›¢å¹´åº¦æœ€ä½³è®¾è®¡å¥–", "org": "ç¾å›¢", "year": "2023", "description": "å•†å®¶ç«¯æ ¸å¿ƒæµç¨‹ä½“éªŒæå‡30%"}],
            },
            "skills": ["Figma", "UI/UX", "Prototyping", "HTML/CSS", "Design System", "User Research"]
        },
        {
            "email": "lihao@mock.dev", "name": "ææµ©", "phone": "13812345606", "password": "mock123456",
            "profile": {
                "display_name": "ææµ©", "current_role": "DevOpså·¥ç¨‹å¸ˆ",
                "experience_years": 7.0,
                "summary": "7å¹´è¿ç»´/DevOpsç»éªŒï¼Œç²¾é€šCI/CDæµæ°´çº¿å’Œäº‘åŸç”Ÿæ¶æ„ã€‚æ›¾å¸¦é¢†5äººSREå›¢é˜Ÿç®¡ç†åƒå°æœåŠ¡å™¨é›†ç¾¤ï¼Œå®ç°99.99%å¯ç”¨æ€§SLAã€‚åœ¨è‡ªåŠ¨åŒ–è¿ç»´å’ŒåŸºç¡€è®¾æ–½å³ä»£ç æ–¹é¢æœ‰æ·±åšç§¯ç´¯ã€‚",
                "ideal_job_persona": "SREè´Ÿè´£äºº / åŸºç¡€æ¶æ„ä¸“å®¶",
                "salary_range": "35K-50K",
                "market_demand": "èµ„æ·±DevOps/SREå²—ä½è–ªèµ„æŒç»­ä¸Šæ¶¨ï¼Œå…·å¤‡å›¢é˜Ÿç®¡ç†ç»éªŒå’Œå¤§è§„æ¨¡é›†ç¾¤è¿ç»´èƒ½åŠ›çš„å€™é€‰äººæåº¦ç¨€ç¼ºã€‚",
                "interview_questions": ["å¦‚ä½•è®¾è®¡ä¸€ä¸ªé›¶å®•æœºçš„è“ç»¿éƒ¨ç½²æ–¹æ¡ˆï¼Ÿ", "K8sé›†ç¾¤åœ¨åƒèŠ‚ç‚¹è§„æ¨¡ä¸‹çš„è°ƒä¼˜ç»éªŒï¼Ÿ", "æè¿°ä¸€æ¬¡ä¸¥é‡çš„çº¿ä¸Šäº‹æ•…ä»¥åŠä½ çš„åº”æ€¥å’Œå¤ç›˜è¿‡ç¨‹ã€‚"],
                "optimization_suggestions": ["å¢åŠ FinOpsäº‘æˆæœ¬ä¼˜åŒ–ç›¸å…³ç»éªŒ", "è¡¥å……å®‰å…¨è¿ç»´ï¼ˆDevSecOpsï¼‰å®è·µ", "æ‹“å±•æ··åˆäº‘å’Œå¤šäº‘ç®¡ç†èƒ½åŠ›"],
                "certifications": [{"name": "CKA (Kubernetes Administrator)", "issuer": "CNCF", "date": "2023-05"}, {"name": "AWS DevOps Engineer Professional", "issuer": "Amazon Web Services", "date": "2024-02"}, {"name": "Terraform Associate", "issuer": "HashiCorp", "date": "2023-11"}],
                "awards": [{"name": "CNCF Ambassador", "org": "CNCF", "year": "2024", "description": "äº‘åŸç”Ÿç¤¾åŒºæ°å‡ºè´¡çŒ®è€…"}],
            },
            "skills": ["Kubernetes", "Docker", "Terraform", "AWS", "Jenkins", "Prometheus", "Linux"]
        },
        {
            "email": "zhaoli@mock.dev", "name": "èµµä¸½", "phone": "13812345607", "password": "mock123456",
            "profile": {
                "display_name": "èµµä¸½", "current_role": "æ•°æ®åˆ†æå¸ˆ",
                "experience_years": 3.0,
                "summary": "3å¹´æ•°æ®åˆ†æç»éªŒï¼Œç²¾é€šSQLå’ŒPythonæ•°æ®å¤„ç†ã€‚æœ‰ä¸°å¯Œçš„A/Bæµ‹è¯•å’Œç”¨æˆ·è¡Œä¸ºåˆ†æç»éªŒï¼Œæ“…é•¿ä»æ•°æ®ä¸­å‘ç°ä¸šåŠ¡å¢é•¿ç‚¹ã€‚æ›¾åœ¨ç½‘æ˜“æ¸¸æˆè´Ÿè´£ç”¨æˆ·å¢é•¿åˆ†æï¼Œæ¨åŠ¨ç•™å­˜ç‡æå‡15%ã€‚",
                "ideal_job_persona": "é«˜çº§æ•°æ®åˆ†æå¸ˆ / æ•°æ®äº§å“ç»ç†",
                "salary_range": "18K-28K",
                "market_demand": "æ•°æ®åˆ†æå¸ˆåœ¨äº’è”ç½‘å’Œä¼ ç»Ÿè¡Œä¸šéƒ½æœ‰æ—ºç››éœ€æ±‚ï¼Œå…¼å…·ä¸šåŠ¡ç†è§£å’ŒæŠ€æœ¯èƒ½åŠ›çš„åˆ†æå¸ˆæœ€å—é’çã€‚",
                "interview_questions": ["å¦‚ä½•è®¾è®¡ä¸€ä¸ªä¸¥è°¨çš„A/Bæµ‹è¯•æ–¹æ¡ˆï¼Ÿ", "æè¿°ä½ é€šè¿‡æ•°æ®åˆ†æå‘ç°å¹¶è§£å†³ä¸šåŠ¡é—®é¢˜çš„æ¡ˆä¾‹ã€‚", "ç”¨æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼ï¼ˆLTVï¼‰çš„è®¡ç®—å’Œåº”ç”¨åœºæ™¯ï¼Ÿ"],
                "optimization_suggestions": ["å­¦ä¹ æœºå™¨å­¦ä¹ å»ºæ¨¡æå‡é¢„æµ‹èƒ½åŠ›", "å¢åŠ æ•°æ®å·¥ç¨‹ï¼ˆETL/æ•°æ®ä»“åº“ï¼‰æ–¹é¢çš„ç»éªŒ", "è¡¥å……æ•°æ®å¯è§†åŒ–å™äº‹èƒ½åŠ›"],
                "certifications": [{"name": "Google Data Analytics Professional Certificate", "issuer": "Google / Coursera", "date": "2024-04"}],
                "awards": [{"name": "ç½‘æ˜“å¹´åº¦æ•°æ®é©±åŠ¨åˆ›æ–°å¥–", "org": "ç½‘æ˜“", "year": "2024", "description": "æ¸¸æˆç”¨æˆ·å¢é•¿åˆ†æé©±åŠ¨ç•™å­˜ç‡æå‡15%"}],
            },
            "skills": ["Python", "SQL", "Tableau", "Pandas", "A/B Testing", "Machine Learning"]
        },
    ]
    
    created_count = 0
    updated_count = 0
    base_id = 1000100  # é¿å…ä¸å·²æœ‰ç”¨æˆ·å†²çª
    
    for i, mc in enumerate(mock_candidates):
        p = mc["profile"]
        radar = {
            "æŠ€æœ¯æ·±åº¦": min(95, int(p["experience_years"] * 13 + 20)),
            "é¡¹ç›®ç»éªŒ": min(90, int(p["experience_years"] * 12 + 15)),
            "æ²Ÿé€šåä½œ": 70 + (i * 3) % 20,
            "å­¦ä¹ èƒ½åŠ›": 75 + (i * 5) % 20,
            "è¡Œä¸šè®¤çŸ¥": 60 + (i * 7) % 25,
        }
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        exists_result = await db.execute(select(User).where(User.email == mc["email"]))
        existing_user = exists_result.scalar_one_or_none()
        
        if existing_user:
            # æ›´æ–°å·²æœ‰ç”¨æˆ·çš„ phone
            existing_user.phone = mc.get("phone")
            
            # æ›´æ–°å€™é€‰äºº profile
            cand_result = await db.execute(
                select(Candidate)
                .options(selectinload(Candidate.profile))
                .where(Candidate.user_id == existing_user.id)
            )
            cand = cand_result.scalar_one_or_none()
            if cand and cand.profile:
                prof = cand.profile
                prof.summary = p["summary"]
                prof.ideal_job_persona = p.get("ideal_job_persona", "")
                prof.salary_range = p.get("salary_range", "")
                prof.market_demand = p.get("market_demand", "")
                prof.radar_data = radar
                prof.interview_questions = p.get("interview_questions")
                prof.optimization_suggestions = p.get("optimization_suggestions")
                prof.certifications = p.get("certifications")
                prof.awards = p.get("awards")
            updated_count += 1
            continue
        
        # åˆ›å»ºç”¨æˆ·
        user = User(
            id=base_id + i,
            email=mc["email"],
            hashed_password=get_password_hash(mc["password"]),
            name=mc["name"],
            phone=mc.get("phone"),
            role=UserRole.CANDIDATE,
            is_active=True,
            is_verified=True,
        )
        db.add(user)
        await db.flush()
        
        # åˆ›å»ºå€™é€‰äºº
        candidate = Candidate(
            user_id=user.id,
            resume_text=p["summary"],
            is_profile_complete=True,
        )
        db.add(candidate)
        await db.flush()
        
        # åˆ›å»ºå€™é€‰äººç”»åƒï¼ˆå®Œæ•´æ•°æ®ï¼‰
        profile = CandidateProfile(
            candidate_id=candidate.id,
            display_name=p["display_name"],
            current_role=p["current_role"],
            experience_years=p["experience_years"],
            summary=p["summary"],
            ideal_job_persona=p.get("ideal_job_persona", ""),
            salary_range=p.get("salary_range", ""),
            market_demand=p.get("market_demand", ""),
            radar_data=radar,
            interview_questions=p.get("interview_questions"),
            optimization_suggestions=p.get("optimization_suggestions"),
            certifications=p.get("certifications"),
            awards=p.get("awards"),
        )
        db.add(profile)
        
        # åˆ›å»ºæŠ€èƒ½
        for skill_name in mc["skills"]:
            skill = Skill(
                candidate_id=candidate.id,
                name=skill_name,
                level=60 + hash(skill_name) % 35,
                category="æŠ€æœ¯"
            )
            db.add(skill)
        
        created_count += 1
    
    await db.commit()
    
    return {
        "message": f"åˆ›å»º {created_count} å / æ›´æ–° {updated_count} åæ¨¡æ‹Ÿå€™é€‰äºº",
        "created": created_count,
        "updated": updated_count,
        "total_candidates": existing_count + created_count,
    }


@router.post("/flows/complete-exchange")
async def complete_exchange_flows(
    req: dict,
    db: AsyncSession = Depends(get_db)
):
    """äº’æ¢è”ç³»æ–¹å¼åæ‰¹é‡å®Œæˆ Flow â€” å°†åŒæ–¹é€šè¿‡çš„å€™é€‰äººæ ‡è®°ä¸º accepted/completed"""
    from app.models.flow import FlowStatus, FlowStage, FlowTimeline
    from datetime import datetime
    
    job_ids = req.get("job_ids", [])
    candidate_names = req.get("candidate_names", [])  # é€šè¿‡çš„å€™é€‰äººåå­—åˆ—è¡¨
    
    if not job_ids:
        return {"updated": 0, "message": "æ— å²—ä½ ID"}
    
    updated = 0
    for jid in job_ids:
        # æŸ¥æ‰¾è¯¥å²—ä½ä¸‹æ‰€æœ‰å¤„äº evaluating çŠ¶æ€ï¼ˆå¾…äº’æ¢ï¼‰çš„ Flow
        flows_result = await db.execute(
            select(Flow).where(Flow.job_id == jid, Flow.status == FlowStatus.EVALUATING)
        )
        flows = flows_result.scalars().all()
        
        for flow in flows:
            flow.status = FlowStatus.ACCEPTED
            flow.current_stage = FlowStage.FINAL
            flow.last_action = "è”ç³»æ–¹å¼å·²äº’æ¢"
            flow.details = (flow.details or "") + " | è”ç³»æ–¹å¼å·²äº’æ¢ï¼Œæ‹›è˜æµç¨‹å®Œæˆ"
            flow.completed_at = datetime.utcnow()
            db.add(FlowTimeline(
                flow=flow,
                action="è”ç³»æ–¹å¼äº’æ¢å®Œæˆï¼Œæ‹›è˜å…¨æµç¨‹ç»“æŸ",
                agent_name="system",
            ))
            updated += 1
    
    if updated > 0:
        await db.commit()
    
    return {"updated": updated, "message": f"å·²å®Œæˆ {updated} æ¡æµç¨‹è®°å½•"}


@router.post("/exchange-contacts")
async def get_exchange_contacts(
    req: dict,
    db: AsyncSession = Depends(get_db)
):
    """è·å–åŒæ–¹é€šè¿‡å€™é€‰äººçš„è”ç³»æ–¹å¼ï¼ˆç”¨äºäº’æ¢è”ç³»æ–¹å¼é˜¶æ®µå±•ç¤ºï¼‰"""
    from app.models.user import User
    
    job_ids = req.get("job_ids", [])
    passed_candidates = req.get("passed_candidates", [])  # [{name, id, source, ...}]
    
    contacts = []
    for pc in passed_candidates:
        c_id = pc.get("id")
        c_name = pc.get("name", "æœªçŸ¥")
        c_source = pc.get("source", "ai_simulated")
        
        contact_info = {
            "name": c_name,
            "id": c_id,
            "source": c_source,
            "match_score": pc.get("match_score", 0),
            "employer_score": pc.get("employer_score", 0),
            "strengths": pc.get("strengths", []),
            "phone": None,
            "email": None,
        }
        
        if c_id and c_source != "ai_simulated":
            # çœŸå®å€™é€‰äºº â€” ä»æ•°æ®åº“è·å–è”ç³»æ–¹å¼
            cand_result = await db.execute(
                select(Candidate).where(Candidate.id == c_id)
            )
            candidate = cand_result.scalar_one_or_none()
            if candidate:
                user_result = await db.execute(
                    select(User).where(User.id == candidate.user_id)
                )
                user_info = user_result.scalar_one_or_none()
                if user_info:
                    contact_info["phone"] = user_info.phone or "æœªå¡«å†™"
                    contact_info["email"] = user_info.email or "æœªå¡«å†™"
        
        if not contact_info["phone"] and not contact_info["email"]:
            # AI æ¨¡æ‹Ÿå€™é€‰äºº â€” ç”Ÿæˆæ¨¡æ‹Ÿè”ç³»æ–¹å¼
            import hashlib
            name_hash = hashlib.md5(c_name.encode()).hexdigest()[:8]
            contact_info["phone"] = f"1{name_hash[:2]}****{name_hash[2:6]}"
            contact_info["email"] = f"{c_name.replace(' ', '')}@example.com"
            contact_info["is_simulated_contact"] = True
        
        contacts.append(contact_info)
    
    return {"contacts": contacts}


@router.post("/candidate-feedback")
async def submit_candidate_feedback(
    req: dict,
    db: AsyncSession = Depends(get_db)
):
    """æäº¤å€™é€‰äººåé¦ˆ â€” å†™å…¥å²—ä½æ—¥å¿— + æ ¹æ®æƒ…å†µå†™å…¥ memory"""
    from app.models.memory import Memory, MemoryType, MemoryImportance, MemoryScope
    
    job_id = req.get("job_id")
    user_id = req.get("user_id", 1)
    candidate_name = req.get("candidate_name", "æœªçŸ¥")
    rating = req.get("rating", "neutral")  # good / neutral / bad
    feedback_text = req.get("feedback_text", "")
    todo_id = req.get("todo_id")
    
    rating_labels = {"good": "æ»¡æ„", "neutral": "ä¸€èˆ¬", "bad": "ä¸æ»¡æ„"}
    rating_emojis = {"good": "ğŸ˜Š", "neutral": "ğŸ˜", "bad": "ğŸ˜"}
    rating_label = rating_labels.get(rating, "æœªçŸ¥")
    rating_emoji = rating_emojis.get(rating, "â“")
    
    # 1. å†™å…¥å²—ä½æ—¥å¿—
    log_content = f"ç”¨æˆ·å¯¹å€™é€‰äººã€Œ{candidate_name}ã€çš„åé¦ˆï¼š{rating_emoji} {rating_label}"
    if feedback_text:
        log_content += f"\nè¯„ä»·è¯¦æƒ…ï¼š{feedback_text}"
    
    if job_id:
        log = JobLog(
            job_id=job_id,
            actor_type="user",
            action=JobLogAction.USER_ACTION,
            title=f"å€™é€‰äººåé¦ˆï¼š{candidate_name} â€” {rating_label}",
            content=log_content,
            extra_data=json.dumps({
                "candidate_name": candidate_name,
                "rating": rating,
                "feedback_text": feedback_text,
                "feedback_type": "candidate_evaluation",
            }, ensure_ascii=False),
            todo_id=todo_id,
        )
        db.add(log)
    
    # 2. æ ¹æ®åé¦ˆå†…å®¹å†³å®šæ˜¯å¦å†™å…¥ memory
    memory_created = False
    memory_content = ""
    
    if rating == "bad" and feedback_text:
        # ä¸æ»¡æ„ + æœ‰å…·ä½“æè¿° â†’ å†™å…¥ä¼ä¸šè®°å¿†ï¼ˆè¦æ±‚ç±»å‹ï¼‰ï¼Œå¸®åŠ© AI æ”¹è¿›
        memory_content = f"æ‹›è˜åé¦ˆï¼šå¯¹å€™é€‰äººã€Œ{candidate_name}ã€ä¸æ»¡æ„ â€” {feedback_text}ã€‚åç»­åŒ¹é…æ—¶æ³¨æ„è§„é¿ç±»ä¼¼é—®é¢˜ã€‚"
        memory_type = MemoryType.REQUIREMENT
    elif rating == "good" and feedback_text:
        # æ»¡æ„ + æœ‰å…·ä½“æè¿° â†’ å†™å…¥ä¼ä¸šè®°å¿†ï¼ˆåå¥½ç±»å‹ï¼‰ï¼Œå¸®åŠ© AI ä¼˜åŒ–
        memory_content = f"æ‹›è˜åå¥½ï¼šå¯¹å€™é€‰äººã€Œ{candidate_name}ã€æ»¡æ„ â€” {feedback_text}ã€‚åç»­åŒ¹é…å¯å‚è€ƒæ­¤ç±»å‹äººæ‰ç‰¹å¾ã€‚"
        memory_type = MemoryType.PREFERENCE
    elif feedback_text and len(feedback_text) >= 10:
        # æœ‰è¶³å¤Ÿè¯¦ç»†çš„åé¦ˆæ–‡å­— â†’ å†™å…¥ç»éªŒè®°å¿†
        memory_content = f"æ‹›è˜ç»éªŒï¼šå…³äºå€™é€‰äººã€Œ{candidate_name}ã€â€” {feedback_text}"
        memory_type = MemoryType.EXPERIENCE
    
    if memory_content:
        # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸ä¼¼è®°å¿†
        existing_result = await db.execute(
            select(Memory)
            .where(Memory.user_id == user_id)
            .where(Memory.type == memory_type)
            .where(Memory.scope == MemoryScope.EMPLOYER)
        )
        existing_memories = existing_result.scalars().all()
        
        found_duplicate = False
        content_lower = memory_content.lower().strip()
        for existing in existing_memories:
            existing_lower = existing.content.lower().strip()
            if candidate_name.lower() in existing_lower and (
                content_lower in existing_lower or existing_lower in content_lower
            ):
                existing.emphasis_count = (existing.emphasis_count or 1) + 1
                existing.content = memory_content
                existing.updated_at = datetime.utcnow()
                found_duplicate = True
                break
        
        if not found_duplicate:
            new_memory = Memory(
                user_id=user_id,
                type=memory_type,
                content=memory_content,
                importance=MemoryImportance.HIGH if rating == "bad" else MemoryImportance.MEDIUM,
                scope=MemoryScope.EMPLOYER,
                color="border-red-300" if rating == "bad" else ("border-green-300" if rating == "good" else "border-slate-300"),
                source="feedback",
                emphasis_count=1,
                ai_reasoning=f"æ¥è‡ªæ‹›è˜æµç¨‹çš„å€™é€‰äººåé¦ˆï¼ˆ{rating_label}ï¼‰ï¼Œç”¨äºæ”¹è¿›åç»­ AI åŒ¹é…å’Œç­›é€‰è´¨é‡ã€‚",
            )
            db.add(new_memory)
        
        memory_created = True
    
    await db.commit()
    
    return {
        "success": True,
        "rating": rating,
        "rating_label": rating_label,
        "memory_created": memory_created,
        "message": f"åé¦ˆå·²è®°å½•{' å¹¶å·²å†™å…¥ä¼ä¸šè®°å¿†' if memory_created else ''}",
    }


# ========== é‚€è¯·å¥–åŠ±ç³»ç»Ÿ ==========

@router.get("/invite/stats")
async def get_invite_stats(
    user_id: int = Query(..., description="ç”¨æˆ·ID"),
    db: AsyncSession = Depends(get_db)
):
    """è·å–ç”¨æˆ·é‚€è¯·ç»Ÿè®¡ï¼šé‚€è¯·ç ã€é‚€è¯·äººæ•°ã€ç´¯è®¡ Token å¥–åŠ±ã€æœ€è¿‘é‚€è¯·è®°å½•"""
    from sqlalchemy import func
    from app.models.user import User
    from app.models.invitation import Invitation
    from app.models.token import TokenUsage, TokenAction, TokenPackage
    import string, random

    # è·å–ç”¨æˆ·ä¿¡æ¯å’Œé‚€è¯·ç 
    result = await db.execute(select(User).where(User.id == user_id))
    user = result.scalar_one_or_none()
    if not user:
        raise HTTPException(status_code=404, detail="ç”¨æˆ·ä¸å­˜åœ¨")

    # è‹¥ç”¨æˆ·è¿˜æ²¡æœ‰é‚€è¯·ç ï¼ŒåŠ¨æ€ç”Ÿæˆ
    invite_code = user.invite_code
    if not invite_code:
        chars = string.ascii_uppercase + string.digits
        for _ in range(10):
            code = ''.join(random.choices(chars, k=6))
            dup = await db.execute(select(User).where(User.invite_code == code))
            if not dup.scalar_one_or_none():
                invite_code = code
                break
        if invite_code:
            user.invite_code = invite_code
            await db.flush()

    # ç»Ÿè®¡é‚€è¯·äººæ•°
    count_result = await db.execute(
        select(func.count(Invitation.id)).where(Invitation.inviter_id == user_id)
    )
    invite_count = count_result.scalar() or 0

    # ç´¯è®¡è·å¾—çš„é‚€è¯·å¥–åŠ± Token
    reward_result = await db.execute(
        select(func.sum(Invitation.reward_tokens)).where(Invitation.inviter_id == user_id)
    )
    total_reward_tokens = reward_result.scalar() or 0

    # Token ä½™é¢
    pkg_result = await db.execute(
        select(func.sum(TokenPackage.remaining_tokens))
        .where(TokenPackage.user_id == user_id, TokenPackage.is_active == True)
    )
    token_balance = pkg_result.scalar() or 100000  # é»˜è®¤10ä¸‡

    # æœ€è¿‘ 10 æ¡é‚€è¯·è®°å½•
    records_result = await db.execute(
        select(Invitation)
        .where(Invitation.inviter_id == user_id)
        .order_by(Invitation.created_at.desc())
        .limit(10)
    )
    records = records_result.scalars().all()

    invite_records = []
    for r in records:
        invitee_result = await db.execute(select(User).where(User.id == r.invitee_id))
        invitee = invitee_result.scalar_one_or_none()
        invitee_name = invitee.name if invitee else "æœªçŸ¥ç”¨æˆ·"
        # è„±æ•å¤„ç†
        if len(invitee_name) > 1:
            masked_name = invitee_name[0] + "*" * (len(invitee_name) - 1)
        else:
            masked_name = invitee_name + "**"
        invite_records.append({
            "id": r.id,
            "invitee_name": masked_name,
            "reward_tokens": r.reward_tokens,
            "status": r.status,
            "created_at": r.created_at.isoformat() if r.created_at else None,
        })

    return {
        "invite_code": invite_code,
        "invite_link": f"https://devnors.ai/register?ref={invite_code}",
        "invite_count": invite_count,
        "total_reward_tokens": total_reward_tokens,
        "token_balance": token_balance,
        "records": invite_records,
        "rules": {
            "per_invite_reward": 500,
            "new_user_bonus": 200,
            "milestone_5": 1000,
            "milestone_10": 3000,
            "milestone_20": 8000,
        },
    }


@router.get("/invite/records")
async def get_invite_records(
    user_id: int = Query(..., description="ç”¨æˆ·ID"),
    limit: int = Query(20, description="æ¯é¡µæ¡æ•°"),
    offset: int = Query(0, description="åç§»é‡"),
    db: AsyncSession = Depends(get_db)
):
    """è·å–è¯¦ç»†é‚€è¯·è®°å½•åˆ—è¡¨ï¼ˆåˆ†é¡µï¼‰"""
    from sqlalchemy import func
    from app.models.user import User
    from app.models.invitation import Invitation

    # æ€»æ•°
    count_result = await db.execute(
        select(func.count(Invitation.id)).where(Invitation.inviter_id == user_id)
    )
    total = count_result.scalar() or 0

    # åˆ†é¡µæŸ¥è¯¢
    records_result = await db.execute(
        select(Invitation)
        .where(Invitation.inviter_id == user_id)
        .order_by(Invitation.created_at.desc())
        .limit(limit)
        .offset(offset)
    )
    records = records_result.scalars().all()

    items = []
    for r in records:
        invitee_result = await db.execute(select(User).where(User.id == r.invitee_id))
        invitee = invitee_result.scalar_one_or_none()
        invitee_name = invitee.name if invitee else "æœªçŸ¥ç”¨æˆ·"
        if len(invitee_name) > 1:
            masked_name = invitee_name[0] + "*" * (len(invitee_name) - 1)
        else:
            masked_name = invitee_name + "**"
        items.append({
            "id": r.id,
            "invitee_name": masked_name,
            "invite_code": r.invite_code,
            "reward_tokens": r.reward_tokens,
            "status": r.status,
            "created_at": r.created_at.isoformat() if r.created_at else None,
        })

    return {"total": total, "items": items}

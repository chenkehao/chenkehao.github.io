"""
Public API Router - å…¬å¼€æ¥å£ï¼Œæ— éœ€ç™»å½•
"""

import json
from typing import List, Optional
from datetime import datetime, timedelta

from fastapi import APIRouter, Depends, Query
from sqlalchemy import select, func
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import selectinload

from app.database import get_db
from app.models.job import Job, JobTag, JobStatus
from app.models.flow import Flow, FlowStatus, FlowStage
from app.models.candidate import Candidate, CandidateProfile
from app.schemas.job import JobListResponse, JobResponse

router = APIRouter()


# ============ èŒä½ç›¸å…³å…¬å¼€æ¥å£ ============

@router.get("/jobs", response_model=JobListResponse)
async def list_public_jobs(
    page: int = Query(1, ge=1),
    page_size: int = Query(20, ge=1, le=100),
    search: Optional[str] = None,
    location: Optional[str] = None,
    db: AsyncSession = Depends(get_db)
):
    """è·å–å…¬å¼€èŒä½åˆ—è¡¨"""
    from sqlalchemy import or_
    
    query = select(Job).options(selectinload(Job.tags)).where(Job.status == JobStatus.ACTIVE)
    
    if search:
        query = query.where(
            or_(
                Job.title.ilike(f"%{search}%"),
                Job.company.ilike(f"%{search}%"),
                Job.description.ilike(f"%{search}%")
            )
        )
    
    if location:
        query = query.where(Job.location.ilike(f"%{location}%"))
    
    count_query = select(func.count()).select_from(query.subquery())
    total_result = await db.execute(count_query)
    total = total_result.scalar()
    
    offset = (page - 1) * page_size
    query = query.offset(offset).limit(page_size).order_by(Job.created_at.desc())
    
    result = await db.execute(query)
    jobs = result.scalars().all()
    
    return JobListResponse(
        items=jobs,
        total=total,
        page=page,
        page_size=page_size,
        pages=(total + page_size - 1) // page_size if total > 0 else 0
    )


@router.get("/jobs/{job_id}")
async def get_public_job(
    job_id: int,
    db: AsyncSession = Depends(get_db)
):
    """è·å–èŒä½è¯¦æƒ…"""
    result = await db.execute(
        select(Job).options(selectinload(Job.tags)).where(Job.id == job_id)
    )
    job = result.scalar_one_or_none()
    
    if not job:
        return {"error": "èŒä½ä¸å­˜åœ¨"}
    
    # å¢åŠ æµè§ˆé‡
    job.view_count += 1
    await db.commit()
    
    return {
        "id": job.id,
        "title": job.title,
        "company": job.company,
        "location": job.location,
        "description": job.description,
        "salary_min": job.salary_min,
        "salary_max": job.salary_max,
        "salary": f"Â¥{job.salary_min//1000}k - Â¥{job.salary_max//1000}k" if job.salary_min and job.salary_max else "é¢è®®",
        "job_type": job.job_type,
        "requirements": job.requirements,
        "benefits": job.benefits,
        "tags": [tag.name for tag in job.tags],
        "logo": job.logo or "ğŸ’¼",
        "ai_intro": job.ai_intro or "AI æ™ºèƒ½ä½“æ­£åœ¨åˆ†æèŒä½åŒ¹é…åº¦",
        "view_count": job.view_count,
        "created_at": job.created_at.isoformat() if job.created_at else None,
    }


@router.get("/jobs-recommended")
async def get_recommended_jobs(
    limit: int = Query(5, ge=1, le=20),
    db: AsyncSession = Depends(get_db)
):
    """è·å–æ¨èèŒä½åˆ—è¡¨"""
    result = await db.execute(
        select(Job)
        .options(selectinload(Job.tags))
        .where(Job.status == JobStatus.ACTIVE)
        .order_by(Job.created_at.desc())
        .limit(limit)
    )
    jobs = result.scalars().all()
    
    return [{
        "id": job.id,
        "title": job.title,
        "company": job.company,
        "location": job.location,
        "salary": f"Â¥{job.salary_min//1000}k - Â¥{job.salary_max//1000}k" if job.salary_min and job.salary_max else "é¢è®®",
        "match": 85 + (job.id % 15),  # æ¨¡æ‹ŸåŒ¹é…åº¦
        "tags": [tag.name for tag in job.tags][:3],
        "logo": job.logo or "ğŸ’¼",
        "aiIntro": job.ai_intro or "AI æ™ºèƒ½ä½“æ­£åœ¨åˆ†æèŒä½åŒ¹é…åº¦",
    } for job in jobs]


# ============ å·¥ä½œæµç›¸å…³å…¬å¼€æ¥å£ ============

@router.get("/flows")
async def get_public_flows(
    limit: int = Query(10, ge=1, le=50),
    db: AsyncSession = Depends(get_db)
):
    """è·å–å·¥ä½œæµåˆ—è¡¨ï¼ˆç¤ºä¾‹æ•°æ®ï¼‰"""
    result = await db.execute(
        select(Flow)
        .options(selectinload(Flow.steps), selectinload(Flow.timeline))
        .order_by(Flow.updated_at.desc())
        .limit(limit)
    )
    flows = result.scalars().all()
    
    # è·å–å…³è”çš„èŒä½å’Œå€™é€‰äººä¿¡æ¯
    flow_list = []
    for flow in flows:
        # è·å–èŒä½ä¿¡æ¯
        job_result = await db.execute(select(Job).where(Job.id == flow.job_id))
        job = job_result.scalar_one_or_none()
        
        # è·å–å€™é€‰äººä¿¡æ¯
        candidate_result = await db.execute(
            select(Candidate)
            .options(selectinload(Candidate.profile))
            .where(Candidate.id == flow.candidate_id)
        )
        candidate = candidate_result.scalar_one_or_none()
        
        # æ˜ å°„çŠ¶æ€ä¸ºå‰ç«¯å‹å¥½çš„æ ¼å¼
        status_map = {
            "interviewing": "active",
            "offer": "completed",
            "accepted": "completed",
        }
        frontend_status = status_map.get(flow.status.value, "pending")
        
        flow_list.append({
            "id": flow.id,
            "candidateName": candidate.profile.display_name if candidate and candidate.profile else "æœªçŸ¥",
            "role": job.title if job else "æœªçŸ¥èŒä½",
            "company": job.company if job else "æœªçŸ¥å…¬å¸",
            "stage": flow.current_stage.value,
            "status": frontend_status,
            "matchScore": flow.match_score or 0,
            "currentStep": flow.current_step,
            "totalSteps": 5,
            "tokensConsumed": flow.tokens_consumed,
            "agentsUsed": flow.agents_used or [],
            "timeline": [{
                "action": t.action,
                "agent": t.agent_name,
                "time": t.timestamp.isoformat() if t.timestamp else None,
            } for t in (flow.timeline or [])[:5]],
            "updatedAt": flow.updated_at.isoformat() if flow.updated_at else None,
        })
    
    return flow_list


@router.get("/flows/{flow_id}")
async def get_public_flow(
    flow_id: int,
    db: AsyncSession = Depends(get_db)
):
    """è·å–å·¥ä½œæµè¯¦æƒ…"""
    result = await db.execute(
        select(Flow)
        .options(selectinload(Flow.steps), selectinload(Flow.timeline))
        .where(Flow.id == flow_id)
    )
    flow = result.scalar_one_or_none()
    
    if not flow:
        return {"error": "æµç¨‹ä¸å­˜åœ¨"}
    
    # è·å–èŒä½ä¿¡æ¯
    job_result = await db.execute(select(Job).where(Job.id == flow.job_id))
    job = job_result.scalar_one_or_none()
    
    # è·å–å€™é€‰äººä¿¡æ¯
    candidate_result = await db.execute(
        select(Candidate)
        .options(selectinload(Candidate.profile))
        .where(Candidate.id == flow.candidate_id)
    )
    candidate = candidate_result.scalar_one_or_none()
    
    return {
        "id": flow.id,
        "candidateName": candidate.profile.display_name if candidate and candidate.profile else "æœªçŸ¥",
        "role": job.title if job else "æœªçŸ¥èŒä½",
        "company": job.company if job else "æœªçŸ¥å…¬å¸",
        "stage": flow.current_stage.value,
        "status": flow.status.value,
        "matchScore": flow.match_score or 0,
        "currentStep": flow.current_step,
        "totalSteps": 5,
        "tokensConsumed": flow.tokens_consumed,
        "agentsUsed": flow.agents_used or [],
        "steps": [{
            "name": s.name,
            "stage": s.stage.value,
            "isCompleted": s.is_completed,
            "completedAt": s.completed_at.isoformat() if s.completed_at else None,
        } for s in sorted(flow.steps or [], key=lambda x: x.order)],
        "timeline": [{
            "action": t.action,
            "agent": t.agent_name,
            "tokens": t.tokens_used,
            "time": t.timestamp.isoformat() if t.timestamp else None,
        } for t in (flow.timeline or [])],
    }


# ============ å€™é€‰äºº/äººæ‰ç›¸å…³å…¬å¼€æ¥å£ ============

@router.get("/talents")
async def get_public_talents(
    limit: int = Query(10, ge=1, le=50),
    db: AsyncSession = Depends(get_db)
):
    """è·å–äººæ‰åˆ—è¡¨"""
    result = await db.execute(
        select(Candidate)
        .options(selectinload(Candidate.profile), selectinload(Candidate.skills))
        .where(Candidate.is_profile_complete == True)
        .limit(limit)
    )
    candidates = result.scalars().all()
    
    return [{
        "id": c.id,
        "name": c.profile.display_name if c.profile else "æœªçŸ¥",
        "role": c.profile.current_role if c.profile else "æœªçŸ¥èŒä½",
        "experienceYears": c.profile.experience_years if c.profile else 0,
        "skills": [s.name for s in (c.skills or [])][:5],
        "radarData": c.profile.radar_data if c.profile else [],
        "summary": c.profile.summary if c.profile else "",
        "matchScore": 85 + (c.id % 15),  # æ¨¡æ‹ŸåŒ¹é…åº¦
        "status": "active",
        "targetJobId": 1,  # æ¨¡æ‹Ÿç›®æ ‡èŒä½
    } for c in candidates]


# ============ ç»Ÿè®¡ç›¸å…³å…¬å¼€æ¥å£ ============

@router.get("/stats/token-usage")
async def get_token_usage_stats(
    db: AsyncSession = Depends(get_db)
):
    """è·å– Token ä½¿ç”¨ç»Ÿè®¡"""
    # æ¨¡æ‹Ÿç»Ÿè®¡æ•°æ®
    return {
        "balance": 1245000,
        "used_today": 12500,
        "used_this_week": 85000,
        "used_this_month": 320000,
        "history": [
            {"date": (datetime.now() - timedelta(days=i)).strftime("%Y-%m-%d"), 
             "amount": 8000 + (i * 500) % 5000, 
             "action": ["ç®€å†è§£æ", "èŒä½åŒ¹é…", "é¢è¯•æ¨¡æ‹Ÿ", "å¸‚åœºåˆ†æ"][i % 4]}
            for i in range(7)
        ],
        "chart": [
            {"name": f"{i+1}æœˆ", "tokens": 20000 + (i * 5000) % 30000}
            for i in range(6)
        ],
    }


@router.get("/stats/qualifications")
async def get_qualifications_stats(
    db: AsyncSession = Depends(get_db)
):
    """è·å–èµ„è´¨è®¤è¯ç»Ÿè®¡"""
    return [
        {"name": "AI æ‹›è˜èµ„æ ¼è®¤è¯", "status": "å·²è®¤è¯", "icon": "ğŸ†"},
        {"name": "æ•°æ®å®‰å…¨åˆè§„", "status": "é€šè¿‡", "icon": "ğŸ”"},
        {"name": "GDPR åˆè§„", "status": "å·²è®¤è¯", "icon": "ğŸŒ"},
    ]


# ============ è®°å¿†/ä»»åŠ¡ç›¸å…³å…¬å¼€æ¥å£ ============

from app.models.memory import Memory, MemoryType, MemoryImportance, MemoryScope
from pydantic import BaseModel

class MemoryCreate(BaseModel):
    type: str
    content: str
    importance: str = "Medium"
    scope: str = "candidate"  # candidate æˆ– employer

# ç±»å‹åˆ°é¢œè‰²çš„æ˜ å°„
TYPE_COLOR_MAP = {
    "culture": "border-rose-300",
    "tech": "border-indigo-300",
    "skill": "border-emerald-300",
    "experience": "border-amber-300",
    "salary": "border-green-300",
    "location": "border-sky-300",
    "reporting": "border-violet-300",
    "team": "border-teal-300",
    "project": "border-amber-300",
    "goal": "border-rose-300",
    "preference": "border-purple-300",
    "company": "border-blue-300",
    "requirement": "border-orange-300",
    "benefit": "border-cyan-300",
}

# ç±»å‹åç§°æ˜ å°„
TYPE_NAME_MAP = {
    "culture": "æ–‡åŒ–",
    "tech": "æŠ€æœ¯",
    "skill": "æŠ€èƒ½",
    "experience": "ç»éªŒ",
    "salary": "è–ªé…¬",
    "location": "åœ°ç‚¹",
    "reporting": "æ±‡æŠ¥",
    "team": "å›¢é˜Ÿ",
    "project": "é¡¹ç›®",
    "goal": "ç›®æ ‡",
    "preference": "åå¥½",
    "company": "å…¬å¸",
    "requirement": "éœ€æ±‚",
    "benefit": "ç¦åˆ©",
}

@router.get("/memories")
async def get_memories(
    user_id: int = Query(1, description="ç”¨æˆ·ID"),
    scope: str = Query("candidate", description="è®°å¿†èŒƒå›´: candidate(äººæ‰ç”»åƒ) æˆ– employer(ä¼ä¸šç”»åƒ)"),
    db: AsyncSession = Depends(get_db)
):
    """è·å–ç”¨æˆ·è®°å¿† - åŒºåˆ†äººæ‰ç”»åƒå’Œä¼ä¸šç”»åƒ"""
    # è§£æ scope
    try:
        memory_scope = MemoryScope(scope.lower())
    except ValueError:
        memory_scope = MemoryScope.CANDIDATE
    
    result = await db.execute(
        select(Memory)
        .where(Memory.user_id == user_id)
        .where(Memory.scope == memory_scope)
        .order_by(Memory.created_at.desc())
    )
    memories = result.scalars().all()
    
    # å¦‚æœæ²¡æœ‰è®°å¿†ï¼Œè¿”å›ç©ºæ•°ç»„ï¼ˆè®©ç”¨æˆ·è‡ªå·±æ·»åŠ æ•°æ®ï¼‰
    if not memories:
        return []
    
    return [{
        "id": m.id,
        "type": TYPE_NAME_MAP.get(m.type.value, m.type.value).upper(),
        "content": m.content,
        "date": m.created_at.strftime("%Y-%m"),
        "color": m.color or TYPE_COLOR_MAP.get(m.type.value, "border-slate-300"),
        "importance": m.importance.value if m.importance else "Medium",
        "scope": m.scope.value if m.scope else "candidate",
        "emphasis_count": m.emphasis_count or 1,
        "ai_reasoning": m.ai_reasoning,
    } for m in memories]


@router.post("/memories")
async def create_memory(
    memory: MemoryCreate,
    user_id: int = Query(1, description="ç”¨æˆ·ID"),
    force_create: bool = Query(False, description="æ˜¯å¦å¼ºåˆ¶åˆ›å»ºï¼ˆå¿½ç•¥é‡å¤æ£€æŸ¥ï¼‰"),
    db: AsyncSession = Depends(get_db)
):
    """åˆ›å»ºæ–°è®°å¿† - è‡ªåŠ¨æ£€æŸ¥é‡å¤å†…å®¹ï¼Œé‡å¤åˆ™å¢åŠ å¼ºè°ƒæ¬¡æ•°"""
    # éªŒè¯ç±»å‹
    try:
        memory_type = MemoryType(memory.type.lower())
    except ValueError:
        memory_type = MemoryType.SKILL  # é»˜è®¤ä¸ºæŠ€èƒ½ç±»å‹
    
    # éªŒè¯é‡è¦æ€§
    try:
        importance = MemoryImportance(memory.importance)
    except ValueError:
        importance = MemoryImportance.MEDIUM
    
    # éªŒè¯ scope
    try:
        memory_scope = MemoryScope(memory.scope.lower())
    except ValueError:
        memory_scope = MemoryScope.CANDIDATE
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸ä¼¼çš„è®°å¿†ï¼ˆåŒç±»å‹ã€åŒèŒƒå›´ã€å†…å®¹ç›¸ä¼¼ï¼‰
    if not force_create:
        result = await db.execute(
            select(Memory)
            .where(Memory.user_id == user_id)
            .where(Memory.type == memory_type)
            .where(Memory.scope == memory_scope)
        )
        existing_memories = result.scalars().all()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å†…å®¹ç›¸åŒæˆ–é«˜åº¦ç›¸ä¼¼çš„è®°å¿†
        content_lower = memory.content.lower().strip()
        for existing in existing_memories:
            existing_content = existing.content.lower().strip()
            # å®Œå…¨ç›¸åŒæˆ–å†…å®¹åŒ…å«å…³ç³»
            if (content_lower == existing_content or 
                content_lower in existing_content or 
                existing_content in content_lower):
                # å¢åŠ å¼ºè°ƒæ¬¡æ•°è€Œä¸æ˜¯åˆ›å»ºæ–°è®°å¿†
                existing.emphasis_count = (existing.emphasis_count or 1) + 1
                existing.updated_at = datetime.utcnow()
                # å¦‚æœæ–°å†…å®¹æ›´é•¿æ›´è¯¦ç»†ï¼Œæ›´æ–°å†…å®¹
                if len(memory.content) > len(existing.content):
                    existing.content = memory.content
                await db.commit()
                await db.refresh(existing)
                
                return {
                    "id": existing.id,
                    "type": TYPE_NAME_MAP.get(memory_type.value, memory_type.value).upper(),
                    "content": existing.content,
                    "date": existing.updated_at.strftime("%Y-%m"),
                    "color": existing.color,
                    "importance": existing.importance.value,
                    "emphasis_count": existing.emphasis_count,
                    "message": f"è®°å¿†å·²å¼ºè°ƒ {existing.emphasis_count} æ¬¡",
                    "is_duplicate": True
                }
    
    # è·å–é¢œè‰²
    color = TYPE_COLOR_MAP.get(memory_type.value, "border-slate-300")
    
    # åˆ›å»ºæ–°è®°å¿†
    new_memory = Memory(
        user_id=user_id,
        type=memory_type,
        content=memory.content,
        importance=importance,
        scope=memory_scope,
        color=color,
        source="manual",
        emphasis_count=1,
    )
    
    db.add(new_memory)
    await db.commit()
    await db.refresh(new_memory)
    
    return {
        "id": new_memory.id,
        "type": TYPE_NAME_MAP.get(memory_type.value, memory_type.value).upper(),
        "content": new_memory.content,
        "date": new_memory.created_at.strftime("%Y-%m"),
        "color": new_memory.color,
        "importance": new_memory.importance.value,
        "emphasis_count": 1,
        "message": "è®°å¿†åˆ›å»ºæˆåŠŸ",
        "is_duplicate": False
    }


@router.delete("/memories/{memory_id}")
async def delete_memory(
    memory_id: int,
    db: AsyncSession = Depends(get_db)
):
    """åˆ é™¤è®°å¿†"""
    result = await db.execute(select(Memory).where(Memory.id == memory_id))
    memory = result.scalar_one_or_none()
    
    if not memory:
        return {"error": "è®°å¿†ä¸å­˜åœ¨"}
    
    await db.delete(memory)
    await db.commit()
    
    return {"message": "è®°å¿†å·²åˆ é™¤"}


@router.post("/memories/optimize")
async def optimize_memories(
    user_id: int = Query(..., description="ç”¨æˆ·ID"),
    scope: str = Query("candidate", description="è®°å¿†èŒƒå›´: candidate æˆ– employer"),
    db: AsyncSession = Depends(get_db)
):
    """AI é©±åŠ¨çš„è®°å¿†ä¼˜åŒ–
    
    1. åˆå¹¶é‡å¤åŒç±»è®°å¿†ï¼Œåˆå¹¶åä¿ç•™çš„è®°å¿†å¾—åˆ°å¢å¼º
    2. åˆ é™¤ä¸é‡è¦çš„æ— æ„ä¹‰çš„è®°å¿†
    3. æ£€æŸ¥è®°å¿†åˆ†ç±»çš„å‡†ç¡®æ€§å¹¶ä¿®æ­£
    4. ä»ç”¨æˆ·è¡Œä¸ºä¸­æ€»ç»“æ–°è®°å¿†
    5. ä¸ºæ‰€æœ‰ä¿ç•™çš„è®°å¿†ç”Ÿæˆ Agent æ¨ç†é€»è¾‘
    """
    import json as json_module
    from datetime import datetime
    
    try:
        memory_scope = MemoryScope(scope.lower())
    except ValueError:
        memory_scope = MemoryScope.CANDIDATE
    
    # è·å–ç”¨æˆ·æ‰€æœ‰è®°å¿†
    result = await db.execute(
        select(Memory)
        .where(Memory.user_id == user_id)
        .where(Memory.scope == memory_scope)
        .order_by(Memory.created_at.desc())
    )
    memories = result.scalars().all()
    
    if not memories:
        return {
            "success": True,
            "message": "æ²¡æœ‰éœ€è¦ä¼˜åŒ–çš„è®°å¿†",
            "actions": [],
            "summary": {"merged": 0, "deleted": 0, "reclassified": 0, "created": 0, "reasoning_updated": 0}
        }
    
    # å‡†å¤‡è®°å¿†æ•°æ®ä¾› AI åˆ†æ
    memories_data = [{
        "id": m.id,
        "type": m.type.value,
        "content": m.content,
        "importance": m.importance.value if m.importance else "Medium",
        "emphasis_count": m.emphasis_count or 1,
        "created_at": m.created_at.strftime("%Y-%m-%d")
    } for m in memories]
    
    scope_desc = "äººæ‰ç”»åƒ" if memory_scope == MemoryScope.CANDIDATE else "ä¼ä¸šç”»åƒ"
    
    # æ„å»º AI åˆ†ææç¤º
    analysis_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½è®°å¿†ä¼˜åŒ–åŠ©æ‰‹ã€‚è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·çš„{scope_desc}è®°å¿†æ•°æ®ï¼Œå¹¶ç»™å‡ºä¼˜åŒ–å»ºè®®ã€‚

ç”¨æˆ·è®°å¿†ç±»å‹è¯´æ˜ï¼š
- skill: æŠ€èƒ½/èƒ½åŠ›
- experience: å·¥ä½œç»å†
- goal: èŒä¸šç›®æ ‡
- preference: æ±‚èŒåå¥½
- culture: æ–‡åŒ–åå¥½
- tech: æŠ€æœ¯è¦æ±‚
- salary: è–ªé…¬æœŸæœ›
- location: å·¥ä½œåœ°ç‚¹
- requirement: æ‹›è˜éœ€æ±‚ï¼ˆä¼ä¸šï¼‰
- company: å…¬å¸ä»‹ç»ï¼ˆä¼ä¸šï¼‰
- benefit: ç¦åˆ©å¾…é‡ï¼ˆä¼ä¸šï¼‰

å½“å‰è®°å¿†æ•°æ®ï¼š
{json_module.dumps(memories_data, ensure_ascii=False, indent=2)}

è¯·åˆ†æå¹¶è¿”å› JSON æ ¼å¼çš„ä¼˜åŒ–å»ºè®®ï¼š
{{
  "merge": [
    {{
      "keep_id": ä¿ç•™çš„è®°å¿†ID,
      "delete_ids": [è¦åˆå¹¶åˆ é™¤çš„è®°å¿†IDåˆ—è¡¨],
      "new_content": "åˆå¹¶åçš„æ–°å†…å®¹",
      "reason": "åˆå¹¶åŸå› ",
      "ai_reasoning": "åˆå¹¶åè¿™æ¡è®°å¿†çš„ Agent æ¨ç†é€»è¾‘ï¼ˆè¯´æ˜ä¸ºä»€ä¹ˆä¿ç•™è¿™æ¡è®°å¿†ã€å®ƒå¯¹ç”¨æˆ·æœ‰ä»€ä¹ˆä»·å€¼ï¼‰"
    }}
  ],
  "delete": [
    {{
      "id": è¦åˆ é™¤çš„è®°å¿†ID,
      "reason": "åˆ é™¤åŸå› ï¼ˆå¦‚ï¼šå†…å®¹æ— æ„ä¹‰ã€é‡å¤ã€è¿‡æ—¶ç­‰ï¼‰"
    }}
  ],
  "reclassify": [
    {{
      "id": è®°å¿†ID,
      "old_type": "åŸåˆ†ç±»",
      "new_type": "æ­£ç¡®çš„åˆ†ç±»",
      "reason": "é‡æ–°åˆ†ç±»çš„åŸå› ",
      "ai_reasoning": "é‡æ–°åˆ†ç±»åçš„ Agent æ¨ç†é€»è¾‘"
    }}
  ],
  "create": [
    {{
      "type": "è®°å¿†ç±»å‹",
      "content": "æ ¹æ®ç°æœ‰è®°å¿†æ¨æ–­å‡ºçš„æ–°è®°å¿†å†…å®¹",
      "importance": "High/Medium/Low",
      "reason": "åˆ›å»ºåŸå› ",
      "ai_reasoning": "åˆ›å»ºè¿™æ¡è®°å¿†çš„ Agent æ¨ç†é€»è¾‘ï¼ˆè¯´æ˜è¿™æ¡è®°å¿†æ˜¯å¦‚ä½•ä»å·²æœ‰ä¿¡æ¯æ¨æ–­å‡ºæ¥çš„ï¼‰"
    }}
  ],
  "reasoning_updates": [
    {{
      "id": è®°å¿†ID,
      "ai_reasoning": "ä¸ºè¿™æ¡è®°å¿†ç”Ÿæˆçš„ Agent æ¨ç†é€»è¾‘ï¼ˆè§£é‡Šä¸ºä»€ä¹ˆè¿™æ¡è®°å¿†å¯¹ç”¨æˆ·é‡è¦ã€å®ƒå°†å¦‚ä½•å½±å“åç»­çš„èŒä½/äººæ‰åŒ¹é…ï¼‰"
    }}
  ]
}}

æ³¨æ„ï¼š
1. åªè¿”å› JSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—
2. åˆå¹¶ç›¸ä¼¼å†…å®¹çš„è®°å¿†ï¼Œä¿ç•™ä¿¡æ¯æœ€å…¨é¢çš„
3. åˆ é™¤æ˜æ˜¾æ— æ„ä¹‰æˆ–è¿‡äºç®€çŸ­çš„è®°å¿†ï¼ˆå¦‚åªæœ‰ä¸€ä¸¤ä¸ªå­—ï¼‰
4. æ£€æŸ¥åˆ†ç±»æ˜¯å¦å‡†ç¡®ï¼ŒæŠ€èƒ½åº”è¯¥åœ¨ skillï¼Œç»å†åº”è¯¥åœ¨ experience ç­‰
5. å¦‚æœèƒ½ä»ç°æœ‰è®°å¿†æ¨æ–­å‡ºæœ‰ä»·å€¼çš„æ–°ä¿¡æ¯ï¼Œå¯ä»¥å»ºè®®åˆ›å»ºæ–°è®°å¿†
6. ä¿å®ˆæ“ä½œï¼Œä¸ç¡®å®šçš„ä¸è¦åˆ é™¤
7. **é‡è¦**ï¼šåœ¨ reasoning_updates ä¸­ä¸ºæ‰€æœ‰ä¿ç•™çš„è®°å¿†ç”Ÿæˆ Agent æ¨ç†é€»è¾‘ï¼Œè§£é‡Šè¿™æ¡è®°å¿†çš„ä»·å€¼å’Œæ„ä¹‰
8. ai_reasoning åº”è¯¥æ˜¯ä¸€æ®µç®€çŸ­çš„æè¿°ï¼ˆ30-80å­—ï¼‰ï¼Œè§£é‡Š Agent ä¸ºä»€ä¹ˆä¿ç•™/åˆ›å»ºè¿™æ¡è®°å¿†ï¼Œä»¥åŠå®ƒå°†å¦‚ä½•å½±å“åç»­åŒ¹é…"""

    # è°ƒç”¨ AI åˆ†æ
    try:
        from app.services.ai_service import get_ai_service
        ai_service = get_ai_service()
        ai_response = await ai_service.chat(analysis_prompt)
        
        # è§£æ AI å“åº”
        # å°è¯•æå– JSON
        response_text = ai_response.strip()
        if response_text.startswith("```"):
            # ç§»é™¤ markdown ä»£ç å—æ ‡è®°
            lines = response_text.split("\n")
            response_text = "\n".join(lines[1:-1] if lines[-1] == "```" else lines[1:])
        
        optimization_plan = json_module.loads(response_text)
    except Exception as e:
        print(f"AI åˆ†æå¤±è´¥: {e}")
        # å¦‚æœ AI åˆ†æå¤±è´¥ï¼Œæ‰§è¡Œç®€å•çš„é‡å¤æ£€æµ‹
        optimization_plan = {"merge": [], "delete": [], "reclassify": [], "create": [], "reasoning_updates": []}
        
        # æŒ‰ç±»å‹åˆ†ç»„è®°å¿†
        type_groups = {}
        for m in memories:
            type_key = m.type.value if m.type else 'unknown'
            if type_key not in type_groups:
                type_groups[type_key] = []
            type_groups[type_key].append(m)
        
        # ç‰¹æ®Šå¤„ç†ï¼šå¯¹äº preference ç±»å‹ï¼Œåªä¿ç•™æœ€æ–°çš„ä¸€æ¡ï¼ˆå› ä¸ºæ±‚èŒåå¥½ä¼šæ›´æ–°ï¼‰
        if 'preference' in type_groups:
            pref_memories = type_groups['preference']
            # æŒ‰åˆ›å»ºæ—¶é—´é™åºæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
            pref_memories.sort(key=lambda x: x.created_at, reverse=True)
            
            if len(pref_memories) > 1:
                # ä¿ç•™æœ€æ–°çš„ï¼Œåˆ é™¤å…¶ä»–çš„
                keep_memory = pref_memories[0]
                delete_memories = pref_memories[1:]
                
                # æ£€æŸ¥æ˜¯å¦éƒ½æ˜¯æ±‚èŒåå¥½ä¿¡æ¯ï¼ˆä»¥ã€æ±‚èŒåå¥½ä¿¡æ¯ã€‘å¼€å¤´ï¼‰
                job_pref_memories = [m for m in pref_memories if m.content.startswith('ã€æ±‚èŒåå¥½ä¿¡æ¯ã€‘')]
                if len(job_pref_memories) > 1:
                    keep_memory = job_pref_memories[0]
                    delete_ids = [m.id for m in job_pref_memories[1:]]
                    
                    optimization_plan["merge"].append({
                        "keep_id": keep_memory.id,
                        "delete_ids": delete_ids,
                        "new_content": None,
                        "reason": f"åˆå¹¶ {len(delete_ids)} æ¡æ—§çš„æ±‚èŒåå¥½è®°å½•ï¼Œä¿ç•™æœ€æ–°ç‰ˆæœ¬",
                        "ai_reasoning": "ä¿ç•™ç”¨æˆ·æœ€æ–°çš„æ±‚èŒåå¥½ä¿¡æ¯ï¼Œåˆ é™¤å†å²ç‰ˆæœ¬ä»¥é¿å…å†²çªã€‚æœ€æ–°åå¥½å°†ç”¨äºæ™ºèƒ½å²—ä½åŒ¹é…ã€‚"
                    })
        
        # å¯¹äºåŒç±»å‹è®°å¿†ï¼Œæ£€æŸ¥å†…å®¹ç›¸ä¼¼åº¦
        seen_contents = {}
        processed_ids = set()
        
        # æ”¶é›†å·²è¢«åˆå¹¶å¤„ç†çš„ID
        for merge in optimization_plan.get("merge", []):
            processed_ids.add(merge.get("keep_id"))
            for del_id in merge.get("delete_ids", []):
                processed_ids.add(del_id)
        
        for m in memories:
            if m.id in processed_ids:
                continue
                
            content_key = m.content.lower().strip()[:50]  # å–å‰50å­—ç¬¦ä½œä¸ºé”®
            if content_key in seen_contents:
                # å‘ç°é‡å¤ï¼Œåˆå¹¶åˆ°ç¬¬ä¸€ä¸ª
                existing_id = seen_contents[content_key]
                optimization_plan["merge"].append({
                    "keep_id": existing_id,
                    "delete_ids": [m.id],
                    "new_content": None,
                    "reason": "å†…å®¹é‡å¤",
                    "ai_reasoning": "åˆå¹¶é‡å¤è®°å¿†ä»¥å¢å¼ºè®°å¿†å¼ºåº¦"
                })
            else:
                seen_contents[content_key] = m.id
                # ä¸ºæ¯æ¡è®°å¿†ç”Ÿæˆé»˜è®¤æ¨ç†é€»è¾‘
                default_reasoning = f"åŸºäºç”¨æˆ·{'èŒä¸šå±¥å†' if memory_scope == MemoryScope.CANDIDATE else 'æ‹›è˜å†å²'}è‡ªåŠ¨æå–çš„{TYPE_NAME_MAP.get(m.type.value, m.type.value)}ä¿¡æ¯ï¼Œç”¨äºä¼˜åŒ–åç»­{'èŒä½' if memory_scope == MemoryScope.CANDIDATE else 'äººæ‰'}åŒ¹é…ã€‚"
                optimization_plan["reasoning_updates"].append({
                    "id": m.id,
                    "ai_reasoning": default_reasoning
                })
    
    # æ‰§è¡Œä¼˜åŒ–æ“ä½œ
    actions = []
    summary = {"merged": 0, "deleted": 0, "reclassified": 0, "created": 0, "reasoning_updated": 0}
    
    # 1. æ‰§è¡Œåˆå¹¶
    for merge_action in optimization_plan.get("merge", []):
        keep_id = merge_action.get("keep_id")
        delete_ids = merge_action.get("delete_ids", [])
        new_content = merge_action.get("new_content")
        ai_reasoning = merge_action.get("ai_reasoning")
        
        if keep_id and delete_ids:
            # è·å–è¦ä¿ç•™çš„è®°å¿†
            keep_result = await db.execute(select(Memory).where(Memory.id == keep_id))
            keep_memory = keep_result.scalar_one_or_none()
            
            if keep_memory:
                # æ›´æ–°å†…å®¹ï¼ˆå¦‚æœæœ‰æ–°å†…å®¹ï¼‰
                if new_content:
                    keep_memory.content = new_content
                
                # æ›´æ–° AI æ¨ç†é€»è¾‘
                if ai_reasoning:
                    keep_memory.ai_reasoning = ai_reasoning
                
                # å¢åŠ å¼ºè°ƒæ¬¡æ•°
                keep_memory.emphasis_count = (keep_memory.emphasis_count or 1) + len(delete_ids)
                keep_memory.updated_at = datetime.utcnow()
                
                # åˆ é™¤è¢«åˆå¹¶çš„è®°å¿†
                for del_id in delete_ids:
                    del_result = await db.execute(select(Memory).where(Memory.id == del_id))
                    del_memory = del_result.scalar_one_or_none()
                    if del_memory:
                        await db.delete(del_memory)
                
                actions.append({
                    "action": "merge",
                    "kept_id": keep_id,
                    "deleted_ids": delete_ids,
                    "reason": merge_action.get("reason", "åˆå¹¶é‡å¤è®°å¿†")
                })
                summary["merged"] += len(delete_ids)
    
    # 2. æ‰§è¡Œåˆ é™¤
    for delete_action in optimization_plan.get("delete", []):
        del_id = delete_action.get("id")
        if del_id:
            del_result = await db.execute(select(Memory).where(Memory.id == del_id))
            del_memory = del_result.scalar_one_or_none()
            if del_memory:
                await db.delete(del_memory)
                actions.append({
                    "action": "delete",
                    "id": del_id,
                    "content": del_memory.content[:50],
                    "reason": delete_action.get("reason", "åˆ é™¤æ— æ„ä¹‰è®°å¿†")
                })
                summary["deleted"] += 1
    
    # 3. æ‰§è¡Œé‡æ–°åˆ†ç±»
    for reclassify_action in optimization_plan.get("reclassify", []):
        mem_id = reclassify_action.get("id")
        new_type = reclassify_action.get("new_type")
        ai_reasoning = reclassify_action.get("ai_reasoning")
        if mem_id and new_type:
            try:
                new_memory_type = MemoryType(new_type.lower())
                recl_result = await db.execute(select(Memory).where(Memory.id == mem_id))
                recl_memory = recl_result.scalar_one_or_none()
                if recl_memory:
                    old_type = recl_memory.type.value
                    recl_memory.type = new_memory_type
                    recl_memory.color = TYPE_COLOR_MAP.get(new_memory_type.value, "border-slate-300")
                    recl_memory.updated_at = datetime.utcnow()
                    if ai_reasoning:
                        recl_memory.ai_reasoning = ai_reasoning
                    actions.append({
                        "action": "reclassify",
                        "id": mem_id,
                        "old_type": old_type,
                        "new_type": new_type,
                        "reason": reclassify_action.get("reason", "ä¿®æ­£åˆ†ç±»")
                    })
                    summary["reclassified"] += 1
            except ValueError:
                pass  # æ— æ•ˆçš„ç±»å‹ï¼Œè·³è¿‡
    
    # 4. åˆ›å»ºæ–°è®°å¿†
    for create_action in optimization_plan.get("create", []):
        try:
            new_type = MemoryType(create_action.get("type", "skill").lower())
            new_importance = MemoryImportance(create_action.get("importance", "Medium"))
            new_content = create_action.get("content")
            ai_reasoning = create_action.get("ai_reasoning")
            
            if new_content:
                new_memory = Memory(
                    user_id=user_id,
                    type=new_type,
                    content=new_content,
                    importance=new_importance,
                    scope=memory_scope,
                    color=TYPE_COLOR_MAP.get(new_type.value, "border-slate-300"),
                    source="ai",
                    emphasis_count=1,
                    ai_reasoning=ai_reasoning or f"åŸºäºç”¨æˆ·ç°æœ‰è®°å¿†åˆ†ææ¨æ–­å‡ºçš„{TYPE_NAME_MAP.get(new_type.value, new_type.value)}ä¿¡æ¯ã€‚"
                )
                db.add(new_memory)
                actions.append({
                    "action": "create",
                    "type": new_type.value,
                    "content": new_content[:50],
                    "reason": create_action.get("reason", "AI æ¨æ–­çš„æ–°è®°å¿†")
                })
                summary["created"] += 1
        except ValueError:
            pass  # æ— æ•ˆçš„ç±»å‹æˆ–é‡è¦æ€§ï¼Œè·³è¿‡
    
    # 5. æ›´æ–°ä¿ç•™è®°å¿†çš„ AI æ¨ç†é€»è¾‘
    for reasoning_update in optimization_plan.get("reasoning_updates", []):
        mem_id = reasoning_update.get("id")
        ai_reasoning = reasoning_update.get("ai_reasoning")
        if mem_id and ai_reasoning:
            reason_result = await db.execute(select(Memory).where(Memory.id == mem_id))
            reason_memory = reason_result.scalar_one_or_none()
            if reason_memory:
                reason_memory.ai_reasoning = ai_reasoning
                reason_memory.updated_at = datetime.utcnow()
                summary["reasoning_updated"] += 1
    
    await db.commit()
    
    return {
        "success": True,
        "message": f"è®°å¿†ä¼˜åŒ–å®Œæˆï¼šåˆå¹¶ {summary['merged']} æ¡ï¼Œåˆ é™¤ {summary['deleted']} æ¡ï¼Œé‡æ–°åˆ†ç±» {summary['reclassified']} æ¡ï¼Œæ–°å¢ {summary['created']} æ¡ï¼Œæ›´æ–°æ¨ç† {summary['reasoning_updated']} æ¡",
        "actions": actions,
        "summary": summary
    }


from app.models.todo import Todo, TodoStatus, TodoPriority, TodoSource, TodoType

class TodoCreate(BaseModel):
    title: str
    description: str = ""
    priority: str = "medium"
    source: str = "user"
    todo_type: str = "system"
    ai_advice: str = ""
    steps: list = []
    due_date: str = None


@router.get("/todos")
async def get_todos(
    user_id: int = Query(1, description="ç”¨æˆ·ID"),
    db: AsyncSession = Depends(get_db)
):
    """è·å–å¾…åŠä»»åŠ¡ - çº¯åŠ¨æ€æ•°æ®"""
    result = await db.execute(
        select(Todo)
        .where(Todo.user_id == user_id)
        .order_by(Todo.created_at.desc())
    )
    todos = result.scalars().all()
    
    # å¦‚æœæ²¡æœ‰ä»»åŠ¡ï¼Œè¿”å›ç©ºæ•°ç»„ï¼ˆä¸ä½¿ç”¨é™æ€æ•°æ®ï¼‰
    if not todos:
        return []
    
    return [{
        "id": str(t.id),
        "title": t.title,
        "task": t.title,  # å…¼å®¹å‰ç«¯å­—æ®µå
        "description": t.description or "",
        "status": t.status.value if t.status else "pending",
        "priority": t.priority.value.capitalize() if t.priority else "Medium",
        "progress": t.progress or 0,
        "source": t.source.value if t.source else "user",
        "icon": t.icon or "Calendar",
        "type": t.todo_type.value if t.todo_type else "system",
        "aiAdvice": t.ai_advice or "",
        "steps": json.loads(t.steps) if isinstance(t.steps, str) else (t.steps or []),
        "dueDate": t.due_date.strftime("%Y-%m-%d") if t.due_date else None,
        "createdAt": t.created_at.strftime("%Y-%m-%d") if t.created_at else None,
    } for t in todos]


@router.post("/todos")
async def create_todo(
    todo: TodoCreate,
    user_id: int = Query(1, description="ç”¨æˆ·ID"),
    db: AsyncSession = Depends(get_db)
):
    """åˆ›å»ºå¾…åŠä»»åŠ¡"""
    # è§£æä¼˜å…ˆçº§
    try:
        priority = TodoPriority(todo.priority.lower())
    except ValueError:
        priority = TodoPriority.MEDIUM
    
    # è§£ææ¥æº
    try:
        source = TodoSource(todo.source.lower())
    except ValueError:
        source = TodoSource.USER
    
    # è§£æç±»å‹
    try:
        todo_type = TodoType(todo.todo_type.lower())
    except ValueError:
        todo_type = TodoType.SYSTEM
    
    # è§£ææˆªæ­¢æ—¥æœŸ
    due_date = None
    if todo.due_date:
        try:
            due_date = datetime.strptime(todo.due_date, "%Y-%m-%d")
        except ValueError:
            pass
    
    new_todo = Todo(
        user_id=user_id,
        title=todo.title,
        description=todo.description,
        status=TodoStatus.PENDING,
        priority=priority,
        source=source,
        todo_type=todo_type,
        progress=0,
        ai_advice=todo.ai_advice,
        steps=todo.steps,
        due_date=due_date,
        icon="Calendar",
    )
    
    db.add(new_todo)
    await db.commit()
    await db.refresh(new_todo)
    
    return {
        "id": str(new_todo.id),
        "title": new_todo.title,
        "message": "ä»»åŠ¡åˆ›å»ºæˆåŠŸ"
    }


@router.put("/todos/{todo_id}")
async def update_todo(
    todo_id: int,
    status: str = Query(None),
    progress: int = Query(None),
    db: AsyncSession = Depends(get_db)
):
    """æ›´æ–°å¾…åŠä»»åŠ¡"""
    result = await db.execute(select(Todo).where(Todo.id == todo_id))
    todo = result.scalar_one_or_none()
    
    if not todo:
        return {"error": "ä»»åŠ¡ä¸å­˜åœ¨"}
    
    if status:
        try:
            todo.status = TodoStatus(status.lower())
            if status.lower() == "completed":
                todo.completed_at = datetime.utcnow()
                todo.progress = 100
        except ValueError:
            pass
    
    if progress is not None:
        todo.progress = progress
        if progress >= 100:
            todo.status = TodoStatus.COMPLETED
            todo.completed_at = datetime.utcnow()
    
    await db.commit()
    
    return {"message": "ä»»åŠ¡æ›´æ–°æˆåŠŸ"}


@router.delete("/todos/{todo_id}")
async def delete_todo(
    todo_id: int,
    db: AsyncSession = Depends(get_db)
):
    """åˆ é™¤å¾…åŠä»»åŠ¡"""
    result = await db.execute(select(Todo).where(Todo.id == todo_id))
    todo = result.scalar_one_or_none()
    
    if not todo:
        return {"error": "ä»»åŠ¡ä¸å­˜åœ¨"}
    
    await db.delete(todo)
    await db.commit()
    
    return {"message": "ä»»åŠ¡å·²åˆ é™¤"}


@router.delete("/todos/cleanup/duplicates")
async def cleanup_duplicate_profile_tasks(
    user_id: int = Query(..., description="ç”¨æˆ·ID"),
    db: AsyncSession = Depends(get_db)
):
    """æ¸…ç†é‡å¤çš„ã€Œå®Œå–„ç®€å†èµ„æ–™ã€ä»»åŠ¡ï¼Œåªä¿ç•™æœ€æ–°çš„ä¸€ä¸ª"""
    from sqlalchemy import or_
    
    # æŸ¥æ‰¾æ‰€æœ‰ã€Œå®Œå–„ç®€å†èµ„æ–™ã€ç±»å‹çš„ä»»åŠ¡ï¼ˆåŒ¹é…ç±»å‹æˆ–ç²¾ç¡®æ ‡é¢˜ï¼‰
    result = await db.execute(
        select(Todo)
        .where(Todo.user_id == user_id)
        .where(
            or_(
                Todo.todo_type == 'profile_complete',
                Todo.title == 'å®Œå–„ä¸ªäººç®€å†èµ„æ–™',  # ç²¾ç¡®åŒ¹é…
                Todo.title.ilike('%å®Œå–„%ç®€å†%'),
                Todo.title.ilike('%å®Œå–„%èµ„æ–™%')
            )
        )
        .order_by(Todo.created_at.desc())
    )
    profile_tasks = result.scalars().all()
    
    if len(profile_tasks) <= 1:
        return {
            "message": "æ— éœ€æ¸…ç†",
            "deleted_count": 0,
            "kept_task_id": profile_tasks[0].id if profile_tasks else None
        }
    
    # ä¿ç•™æœ€æ–°çš„ä¸€ä¸ªï¼Œåˆ é™¤å…¶ä»–çš„
    kept_task = profile_tasks[0]
    deleted_count = 0
    
    for task in profile_tasks[1:]:
        await db.delete(task)
        deleted_count += 1
    
    await db.commit()
    
    return {
        "message": f"å·²æ¸…ç† {deleted_count} ä¸ªé‡å¤ä»»åŠ¡",
        "deleted_count": deleted_count,
        "kept_task_id": kept_task.id
    }


@router.get("/tasks")
async def get_tasks(
    user_id: int = Query(1, description="ç”¨æˆ·ID"),
    db: AsyncSession = Depends(get_db)
):
    """è·å–ä»»åŠ¡åˆ—è¡¨ï¼ˆç”¨äº AI åŠ©æ‰‹ä¾§è¾¹æ ï¼‰- çº¯åŠ¨æ€æ•°æ®"""
    result = await db.execute(
        select(Todo)
        .where(Todo.user_id == user_id)
        .order_by(Todo.updated_at.desc())
        .limit(10)
    )
    todos = result.scalars().all()
    
    # å¦‚æœæ²¡æœ‰ä»»åŠ¡ï¼Œè¿”å›ç©ºæ•°ç»„ï¼ˆä¸ä½¿ç”¨é™æ€æ•°æ®ï¼‰
    if not todos:
        return []
    
    # çŠ¶æ€æ˜ å°„
    status_map = {
        TodoStatus.PENDING: "pending",
        TodoStatus.IN_PROGRESS: "running",
        TodoStatus.RUNNING: "running",
        TodoStatus.COMPLETED: "completed",
        TodoStatus.CANCELLED: "completed",
    }
    
    return [{
        "id": str(t.id),
        "title": t.title,
        "status": status_map.get(t.status, "pending"),
        "time": t.updated_at.strftime("%H:%M") if t.updated_at else "--:--",
        "icon": t.icon or "Calendar",
        "priority": t.priority.value.capitalize() if t.priority else "Medium",
        "source": t.source.value if t.source else "user",
        "type": t.todo_type.value if t.todo_type else "system",
    } for t in todos]


# ============ AI èŠå¤©æ¥å£ ============

def generate_smart_response(message: str, context: str, model: str) -> dict:
    """ç”Ÿæˆæ™ºèƒ½æœ¬åœ°å“åº”ï¼ˆå½“ AI API ä¸å¯ç”¨æ—¶ï¼‰"""
    import re
    
    message_lower = message.lower()
    response = ""
    
    # é¢è¯•ç›¸å…³
    if any(kw in message_lower for kw in ["é¢è¯•", "interview", "å‡†å¤‡", "æŠ€å·§"]):
        if "å‰ç«¯" in message_lower or "frontend" in message_lower:
            response = """å…³äºå‰ç«¯å¼€å‘é¢è¯•å‡†å¤‡ï¼Œæˆ‘å»ºè®®ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ç€æ‰‹ï¼š

**1. æ ¸å¿ƒæŠ€æœ¯æ ˆ**
â€¢ HTML5/CSS3 è¯­ä¹‰åŒ–ã€Flexbox/Grid å¸ƒå±€
â€¢ JavaScript åŸºç¡€ï¼šé—­åŒ…ã€åŸå‹é“¾ã€äº‹ä»¶å¾ªç¯ã€Promise/async-await
â€¢ æ¡†æ¶æ·±å…¥ï¼šReact Hooks åŸç†ã€Vue å“åº”å¼åŸç†ã€è™šæ‹Ÿ DOM

**2. å·¥ç¨‹åŒ–èƒ½åŠ›**
â€¢ Webpack/Vite æ„å»ºä¼˜åŒ–
â€¢ ä»£ç è§„èŒƒä¸ ESLint/Prettier
â€¢ Git å·¥ä½œæµä¸ CI/CD

**3. æ€§èƒ½ä¼˜åŒ–**
â€¢ é¦–å±åŠ è½½ä¼˜åŒ–ã€æ‡’åŠ è½½
â€¢ ç¼“å­˜ç­–ç•¥ã€CDN é…ç½®
â€¢ Web Vitals æŒ‡æ ‡ä¼˜åŒ–

**4. ç®—æ³•ä¸æ•°æ®ç»“æ„**
â€¢ å¸¸è§æ’åºã€æŸ¥æ‰¾ç®—æ³•
â€¢ æ ‘ã€é“¾è¡¨ã€æ ˆã€é˜Ÿåˆ—åŸºæœ¬æ“ä½œ
â€¢ LeetCode åˆ·é¢˜ï¼ˆå»ºè®® 100-200 é“ï¼‰

**5. é¡¹ç›®ç»éªŒå‡†å¤‡**
â€¢ ç”¨ STAR æ³•åˆ™å‡†å¤‡é¡¹ç›®ä»‹ç»
â€¢ å‡†å¤‡æŠ€æœ¯éš¾ç‚¹ä¸è§£å†³æ–¹æ¡ˆ
â€¢ äº†è§£ä¸šåŠ¡æŒ‡æ ‡ä¸æŠ€æœ¯ä»·å€¼

éœ€è¦æˆ‘é’ˆå¯¹æŸä¸ªæ–¹é¢è¯¦ç»†å±•å¼€å—ï¼Ÿ"""
        elif "åç«¯" in message_lower or "backend" in message_lower:
            response = """åç«¯å¼€å‘é¢è¯•å‡†å¤‡è¦ç‚¹ï¼š

**1. è¯­è¨€åŸºç¡€**
â€¢ å†…å­˜ç®¡ç†ã€å¹¶å‘æ¨¡å‹ã€GC æœºåˆ¶
â€¢ å¸¸ç”¨è®¾è®¡æ¨¡å¼ä¸åº”ç”¨åœºæ™¯

**2. æ•°æ®åº“**
â€¢ SQL ä¼˜åŒ–ã€ç´¢å¼•åŸç†
â€¢ äº‹åŠ¡éš”ç¦»çº§åˆ«ã€MVCC
â€¢ NoSQL ä½¿ç”¨åœºæ™¯

**3. ç³»ç»Ÿè®¾è®¡**
â€¢ åˆ†å¸ƒå¼ç³»ç»ŸåŸºç¡€
â€¢ ç¼“å­˜ç­–ç•¥ã€æ¶ˆæ¯é˜Ÿåˆ—
â€¢ é«˜å¯ç”¨ã€é«˜å¹¶å‘è®¾è®¡

**4. ç½‘ç»œä¸å®‰å…¨**
â€¢ HTTP/HTTPSã€TCP/IP
â€¢ è®¤è¯æˆæƒæœºåˆ¶
â€¢ å¸¸è§å®‰å…¨æ¼æ´é˜²æŠ¤

éœ€è¦é’ˆå¯¹å“ªä¸ªæ–¹å‘æ·±å…¥äº†è§£ï¼Ÿ"""
        else:
            response = """é¢è¯•å‡†å¤‡å»ºè®®ï¼š

**1. æŠ€æœ¯å‡†å¤‡**
â€¢ å¤¯å®åŸºç¡€çŸ¥è¯†ï¼Œç†è§£åº•å±‚åŸç†
â€¢ åˆ·ç®—æ³•é¢˜ï¼Œä¿æŒæ‰‹æ„Ÿ
â€¢ å‡†å¤‡é¡¹ç›®æ·±åº¦è®²è§£

**2. è½¯å®åŠ›**
â€¢ è‡ªæˆ‘ä»‹ç»ç²¾ç‚¼åˆ°ä½
â€¢ æ²Ÿé€šè¡¨è¾¾æ¸…æ™°æœ‰æ¡ç†
â€¢ å±•ç°å­¦ä¹ èƒ½åŠ›ä¸æ½œåŠ›

**3. å…¬å¸è°ƒç ”**
â€¢ äº†è§£å…¬å¸ä¸šåŠ¡ä¸æŠ€æœ¯æ ˆ
â€¢ å‡†å¤‡æœ‰ä»·å€¼çš„æé—®

éœ€è¦æˆ‘å¸®æ‚¨å‡†å¤‡å…·ä½“çš„å†…å®¹å—ï¼Ÿ"""
    
    # ç®€å†ç›¸å…³
    elif any(kw in message_lower for kw in ["ç®€å†", "resume", "cv", "ä¼˜åŒ–"]):
        response = """ç®€å†ä¼˜åŒ–å»ºè®®ï¼š

**1. åŸºæœ¬ä¿¡æ¯**
â€¢ è”ç³»æ–¹å¼å®Œæ•´ã€é‚®ç®±ä¸“ä¸š
â€¢ æ±‚èŒæ„å‘æ˜ç¡®

**2. å·¥ä½œç»å†**
â€¢ ä½¿ç”¨ STAR æ³•åˆ™æè¿°é¡¹ç›®
â€¢ é‡åŒ–æˆæœï¼ˆæå‡ XX%ã€èŠ‚çœ XX æˆæœ¬ï¼‰
â€¢ çªå‡ºæŠ€æœ¯äº®ç‚¹ä¸åˆ›æ–°ç‚¹

**3. æŠ€æœ¯æ ˆ**
â€¢ æŒ‰ç†Ÿç»ƒåº¦æ’åº
â€¢ ä¸ç›®æ ‡å²—ä½åŒ¹é…
â€¢ é¿å…å†™ä¸ç†Ÿæ‚‰çš„æŠ€æœ¯

**4. æ ¼å¼æ’ç‰ˆ**
â€¢ ä¸€é¡µçº¸åŸåˆ™
â€¢ é‡ç‚¹ä¿¡æ¯çªå‡º
â€¢ PDF æ ¼å¼æŠ•é€’

éœ€è¦æˆ‘å¸®æ‚¨é’ˆå¯¹ç‰¹å®šå²—ä½ä¼˜åŒ–ç®€å†å—ï¼Ÿ"""
    
    # èŒä½/å²—ä½ç›¸å…³
    elif any(kw in message_lower for kw in ["èŒä½", "å²—ä½", "æ¨è", "å·¥ä½œ", "job"]):
        response = """å…³äºèŒä½æ¨èï¼Œæˆ‘å¯ä»¥å¸®æ‚¨ï¼š

**1. å²—ä½åŒ¹é…åˆ†æ**
â€¢ åˆ†ææ‚¨çš„æŠ€èƒ½ä¸ç»éªŒ
â€¢ åŒ¹é…é€‚åˆçš„èŒä½ç±»å‹
â€¢ è¯„ä¼°è–ªèµ„æœŸæœ›åˆç†æ€§

**2. è¡Œä¸šè¶‹åŠ¿**
â€¢ å½“å‰çƒ­é—¨æŠ€æœ¯æ–¹å‘
â€¢ å„è¡Œä¸šæ‹›è˜éœ€æ±‚
â€¢ è¿œç¨‹/æ··åˆåŠå…¬æœºä¼š

**3. æ±‚èŒç­–ç•¥**
â€¢ ç›®æ ‡å…¬å¸ç­›é€‰
â€¢ æŠ•é€’æ—¶æœºæŠŠæ¡
â€¢ å¤šæ¸ é“æ±‚èŒå»ºè®®

è¯·å‘Šè¯‰æˆ‘æ‚¨çš„èƒŒæ™¯å’ŒæœŸæœ›ï¼Œæˆ‘æ¥ä¸ºæ‚¨æ¨èåˆé€‚çš„æ–¹å‘ã€‚"""
    
    # ç“¶é¢ˆ/é—®é¢˜åˆ†æ
    elif any(kw in message_lower for kw in ["ç“¶é¢ˆ", "é—®é¢˜", "åˆ†æ", "å›°éš¾", "æŒ‘æˆ˜"]):
        if context:
            response = f"""æ ¹æ®å½“å‰ä»»åŠ¡ã€Œ{context}ã€ï¼Œæˆ‘æ¥å¸®æ‚¨åˆ†æï¼š

**å¯èƒ½çš„ç“¶é¢ˆç‚¹ï¼š**

1. **çŸ¥è¯†å‚¨å¤‡**
   â€¢ æ˜¯å¦æœ‰çŸ¥è¯†ç›²åŒºéœ€è¦è¡¥å……ï¼Ÿ
   â€¢ æŠ€æœ¯æ·±åº¦æ˜¯å¦è¶³å¤Ÿï¼Ÿ

2. **æ—¶é—´ç®¡ç†**
   â€¢ å‡†å¤‡æ—¶é—´æ˜¯å¦å……è¶³ï¼Ÿ
   â€¢ æ˜¯å¦éœ€è¦è°ƒæ•´ä¼˜å…ˆçº§ï¼Ÿ

3. **å®è·µç»éªŒ**
   â€¢ æ˜¯å¦ç¼ºå°‘é¡¹ç›®å®æˆ˜ï¼Ÿ
   â€¢ éœ€è¦æ›´å¤šæ¨¡æ‹Ÿç»ƒä¹ ï¼Ÿ

**å»ºè®®çš„çªç ´æ–¹å‘ï¼š**
â€¢ åˆ¶å®šæ˜ç¡®çš„å­¦ä¹ è®¡åˆ’
â€¢ æ‰¾åˆ°è–„å¼±ç¯èŠ‚é‡ç‚¹çªç ´
â€¢ å¯»æ±‚åé¦ˆæŒç»­æ”¹è¿›

æ‚¨è§‰å¾—ä¸»è¦å¡åœ¨å“ªä¸ªæ–¹é¢ï¼Ÿæˆ‘å¯ä»¥æä¾›æ›´é’ˆå¯¹æ€§çš„å»ºè®®ã€‚"""
        else:
            response = """å¸®æ‚¨åˆ†æå½“å‰å¯èƒ½çš„ç“¶é¢ˆï¼š

**å¸¸è§ç“¶é¢ˆç±»å‹ï¼š**

1. **æŠ€æœ¯ç“¶é¢ˆ** - æŸäº›çŸ¥è¯†ç‚¹ä¸å¤Ÿæ·±å…¥
2. **é¡¹ç›®ç“¶é¢ˆ** - ç¼ºå°‘äº®ç‚¹é¡¹ç›®ç»éªŒ
3. **æ²Ÿé€šç“¶é¢ˆ** - è¡¨è¾¾ä¸å¤Ÿæ¸…æ™°æœ‰åŠ›
4. **å¿ƒæ€ç“¶é¢ˆ** - é¢è¯•ç´§å¼ å½±å“å‘æŒ¥

è¯·æè¿°ä¸€ä¸‹æ‚¨ç›®å‰é‡åˆ°çš„å…·ä½“æƒ…å†µï¼Œæˆ‘æ¥å¸®æ‚¨æ·±å…¥åˆ†æã€‚"""
    
    # è®¡åˆ’/å»ºè®®ç›¸å…³
    elif any(kw in message_lower for kw in ["è®¡åˆ’", "å»ºè®®", "è§„åˆ’", "æ€ä¹ˆåš", "å¦‚ä½•"]):
        response = """åˆ¶å®šæ‰§è¡Œè®¡åˆ’çš„å»ºè®®ï¼š

**1. æ˜ç¡®ç›®æ ‡**
â€¢ ç¡®å®šå…·ä½“å¯è¡¡é‡çš„ç›®æ ‡
â€¢ è®¾å®šåˆç†çš„æ—¶é—´èŠ‚ç‚¹

**2. æ‹†åˆ†ä»»åŠ¡**
â€¢ å°†å¤§ç›®æ ‡åˆ†è§£ä¸ºå°ä»»åŠ¡
â€¢ æ¯ä¸ªä»»åŠ¡æœ‰æ˜ç¡®çš„äº¤ä»˜ç‰©

**3. ä¼˜å…ˆæ’åº**
â€¢ è¯†åˆ«å…³é”®è·¯å¾„
â€¢ å…ˆè§£å†³é‡è¦ç´§æ€¥çš„äº‹é¡¹

**4. æ‰§è¡Œä¸å¤ç›˜**
â€¢ æ¯æ—¥/æ¯å‘¨æ£€æŸ¥è¿›åº¦
â€¢ åŠæ—¶è°ƒæ•´è®¡åˆ’

éœ€è¦æˆ‘å¸®æ‚¨åˆ¶å®šå…·ä½“çš„è¡ŒåŠ¨è®¡åˆ’å—ï¼Ÿ"""
    
    # é»˜è®¤å“åº”
    else:
        response = f"""æ”¶åˆ°æ‚¨çš„é—®é¢˜ï¼šã€Œ{message}ã€

æˆ‘æ˜¯ Devnors AI æ‹›è˜åŠ©æ‰‹ï¼Œå¯ä»¥å¸®æ‚¨ï¼š

â€¢ **æ±‚èŒå‡†å¤‡** - ç®€å†ä¼˜åŒ–ã€é¢è¯•æŠ€å·§ã€æŠ€æœ¯æå‡
â€¢ **èŒä½æ¨è** - æ ¹æ®æ‚¨çš„èƒŒæ™¯åŒ¹é…åˆé€‚å²—ä½
â€¢ **èŒä¸šè§„åˆ’** - åˆ†æå‘å±•è·¯å¾„ã€è¡Œä¸šè¶‹åŠ¿
â€¢ **ä»»åŠ¡æ‰§è¡Œ** - ååŠ©å®Œæˆæ‹›è˜ç›¸å…³ä»»åŠ¡

è¯·é—®æ‚¨éœ€è¦å“ªæ–¹é¢çš„å¸®åŠ©ï¼Ÿæˆ–è€…å¯ä»¥æ›´å…·ä½“åœ°æè¿°æ‚¨çš„éœ€æ±‚ã€‚"""
    
    return {
        "response": response,
        "tokens_used": 0,
        "model": model
    }


class ChatRequest(BaseModel):
    message: str
    history: list = []
    model: str = "Devnors 1.0"
    context: str = ""  # å¯é€‰çš„ä¸Šä¸‹æ–‡ï¼Œå¦‚ä»»åŠ¡ä¿¡æ¯


@router.post("/chat")
async def chat_with_assistant(
    request: ChatRequest,
    db: AsyncSession = Depends(get_db)
):
    """AI åŠ©æ‰‹èŠå¤©æ¥å£ - ä¼˜å…ˆä½¿ç”¨ MiniMax"""
    import httpx
    from app.config import settings
    
    # ç³»ç»Ÿæç¤º
    system_prompt = """ä½ æ˜¯ Devnors å¾—è‹¥ AI æ™ºèƒ½æ‹›è˜åŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·è§£å†³æ‹›è˜ç›¸å…³çš„é—®é¢˜ã€‚ä½ çš„èƒ½åŠ›åŒ…æ‹¬ï¼š

1. è§£ç­”æ‹›è˜ç›¸å…³é—®é¢˜
2. æä¾›æ±‚èŒ/æ‹›è˜å»ºè®®
3. åˆ†æèŒä½åŒ¹é…åº¦
4. ä¼˜åŒ–ç®€å†å’ŒèŒä½æè¿°
5. è§„åˆ’èŒä¸šå‘å±•æ–¹å‘
6. å¸®åŠ©æ‰§è¡Œæ‹›è˜ä»»åŠ¡

è¯·ç”¨ç®€æ´ã€ä¸“ä¸šã€å‹å¥½çš„æ–¹å¼å›å¤ç”¨æˆ·ã€‚å›å¤è¯·ä½¿ç”¨ä¸­æ–‡ã€‚"""
    
    if request.context:
        system_prompt += f"\n\nå½“å‰ä»»åŠ¡ä¸Šä¸‹æ–‡ï¼š{request.context}"
    
    # ä¼˜å…ˆä½¿ç”¨ MiniMax APIï¼ˆä½¿ç”¨ OpenAI å…¼å®¹æ ¼å¼ï¼Œä¸éœ€è¦ GROUP_IDï¼‰
    minimax_api_key = settings.minimax_api_key or ""
    
    if minimax_api_key:
        try:
            # æ„å»º OpenAI å…¼å®¹çš„æ¶ˆæ¯æ ¼å¼
            messages = [
                {"role": "system", "content": system_prompt}
            ]
            
            # æ·»åŠ å†å²æ¶ˆæ¯
            for msg in request.history[-10:]:
                role = msg.get("role", "user")
                content = msg.get("content", "")
                messages.append({"role": role, "content": content})
            
            # æ·»åŠ å½“å‰æ¶ˆæ¯
            messages.append({"role": "user", "content": request.message})
            
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    "https://api.minimax.chat/v1/text/chatcompletion_v2",
                    headers={
                        "Authorization": f"Bearer {minimax_api_key}",
                        "Content-Type": "application/json",
                    },
                    json={
                        "model": "abab6.5s-chat",
                        "messages": messages,
                        "max_tokens": 2048,
                        "temperature": 0.7,
                        "top_p": 0.9,
                    }
                )
                
                result = response.json()
                
                # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
                if result.get("base_resp", {}).get("status_code", 0) == 0:
                    if "choices" in result and len(result["choices"]) > 0:
                        reply = result["choices"][0].get("message", {}).get("content", "")
                        tokens = result.get("usage", {}).get("total_tokens", 0)
                        
                        return {
                            "response": reply,
                            "tokens_used": tokens,
                            "model": request.model
                        }
                else:
                    error_msg = result.get("base_resp", {}).get("status_msg", "")
                    print(f"MiniMax API error: {error_msg}")
        except Exception as e:
            print(f"MiniMax API error: {e}")
    
    # å¤‡é€‰ï¼šä½¿ç”¨ Gemini API
    gemini_api_key = settings.gemini_api_key or ""
    
    if gemini_api_key:
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                contents = []
                contents.append({"role": "user", "parts": [{"text": f"[ç³»ç»ŸæŒ‡ä»¤] {system_prompt}"}]})
                contents.append({"role": "model", "parts": [{"text": "å¥½çš„ï¼Œæˆ‘æ˜¯ Devnors AI æ™ºèƒ½æ‹›è˜åŠ©æ‰‹ï¼Œéšæ—¶ä¸ºæ‚¨æä¾›ä¸“ä¸šçš„æ‹›è˜æœåŠ¡ã€‚"}]})
                
                for msg in request.history[-10:]:
                    role = msg.get("role", "user")
                    content = msg.get("content", "")
                    if role == "user":
                        contents.append({"role": "user", "parts": [{"text": content}]})
                    else:
                        contents.append({"role": "model", "parts": [{"text": content}]})
                
                contents.append({"role": "user", "parts": [{"text": request.message}]})
                
                response = await client.post(
                    f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={gemini_api_key}",
                    headers={"Content-Type": "application/json"},
                    json={
                        "contents": contents,
                        "generationConfig": {"temperature": 0.7, "topP": 0.9, "maxOutputTokens": 2048}
                    }
                )
                
                result = response.json()
                
                if "candidates" in result and len(result["candidates"]) > 0:
                    reply = result["candidates"][0].get("content", {}).get("parts", [{}])[0].get("text", "")
                    tokens = result.get("usageMetadata", {}).get("totalTokenCount", 0)
                    return {"response": reply, "tokens_used": tokens, "model": request.model}
        except Exception as e:
            print(f"Gemini API error: {e}")
    
    # å¦‚æœæ‰€æœ‰ AI API éƒ½ä¸å¯ç”¨ï¼Œä½¿ç”¨æ™ºèƒ½æœ¬åœ°å“åº”
    return generate_smart_response(request.message, request.context, request.model)


# ============ ç”¨æˆ·èµ„æ–™æ¥å£ ============

from app.models.profile import UserProfile, ProfileType
from pydantic import BaseModel

class ProfileUpdate(BaseModel):
    """èµ„æ–™æ›´æ–°è¯·æ±‚"""
    display_name: Optional[str] = None
    title: Optional[str] = None
    summary: Optional[str] = None
    avatar_url: Optional[str] = None
    cover_url: Optional[str] = None
    candidate_data: Optional[dict] = None
    employer_data: Optional[dict] = None


@router.get("/profile")
async def get_user_profile(
    user_id: int = Query(..., description="ç”¨æˆ·ID"),
    profile_type: str = Query("candidate", description="èµ„æ–™ç±»å‹: candidate æˆ– employer"),
    db: AsyncSession = Depends(get_db)
):
    """è·å–ç”¨æˆ·èµ„æ–™"""
    try:
        ptype = ProfileType(profile_type.lower())
    except ValueError:
        ptype = ProfileType.CANDIDATE
    
    result = await db.execute(
        select(UserProfile)
        .where(UserProfile.user_id == user_id)
        .where(UserProfile.profile_type == ptype)
    )
    profile = result.scalar_one_or_none()
    
    if not profile:
        # è¿”å›ç©ºèµ„æ–™æ¨¡æ¿
        return {
            "id": None,
            "user_id": user_id,
            "profile_type": profile_type,
            "display_name": None,
            "title": None,
            "summary": None,
            "avatar_url": None,
            "cover_url": None,
            "candidate_data": {
                "skills": [],
                "experience_years": 0,
                "career_path": [],
                "certifications": [],
                "awards": [],
                "radar_data": [],
                "ideal_job": ""
            } if ptype == ProfileType.CANDIDATE else None,
            "employer_data": {
                "company_name": "",
                "industry": "",
                "size": "",
                "location": "",
                "founded": "",
                "website": "",
                "culture": "",
                "benefits": [],
                "tech_stack": [],
                "open_positions": [],
                "mission": "",
                "vision": ""
            } if ptype == ProfileType.EMPLOYER else None,
            "is_empty": True
        }
    
    return {
        "id": profile.id,
        "user_id": profile.user_id,
        "profile_type": profile.profile_type.value,
        "display_name": profile.display_name,
        "title": profile.title,
        "summary": profile.summary,
        "avatar_url": profile.avatar_url,
        "cover_url": profile.cover_url,
        "candidate_data": profile.candidate_data,
        "employer_data": profile.employer_data,
        "is_empty": False,
        "created_at": profile.created_at.isoformat() if profile.created_at else None,
        "updated_at": profile.updated_at.isoformat() if profile.updated_at else None
    }


@router.post("/profile")
async def update_user_profile(
    profile_data: ProfileUpdate,
    user_id: int = Query(..., description="ç”¨æˆ·ID"),
    profile_type: str = Query("candidate", description="èµ„æ–™ç±»å‹: candidate æˆ– employer"),
    db: AsyncSession = Depends(get_db)
):
    """åˆ›å»ºæˆ–æ›´æ–°ç”¨æˆ·èµ„æ–™"""
    try:
        ptype = ProfileType(profile_type.lower())
    except ValueError:
        ptype = ProfileType.CANDIDATE
    
    result = await db.execute(
        select(UserProfile)
        .where(UserProfile.user_id == user_id)
        .where(UserProfile.profile_type == ptype)
    )
    profile = result.scalar_one_or_none()
    
    if not profile:
        # åˆ›å»ºæ–°èµ„æ–™
        profile = UserProfile(
            user_id=user_id,
            profile_type=ptype
        )
        db.add(profile)
    
    # æ›´æ–°å­—æ®µ
    if profile_data.display_name is not None:
        profile.display_name = profile_data.display_name
    if profile_data.title is not None:
        profile.title = profile_data.title
    if profile_data.summary is not None:
        profile.summary = profile_data.summary
    if profile_data.avatar_url is not None:
        profile.avatar_url = profile_data.avatar_url
    if profile_data.cover_url is not None:
        profile.cover_url = profile_data.cover_url
    if profile_data.candidate_data is not None:
        # åˆå¹¶ç°æœ‰æ•°æ®
        existing = profile.candidate_data or {}
        if isinstance(existing, str):
            existing = json.loads(existing)
        existing.update(profile_data.candidate_data)
        profile.candidate_data = existing
    if profile_data.employer_data is not None:
        existing = profile.employer_data or {}
        if isinstance(existing, str):
            existing = json.loads(existing)
        existing.update(profile_data.employer_data)
        profile.employer_data = existing
    
    await db.commit()
    await db.refresh(profile)
    
    return {
        "success": True,
        "message": "èµ„æ–™æ›´æ–°æˆåŠŸ",
        "profile": {
            "id": profile.id,
            "user_id": profile.user_id,
            "profile_type": profile.profile_type.value,
            "display_name": profile.display_name,
            "title": profile.title,
            "summary": profile.summary,
            "candidate_data": profile.candidate_data,
            "employer_data": profile.employer_data
        }
    }


@router.patch("/profile/field")
async def update_profile_field(
    field: str = Query(..., description="å­—æ®µå"),
    value: str = Query(..., description="å­—æ®µå€¼"),
    user_id: int = Query(..., description="ç”¨æˆ·ID"),
    profile_type: str = Query("candidate", description="èµ„æ–™ç±»å‹"),
    force_update: bool = Query(False, description="æ˜¯å¦å¼ºåˆ¶è¦†ç›–å·²æœ‰å€¼"),
    db: AsyncSession = Depends(get_db)
):
    """æ›´æ–°ç”¨æˆ·èµ„æ–™çš„å•ä¸ªå­—æ®µï¼ˆç”¨äº AI åŠ©æ‰‹ç¼–è¾‘ï¼‰
    
    é»˜è®¤åªåœ¨å­—æ®µä¸ºç©ºæ—¶ä¿å­˜ï¼ˆé¦–æ¬¡è¾“å…¥ï¼‰ï¼Œå¦‚æœå·²æœ‰å€¼éœ€è¦ force_update=true æ‰ä¼šè¦†ç›–
    """
    try:
        ptype = ProfileType(profile_type.lower())
    except ValueError:
        ptype = ProfileType.CANDIDATE
    
    result = await db.execute(
        select(UserProfile)
        .where(UserProfile.user_id == user_id)
        .where(UserProfile.profile_type == ptype)
    )
    profile = result.scalar_one_or_none()
    
    if not profile:
        profile = UserProfile(user_id=user_id, profile_type=ptype)
        db.add(profile)
    
    # è¾…åŠ©å‡½æ•°ï¼šæ£€æŸ¥å­—æ®µæ˜¯å¦å·²æœ‰å€¼
    def has_existing_value(field_name: str) -> tuple:
        """è¿”å› (æ˜¯å¦æœ‰å€¼, ç°æœ‰å€¼)"""
        if field_name in ['display_name', 'title', 'summary', 'avatar_url', 'cover_url']:
            existing = getattr(profile, field_name, None)
            if existing and str(existing).strip():
                return True, existing
            return False, None
        
        if ptype == ProfileType.CANDIDATE:
            data = profile.candidate_data or {}
            if isinstance(data, str):
                data = json.loads(data)
            existing = data.get(field_name)
            if existing:
                if isinstance(existing, list) and len(existing) > 0:
                    return True, existing
                if isinstance(existing, str) and existing.strip():
                    return True, existing
            return False, None
        
        if ptype == ProfileType.EMPLOYER:
            data = profile.employer_data or {}
            if isinstance(data, str):
                data = json.loads(data)
            existing = data.get(field_name)
            if existing:
                if isinstance(existing, list) and len(existing) > 0:
                    return True, existing
                if isinstance(existing, str) and existing.strip():
                    return True, existing
            return False, None
        
        return False, None
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰å€¼
    has_value, existing_value = has_existing_value(field)
    if has_value and not force_update:
        # å·²æœ‰å€¼ä¸”æœªå¼ºåˆ¶è¦†ç›–ï¼Œè¿”å›æç¤º
        return {
            "success": False,
            "has_existing": True,
            "existing_value": existing_value if isinstance(existing_value, str) else str(existing_value),
            "message": f"å­—æ®µ {field} å·²æœ‰å€¼ï¼Œå¦‚éœ€ä¿®æ”¹è¯·ç¡®è®¤è¦†ç›–",
            "field": field,
            "new_value": value
        }
    
    # æ ¹æ®å­—æ®µåæ›´æ–°
    if field in ['display_name', 'title', 'summary', 'avatar_url', 'cover_url']:
        setattr(profile, field, value)
    elif ptype == ProfileType.CANDIDATE:
        # æ›´æ–° candidate_data ä¸­çš„å­—æ®µ
        # åˆ›å»ºæ–°å­—å…¸å‰¯æœ¬ä»¥ç¡®ä¿ SQLAlchemy æ£€æµ‹åˆ°å˜åŒ–
        old_data = profile.candidate_data or {}
        if isinstance(old_data, str):
            old_data = json.loads(old_data)
        data = dict(old_data)  # åˆ›å»ºå‰¯æœ¬
        
        if field == 'skills':
            data['skills'] = [s.strip() for s in value.split(',') if s.strip()]
        elif field == 'experience':
            # å·¥ä½œç»å†ä¿å­˜ä¸ºæ•°ç»„
            data['experience'] = [value.strip()] if value.strip() else []
        elif field == 'education':
            # æ•™è‚²èƒŒæ™¯ä¿å­˜ä¸ºæ•°ç»„
            data['education'] = [value.strip()] if value.strip() else []
        elif field == 'projects':
            # é¡¹ç›®ç»å†ä¿å­˜ä¸ºæ•°ç»„
            data['projects'] = [value.strip()] if value.strip() else []
        elif field == 'experience_years':
            data['experience_years'] = int(value) if value.isdigit() else 0
        elif field == 'ideal_job':
            data['ideal_job'] = value
        else:
            data[field] = value
        
        # æ˜¾å¼èµ‹å€¼æ–°å­—å…¸ä»¥è§¦å‘ SQLAlchemy å˜æ›´æ£€æµ‹
        profile.candidate_data = data
        # å¼ºåˆ¶æ ‡è®°å­—æ®µä¸ºå·²ä¿®æ”¹
        from sqlalchemy.orm.attributes import flag_modified
        flag_modified(profile, 'candidate_data')
    elif ptype == ProfileType.EMPLOYER:
        # æ›´æ–° employer_data ä¸­çš„å­—æ®µ
        old_data = profile.employer_data or {}
        if isinstance(old_data, str):
            old_data = json.loads(old_data)
        data = dict(old_data)  # åˆ›å»ºå‰¯æœ¬
        
        if field in ['benefits', 'tech_stack']:
            data[field] = [s.strip() for s in value.split(',') if s.strip()]
        else:
            data[field] = value
        
        profile.employer_data = data
        # å¼ºåˆ¶æ ‡è®°å­—æ®µä¸ºå·²ä¿®æ”¹
        from sqlalchemy.orm.attributes import flag_modified
        flag_modified(profile, 'employer_data')
    
    await db.commit()
    await db.refresh(profile)
    
    return {
        "success": True,
        "message": f"å­—æ®µ {field} æ›´æ–°æˆåŠŸ",
        "field": field,
        "value": value,
        "was_overwrite": has_value
    }


# ============ Token èµ„é‡‘è´¦æˆ·æ¥å£ ============

from app.models.token import TokenUsage, TokenPackage, TokenAction, PackageType

# Token æ¶ˆè€—ç±»å‹ä¸­æ–‡åæ˜ å°„
TOKEN_ACTION_NAMES = {
    TokenAction.RESUME_PARSE: "ç®€å†è§£æ",
    TokenAction.PROFILE_BUILD: "ç”»åƒè°ƒä¼˜",
    TokenAction.JOB_MATCH: "èŒä½åŒ¹é…",
    TokenAction.INTERVIEW: "å¤šæ™ºèƒ½ä½“é¢è¯•",
    TokenAction.MARKET_ANALYSIS: "å¸‚åœºåˆ†æ",
    TokenAction.ROUTE_DISPATCH: "å…¨å±€è·¯ç”±",
    TokenAction.CHAT: "AI å¯¹è¯",
    TokenAction.OTHER: "å…¶ä»–",
}


@router.get("/tokens/stats")
async def get_token_stats(
    user_id: int = Query(..., description="ç”¨æˆ·ID"),
    db: AsyncSession = Depends(get_db)
):
    """è·å–ç”¨æˆ· Token ç»Ÿè®¡ä¿¡æ¯"""
    from sqlalchemy import func
    from datetime import timedelta
    
    # è·å–ç”¨æˆ·å½“å‰å¥—é¤å’Œä½™é¢
    result = await db.execute(
        select(TokenPackage)
        .where(TokenPackage.user_id == user_id)
        .where(TokenPackage.is_active == True)
        .order_by(TokenPackage.purchased_at.desc())
    )
    packages = result.scalars().all()
    
    # è®¡ç®—æ€»ä½™é¢
    total_remaining = sum(p.remaining_tokens for p in packages) if packages else 100000  # é»˜è®¤é€10ä¸‡
    total_used = sum(p.used_tokens for p in packages) if packages else 0
    total_purchased = sum(p.price for p in packages) if packages else 0
    
    # è·å–ä»Šæ—¥æ¶ˆè€—
    today = datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0)
    result = await db.execute(
        select(func.sum(TokenUsage.tokens_used))
        .where(TokenUsage.user_id == user_id)
        .where(TokenUsage.created_at >= today)
    )
    today_usage = result.scalar() or 0
    
    # è·å–æ˜¨æ—¥æ¶ˆè€—ï¼ˆç”¨äºè®¡ç®—ç¯æ¯”ï¼‰
    yesterday = today - timedelta(days=1)
    result = await db.execute(
        select(func.sum(TokenUsage.tokens_used))
        .where(TokenUsage.user_id == user_id)
        .where(TokenUsage.created_at >= yesterday)
        .where(TokenUsage.created_at < today)
    )
    yesterday_usage = result.scalar() or 1  # é¿å…é™¤é›¶
    
    # è®¡ç®—ç¯æ¯”å¢é•¿
    change_rate = ((today_usage - yesterday_usage) / yesterday_usage * 100) if yesterday_usage > 0 else 0
    
    # è®¡ç®—é¢„ä¼°ç»­èˆªå¤©æ•°ï¼ˆåŸºäºè¿‘7å¤©å¹³å‡æ¶ˆè€—ï¼‰
    week_ago = today - timedelta(days=7)
    result = await db.execute(
        select(func.sum(TokenUsage.tokens_used))
        .where(TokenUsage.user_id == user_id)
        .where(TokenUsage.created_at >= week_ago)
    )
    week_usage = result.scalar() or 1
    daily_avg = week_usage / 7
    estimated_days = int(total_remaining / daily_avg) if daily_avg > 0 else 999
    
    return {
        "balance": total_remaining,
        "balance_display": f"{total_remaining/1000000:.2f}M" if total_remaining >= 100000 else f"{total_remaining/1000:.1f}K",
        "today_usage": today_usage,
        "today_usage_display": f"{today_usage:,}",
        "change_rate": round(change_rate, 1),
        "change_direction": "up" if change_rate > 0 else "down" if change_rate < 0 else "stable",
        "total_purchased": total_purchased,
        "total_purchased_display": f"Â¥ {total_purchased:,.2f}",
        "estimated_days": estimated_days,
        "packages": [{
            "id": p.id,
            "type": p.package_type.value,
            "total": p.total_tokens,
            "used": p.used_tokens,
            "remaining": p.remaining_tokens,
            "expires_at": p.expires_at.isoformat() if p.expires_at else None
        } for p in packages]
    }


@router.get("/tokens/history")
async def get_token_history(
    user_id: int = Query(..., description="ç”¨æˆ·ID"),
    limit: int = Query(20, description="è¿”å›æ¡æ•°"),
    offset: int = Query(0, description="åç§»é‡"),
    db: AsyncSession = Depends(get_db)
):
    """è·å– Token æ¶ˆè€—å†å²"""
    result = await db.execute(
        select(TokenUsage)
        .where(TokenUsage.user_id == user_id)
        .order_by(TokenUsage.created_at.desc())
        .limit(limit)
        .offset(offset)
    )
    records = result.scalars().all()
    
    # å¦‚æœæ²¡æœ‰è®°å½•ï¼Œè¿”å›ä¸€äº›ç¤ºä¾‹æ•°æ®
    if not records:
        # ç”Ÿæˆæœ€è¿‘7å¤©çš„æ¨¡æ‹Ÿæ•°æ®
        sample_data = []
        for i in range(7):
            date = datetime.utcnow() - timedelta(days=i)
            actions = [TokenAction.RESUME_PARSE, TokenAction.INTERVIEW, TokenAction.PROFILE_BUILD, TokenAction.ROUTE_DISPATCH]
            action = actions[i % len(actions)]
            tokens = [42500, 89000, 12400, 56000, 92000, 15000, 34000][i]
            sample_data.append({
                "id": i + 1,
                "date": date.strftime("%Y-%m-%d"),
                "action": action.value,
                "type": TOKEN_ACTION_NAMES.get(action, "å…¶ä»–"),
                "tokens": tokens,
                "cost": f"Â¥{tokens/10000:.2f}",
                "description": f"AI {TOKEN_ACTION_NAMES.get(action, 'ä»»åŠ¡')}"
            })
        return {
            "items": sample_data,
            "total": len(sample_data),
            "has_more": False
        }
    
    return {
        "items": [{
            "id": r.id,
            "date": r.created_at.strftime("%Y-%m-%d"),
            "action": r.action.value,
            "type": TOKEN_ACTION_NAMES.get(r.action, "å…¶ä»–"),
            "tokens": r.tokens_used,
            "cost": f"Â¥{r.tokens_used/10000:.2f}",
            "description": r.description or f"AI {TOKEN_ACTION_NAMES.get(r.action, 'ä»»åŠ¡')}"
        } for r in records],
        "total": len(records),
        "has_more": len(records) >= limit
    }


@router.get("/tokens/chart")
async def get_token_chart(
    user_id: int = Query(..., description="ç”¨æˆ·ID"),
    days: int = Query(7, description="å¤©æ•°"),
    db: AsyncSession = Depends(get_db)
):
    """è·å– Token æ¶ˆè€—è¶‹åŠ¿å›¾æ•°æ®"""
    from sqlalchemy import func, cast, Date
    
    start_date = datetime.utcnow() - timedelta(days=days)
    
    # æŒ‰æ—¥æœŸåˆ†ç»„ç»Ÿè®¡
    result = await db.execute(
        select(
            func.date(TokenUsage.created_at).label('date'),
            func.sum(TokenUsage.tokens_used).label('total')
        )
        .where(TokenUsage.user_id == user_id)
        .where(TokenUsage.created_at >= start_date)
        .group_by(func.date(TokenUsage.created_at))
        .order_by(func.date(TokenUsage.created_at))
    )
    rows = result.all()
    
    # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    if not rows:
        chart_data = []
        for i in range(days):
            date = datetime.utcnow() - timedelta(days=days-1-i)
            values = [42500, 89000, 12400, 56000, 92000, 15000, 34000]
            chart_data.append({
                "name": date.strftime("%m-%d"),
                "value": values[i % len(values)]
            })
        return {
            "data": chart_data,
            "peak": max(v["value"] for v in chart_data),
            "average": sum(v["value"] for v in chart_data) // len(chart_data)
        }
    
    # æ„å»ºå®Œæ•´çš„æ—¥æœŸåºåˆ—ï¼ˆå¡«å……æ— æ•°æ®çš„æ—¥æœŸï¼‰
    date_map = {str(row.date): row.total for row in rows}
    chart_data = []
    for i in range(days):
        date = datetime.utcnow() - timedelta(days=days-1-i)
        date_str = date.strftime("%Y-%m-%d")
        chart_data.append({
            "name": date.strftime("%m-%d"),
            "value": date_map.get(date_str, 0)
        })
    
    values = [d["value"] for d in chart_data]
    return {
        "data": chart_data,
        "peak": max(values) if values else 0,
        "average": sum(values) // len(values) if values else 0
    }


@router.get("/tokens/packages")
async def get_available_packages():
    """è·å–å¯è´­ä¹°çš„ Token å¥—é¤"""
    return {
        "packages": [
            {
                "id": "starter",
                "name": "å…¥é—¨ä½“éªŒ",
                "tokens": 100000,
                "tokens_display": "100,000",
                "price": 99,
                "discount": None,
                "accent": "bg-indigo-50"
            },
            {
                "id": "pro",
                "name": "ç²¾è‹±çŒè˜",
                "tokens": 1000000,
                "tokens_display": "1,000,000",
                "price": 799,
                "discount": "æ€§ä»·æ¯”æœ€é«˜",
                "accent": "bg-amber-50"
            },
            {
                "id": "enterprise",
                "name": "ä¼ä¸šæ——èˆ°",
                "tokens": 10000000,
                "tokens_display": "10,000,000",
                "price": 6999,
                "discount": "-20%",
                "accent": "bg-rose-50"
            }
        ]
    }


@router.post("/tokens/record")
async def record_token_usage(
    user_id: int = Query(..., description="ç”¨æˆ·ID"),
    action: str = Query(..., description="æ¶ˆè€—ç±»å‹"),
    tokens: int = Query(..., description="æ¶ˆè€—æ•°é‡"),
    description: Optional[str] = Query(None, description="æè¿°"),
    db: AsyncSession = Depends(get_db)
):
    """è®°å½• Token æ¶ˆè€—"""
    try:
        token_action = TokenAction(action)
    except ValueError:
        token_action = TokenAction.OTHER
    
    # åˆ›å»ºæ¶ˆè€—è®°å½•
    usage = TokenUsage(
        user_id=user_id,
        action=token_action,
        tokens_used=tokens,
        description=description
    )
    db.add(usage)
    
    # æ›´æ–°ç”¨æˆ·å¥—é¤ä½™é¢
    result = await db.execute(
        select(TokenPackage)
        .where(TokenPackage.user_id == user_id)
        .where(TokenPackage.is_active == True)
        .where(TokenPackage.remaining_tokens > 0)
        .order_by(TokenPackage.expires_at.asc().nullslast())
    )
    package = result.scalar_one_or_none()
    
    if package:
        package.used_tokens += tokens
        package.remaining_tokens = max(0, package.remaining_tokens - tokens)
    
    await db.commit()
    
    return {
        "success": True,
        "message": "Token æ¶ˆè€—å·²è®°å½•",
        "tokens_used": tokens,
        "action": token_action.value
    }


# ============ æ¶ˆæ¯é€šçŸ¥ç›¸å…³æ¥å£ ============

@router.get("/notifications")
async def get_notifications(
    user_id: int = Query(..., description="ç”¨æˆ·ID"),
    notification_type: Optional[str] = Query(None, description="é€šçŸ¥ç±»å‹: system/match/interview/message"),
    unread_only: bool = Query(False, description="ä»…æœªè¯»"),
    page: int = Query(1, ge=1),
    page_size: int = Query(20, ge=1, le=100),
    db: AsyncSession = Depends(get_db)
):
    """è·å–ç”¨æˆ·é€šçŸ¥åˆ—è¡¨"""
    import random
    
    # åŸºç¡€é€šçŸ¥æ¨¡æ¿
    notification_templates = [
        {
            "type": "match",
            "title": "æ–°çš„èŒä½åŒ¹é…",
            "content_template": "æ‚¨çš„ç®€å†ä¸ã€Œ{job_title} - {company}ã€åŒ¹é…åº¦è¾¾åˆ° {match_rate}%",
            "icon": "Target",
            "color": "text-indigo-600",
            "bgColor": "bg-indigo-50",
            "link": "/jobs"
        },
        {
            "type": "interview",
            "title": "é¢è¯•é‚€è¯·",
            "content_template": "{company}é‚€è¯·æ‚¨å‚åŠ ã€Œ{job_title}ã€å²—ä½çš„{interview_type}",
            "icon": "Calendar",
            "color": "text-emerald-600",
            "bgColor": "bg-emerald-50",
            "link": "/workbench"
        },
        {
            "type": "system",
            "title": "ç³»ç»Ÿé€šçŸ¥",
            "content_template": "{message}",
            "icon": "Bell",
            "color": "text-amber-600",
            "bgColor": "bg-amber-50",
            "link": "/settings"
        },
        {
            "type": "message",
            "title": "æ–°æ¶ˆæ¯",
            "content_template": "{sender}å›å¤äº†æ‚¨ï¼šã€Œ{preview}ã€",
            "icon": "MessageSquare",
            "color": "text-violet-600",
            "bgColor": "bg-violet-50",
            "link": "/ai-assistant"
        },
        {
            "type": "match",
            "title": "ç®€å†è¢«æŸ¥çœ‹",
            "content_template": "{company}çš„æ‹›è˜å®˜æŸ¥çœ‹äº†æ‚¨çš„ç®€å†",
            "icon": "Eye",
            "color": "text-cyan-600",
            "bgColor": "bg-cyan-50",
            "link": "/candidate/profile"
        },
        {
            "type": "system",
            "title": "Token ä½™é¢æé†’",
            "content_template": "æ‚¨çš„ Token ä½™é¢ä¸è¶³ {threshold}ï¼Œå»ºè®®åŠæ—¶å……å€¼",
            "icon": "AlertCircle",
            "color": "text-rose-600",
            "bgColor": "bg-rose-50",
            "link": "/tokens"
        },
        {
            "type": "interview",
            "title": "é¢è¯•ç»“æœé€šçŸ¥",
            "content_template": "æ­å–œï¼æ‚¨é€šè¿‡äº†ã€Œ{company} - {job_title}ã€çš„{stage}",
            "icon": "CheckCircle2",
            "color": "text-emerald-600",
            "bgColor": "bg-emerald-50",
            "link": "/workbench"
        },
        {
            "type": "match",
            "title": "äººæ‰æ¨è",
            "content_template": "ç³»ç»Ÿä¸ºæ‚¨æ¨èäº† {count} ä½é«˜åŒ¹é…åº¦å€™é€‰äºº",
            "icon": "Users",
            "color": "text-indigo-600",
            "bgColor": "bg-indigo-50",
            "link": "/employer/talent-pool"
        },
        {
            "type": "system",
            "title": "è´¦æˆ·å‡çº§æˆåŠŸ",
            "content_template": "æ‚¨çš„è´¦æˆ·å·²å‡çº§ä¸º {plan} ç‰ˆæœ¬ï¼Œè§£é”æ›´å¤šé«˜çº§åŠŸèƒ½",
            "icon": "Zap",
            "color": "text-amber-600",
            "bgColor": "bg-amber-50",
            "link": "/settings"
        },
        {
            "type": "match",
            "title": "èŒä½æ›´æ–°æé†’",
            "content_template": "æ‚¨å…³æ³¨çš„ã€Œ{company}ã€å‘å¸ƒäº†æ–°èŒä½ï¼š{job_title}",
            "icon": "Briefcase",
            "color": "text-indigo-600",
            "bgColor": "bg-indigo-50",
            "link": "/jobs"
        }
    ]
    
    # ç¤ºä¾‹æ•°æ®
    companies = ["å­—èŠ‚è·³åŠ¨", "è…¾è®¯ç§‘æŠ€", "é˜¿é‡Œå·´å·´", "ç¾å›¢", "äº¬ä¸œ", "ç™¾åº¦", "åä¸º", "å°ç±³", "æ»´æ»´", "èš‚èšé›†å›¢"]
    job_titles = ["é«˜çº§å‰ç«¯å·¥ç¨‹å¸ˆ", "èµ„æ·±åç«¯å·¥ç¨‹å¸ˆ", "ç®—æ³•å·¥ç¨‹å¸ˆ", "äº§å“ç»ç†", "æŠ€æœ¯è´Ÿè´£äºº", "å…¨æ ˆå¼€å‘", "æ•°æ®åˆ†æå¸ˆ", "AI å·¥ç¨‹å¸ˆ"]
    interview_types = ["è§†é¢‘é¢è¯•", "ç”µè¯é¢è¯•", "ç°åœºé¢è¯•", "æŠ€æœ¯é¢è¯•", "HR é¢è¯•"]
    stages = ["ä¸€é¢", "äºŒé¢", "ç»ˆé¢", "HR é¢"]
    senders = ["HR ææ˜", "æ‹›è˜ç»ç†å¼ æ€»", "æŠ€æœ¯é¢è¯•å®˜ç‹å·¥", "çŒå¤´é¡¾é—®é™ˆç»ç†"]
    plans = ["Pro", "Ultra", "Enterprise"]
    messages = [
        "æ‚¨çš„ç®€å†å·²é€šè¿‡åˆç­›",
        "æ–°ç‰ˆæœ¬å·²å‘å¸ƒï¼Œè¯·åŠæ—¶æ›´æ–°",
        "æ‚¨çš„è´¦æˆ·å®‰å…¨è®¾ç½®å·²æ›´æ–°",
        "æ„Ÿè°¢æ‚¨çš„åé¦ˆï¼Œæˆ‘ä»¬ä¼šæŒç»­æ”¹è¿›"
    ]
    previews = [
        "å…³äºè–ªèµ„èŒƒå›´å¯ä»¥é¢è°ˆ...",
        "æœŸå¾…ä¸æ‚¨è¿›ä¸€æ­¥æ²Ÿé€š...",
        "æ‚¨çš„æŠ€æœ¯èƒŒæ™¯éå¸¸åŒ¹é…æˆ‘ä»¬çš„éœ€æ±‚...",
        "è¯·é—®æ‚¨æ–¹ä¾¿å‚åŠ ä¸‹å‘¨çš„é¢è¯•å—..."
    ]
    
    # æ—¶é—´ç”Ÿæˆå‡½æ•°
    def generate_time(minutes_ago: int) -> str:
        if minutes_ago < 60:
            return f"{minutes_ago}åˆ†é’Ÿå‰"
        elif minutes_ago < 1440:
            return f"{minutes_ago // 60}å°æ—¶å‰"
        elif minutes_ago < 2880:
            return "æ˜¨å¤©"
        else:
            return f"{minutes_ago // 1440}å¤©å‰"
    
    # ç”Ÿæˆé€šçŸ¥
    notifications = []
    for i in range(15):
        template = random.choice(notification_templates)
        
        # å¡«å……æ¨¡æ¿
        content = template["content_template"]
        content = content.replace("{company}", random.choice(companies))
        content = content.replace("{job_title}", random.choice(job_titles))
        content = content.replace("{match_rate}", str(random.randint(80, 98)))
        content = content.replace("{interview_type}", random.choice(interview_types))
        content = content.replace("{stage}", random.choice(stages))
        content = content.replace("{sender}", random.choice(senders))
        content = content.replace("{preview}", random.choice(previews))
        content = content.replace("{threshold}", "10,000")
        content = content.replace("{count}", str(random.randint(3, 8)))
        content = content.replace("{plan}", random.choice(plans))
        content = content.replace("{message}", random.choice(messages))
        
        # æ—¶é—´é€’å¢
        minutes_ago = i * random.randint(30, 180)
        
        notifications.append({
            "id": i + 1,
            "type": template["type"],
            "title": template["title"],
            "content": content,
            "time": generate_time(minutes_ago),
            "timestamp": (datetime.utcnow() - timedelta(minutes=minutes_ago)).isoformat(),
            "read": i >= 3,  # å‰ 3 æ¡æœªè¯»
            "icon": template["icon"],
            "color": template["color"],
            "bgColor": template["bgColor"],
            "link": template["link"]
        })
    
    # ç­›é€‰
    if notification_type:
        notifications = [n for n in notifications if n["type"] == notification_type]
    
    if unread_only:
        notifications = [n for n in notifications if not n["read"]]
    
    # åˆ†é¡µ
    total = len(notifications)
    start = (page - 1) * page_size
    end = start + page_size
    paginated = notifications[start:end]
    
    # ç»Ÿè®¡
    unread_count = len([n for n in notifications if not n["read"]])
    
    return {
        "notifications": paginated,
        "total": total,
        "unread_count": unread_count,
        "page": page,
        "page_size": page_size
    }


@router.post("/notifications/read")
async def mark_notification_read(
    user_id: int = Query(..., description="ç”¨æˆ·ID"),
    notification_id: Optional[int] = Query(None, description="é€šçŸ¥IDï¼Œä¸ä¼ åˆ™æ ‡è®°å…¨éƒ¨å·²è¯»"),
    db: AsyncSession = Depends(get_db)
):
    """æ ‡è®°é€šçŸ¥å·²è¯»"""
    # è¿™é‡Œåº”è¯¥æ›´æ–°æ•°æ®åº“ä¸­çš„é€šçŸ¥çŠ¶æ€
    # ç”±äºæ²¡æœ‰é€šçŸ¥è¡¨ï¼Œè¿”å›æ¨¡æ‹Ÿå“åº”
    if notification_id:
        return {
            "success": True,
            "message": f"é€šçŸ¥ {notification_id} å·²æ ‡è®°ä¸ºå·²è¯»"
        }
    else:
        return {
            "success": True,
            "message": "æ‰€æœ‰é€šçŸ¥å·²æ ‡è®°ä¸ºå·²è¯»"
        }


@router.delete("/notifications/{notification_id}")
async def delete_notification(
    notification_id: int,
    user_id: int = Query(..., description="ç”¨æˆ·ID"),
    db: AsyncSession = Depends(get_db)
):
    """åˆ é™¤é€šçŸ¥"""
    return {
        "success": True,
        "message": f"é€šçŸ¥ {notification_id} å·²åˆ é™¤"
    }


@router.get("/notifications/unread-count")
async def get_unread_count(
    user_id: int = Query(..., description="ç”¨æˆ·ID"),
    db: AsyncSession = Depends(get_db)
):
    """è·å–æœªè¯»é€šçŸ¥æ•°é‡"""
    import random
    # è¿”å›éšæœºæœªè¯»æ•°é‡ï¼ˆå®é™…åº”ä»æ•°æ®åº“æŸ¥è¯¢ï¼‰
    return {
        "unread_count": random.randint(1, 5)
    }


# ============ æ–‡ä»¶è§£ææ¥å£ ============

from fastapi import UploadFile, File, HTTPException
import tempfile
import os

@router.post("/parse-resume")
async def parse_resume_file(
    file: UploadFile = File(..., description="ç®€å†æ–‡ä»¶ (PDF/Word/TXT)")
):
    """
    è§£æä¸Šä¼ çš„ç®€å†æ–‡ä»¶ï¼Œè¿”å›æ–‡æœ¬å†…å®¹
    æ”¯æŒæ ¼å¼: PDF, Word (.doc/.docx), æ–‡æœ¬æ–‡ä»¶ (.txt/.md)
    """
    if not file.filename:
        raise HTTPException(status_code=400, detail="æœªé€‰æ‹©æ–‡ä»¶")
    
    # è·å–æ–‡ä»¶æ‰©å±•å
    file_ext = file.filename.split('.')[-1].lower() if '.' in file.filename else ''
    
    # æ£€æŸ¥æ–‡ä»¶ç±»å‹
    allowed_extensions = ['pdf', 'doc', 'docx', 'txt', 'md']
    if file_ext not in allowed_extensions:
        raise HTTPException(
            status_code=400, 
            detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}ã€‚æ”¯æŒçš„æ ¼å¼: PDF, Word, TXT"
        )
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å° (æœ€å¤§ 10MB)
    content = await file.read()
    if len(content) > 10 * 1024 * 1024:
        raise HTTPException(status_code=400, detail="æ–‡ä»¶è¿‡å¤§ï¼Œæœ€å¤§æ”¯æŒ 10MB")
    
    try:
        extracted_text = ""
        
        if file_ext == 'pdf':
            # è§£æ PDF
            import pdfplumber
            import io
            
            with pdfplumber.open(io.BytesIO(content)) as pdf:
                pages_text = []
                for page in pdf.pages:
                    text = page.extract_text()
                    if text:
                        pages_text.append(text)
                extracted_text = '\n\n'.join(pages_text)
        
        elif file_ext in ['doc', 'docx']:
            # è§£æ Word
            from docx import Document
            import io
            
            doc = Document(io.BytesIO(content))
            paragraphs = [p.text for p in doc.paragraphs if p.text.strip()]
            extracted_text = '\n'.join(paragraphs)
        
        elif file_ext in ['txt', 'md']:
            # æ–‡æœ¬æ–‡ä»¶ç›´æ¥è§£ç 
            try:
                extracted_text = content.decode('utf-8')
            except UnicodeDecodeError:
                extracted_text = content.decode('gbk', errors='ignore')
        
        if not extracted_text.strip():
            raise HTTPException(status_code=400, detail="æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æ— æ³•è§£æ")
        
        return {
            "success": True,
            "filename": file.filename,
            "file_type": file_ext,
            "content": extracted_text.strip(),
            "char_count": len(extracted_text.strip())
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"æ–‡ä»¶è§£æé”™è¯¯: {e}")
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶è§£æå¤±è´¥: {str(e)}")


from pydantic import BaseModel

class AutoFillRequest(BaseModel):
    user_id: int
    resume_content: str

@router.post("/auto-fill-profile")
async def auto_fill_profile_from_resume(
    request: AutoFillRequest,
    db: AsyncSession = Depends(get_db)
):
    """
    æ™ºèƒ½è§£æç®€å†å†…å®¹ï¼Œè‡ªåŠ¨å¡«å……ç”¨æˆ·èµ„æ–™å’Œè®°å¿†
    """
    from app.agents.base_agent import BaseAgent
    from app.models.memory import Memory, MemoryType, MemoryImportance, MemoryScope
    from app.models.profile import UserProfile, ProfileType
    from sqlalchemy.orm.attributes import flag_modified
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„ AI Agent ç”¨äºè§£æç®€å†
    class ResumeExtractor(BaseAgent):
        def _get_fallback_response(self, prompt: str) -> str:
            return "{}"
        def _get_fallback_json(self, prompt: str) -> dict:
            return {}
    
    extractor = ResumeExtractor(
        system_instruction="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç®€å†è§£æåŠ©æ‰‹ï¼Œè¯·å‡†ç¡®æå–ç®€å†ä¸­çš„ä¿¡æ¯å¹¶è¿”å› JSON æ ¼å¼ã€‚ç¡®ä¿ JSON å®Œæ•´ä¸”æ ¼å¼æ­£ç¡®ã€‚"
    )
    
    user_id = request.user_id
    resume_content = request.resume_content
    
    # ä½¿ç”¨ AI è§£æç®€å†å†…å®¹
    parse_prompt = f"""è¯·ä»ä»¥ä¸‹ç®€å†å†…å®¹ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼Œè¿”å› JSON æ ¼å¼ï¼š

ç®€å†å†…å®¹ï¼š
{resume_content[:6000]}

è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼ˆå¦‚æœç®€å†ä¸­æ²¡æœ‰ï¼Œå¡« nullï¼‰ï¼š
{{
    "display_name": "å§“å",
    "title": "å½“å‰èŒä½/èŒç§°",
    "summary": "ä¸ªäººç®€ä»‹/è‡ªæˆ‘è¯„ä»·ï¼ˆ50-200å­—ï¼Œå¦‚æœæ²¡æœ‰è¯·æ ¹æ®ç®€å†å†…å®¹æ€»ç»“ï¼‰",
    "skills": ["æŠ€èƒ½1", "æŠ€èƒ½2"],
    "experience": [
        {{
            "company": "å…¬å¸åç§°",
            "position": "èŒä½",
            "period": "æ—¶é—´æ®µ",
            "description": "å·¥ä½œæè¿°"
        }}
    ],
    "education": [
        {{
            "school": "å­¦æ ¡åç§°",
            "degree": "å­¦ä½",
            "major": "ä¸“ä¸š",
            "period": "æ—¶é—´æ®µ"
        }}
    ],
    "projects": [
        {{
            "name": "é¡¹ç›®åç§°",
            "role": "è§’è‰²",
            "description": "é¡¹ç›®æè¿°"
        }}
    ],
    "expected_salary": "æœŸæœ›è–ªèµ„ï¼ˆå¦‚æœæ²¡æœ‰å¡« nullï¼‰",
    "expected_location": "æœŸæœ›å·¥ä½œåœ°ç‚¹ï¼ˆå¦‚æœæ²¡æœ‰å¡« nullï¼‰",
    "extra_info": ["ç®€å†ä¸­çš„å…¶ä»–é‡è¦ä¿¡æ¯ï¼Œå¦‚è¯ä¹¦ã€è·å¥–ç­‰"]
}}

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

    try:
        parsed_data = await extractor.generate_json(parse_prompt)
        
        if not parsed_data:
            raise HTTPException(status_code=500, detail="AI è§£æå¤±è´¥ï¼Œæ— æ³•æå–ç»“æ„åŒ–ä¿¡æ¯")
        
        # è·å–æˆ–åˆ›å»ºç”¨æˆ·èµ„æ–™
        profile_query = select(UserProfile).where(
            UserProfile.user_id == user_id,
            UserProfile.profile_type == ProfileType.CANDIDATE
        )
        result = await db.execute(profile_query)
        profile = result.scalar_one_or_none()
        
        if not profile:
            profile = UserProfile(
                user_id=user_id,
                profile_type=ProfileType.CANDIDATE,
                candidate_data={}
            )
            db.add(profile)
        
        # æ›´æ–°èµ„æ–™å­—æ®µ
        updates_made = []
        
        if parsed_data.get('display_name'):
            profile.display_name = parsed_data['display_name']
            updates_made.append('å§“å')
        
        if parsed_data.get('title'):
            profile.title = parsed_data['title']
            updates_made.append('èŒä½')
        
        if parsed_data.get('summary'):
            profile.summary = parsed_data['summary']
            updates_made.append('ä¸ªäººç®€ä»‹')
        
        # æ›´æ–° candidate_data
        candidate_data = dict(profile.candidate_data or {})
        
        if parsed_data.get('skills'):
            candidate_data['skills'] = parsed_data['skills']
            updates_made.append('æŠ€èƒ½')
        
        if parsed_data.get('experience'):
            candidate_data['experience'] = parsed_data['experience']
            updates_made.append('å·¥ä½œç»å†')
        
        if parsed_data.get('education'):
            candidate_data['education'] = parsed_data['education']
            updates_made.append('æ•™è‚²èƒŒæ™¯')
        
        if parsed_data.get('projects'):
            candidate_data['projects'] = parsed_data['projects']
            updates_made.append('é¡¹ç›®ç»å†')
        
        if parsed_data.get('expected_salary'):
            candidate_data['expected_salary'] = parsed_data['expected_salary']
            updates_made.append('æœŸæœ›è–ªèµ„')
        
        if parsed_data.get('expected_location'):
            candidate_data['expected_location'] = parsed_data['expected_location']
            updates_made.append('æœŸæœ›åœ°ç‚¹')
        
        profile.candidate_data = candidate_data
        flag_modified(profile, 'candidate_data')
        
        # ä¿å­˜é¢å¤–ä¿¡æ¯åˆ°è®°å¿†
        memories_created = []
        extra_info = parsed_data.get('extra_info', [])
        
        if extra_info:
            for info in extra_info[:5]:  # æœ€å¤šä¿å­˜5æ¡é¢å¤–ä¿¡æ¯
                if info and len(info) > 10:
                    memory = Memory(
                        user_id=user_id,
                        scope=MemoryScope.CANDIDATE,
                        type=MemoryType.PREFERENCE,
                        content=info,
                        importance=MemoryImportance.MEDIUM,
                        ai_reasoning="ä»ç®€å†ä¸­è‡ªåŠ¨æå–çš„é¢å¤–ä¿¡æ¯"
                    )
                    db.add(memory)
                    memories_created.append(info[:50] + '...' if len(info) > 50 else info)
        
        await db.commit()
        
        # è®¡ç®—å®Œå–„åº¦
        total_fields = 9
        filled_fields = 0
        if profile.display_name: filled_fields += 1
        if profile.title: filled_fields += 1
        if profile.summary and len(profile.summary) >= 20: filled_fields += 1
        if candidate_data.get('skills'): filled_fields += 1
        if candidate_data.get('experience'): filled_fields += 1
        if candidate_data.get('education'): filled_fields += 1
        if candidate_data.get('projects'): filled_fields += 1
        if candidate_data.get('expected_salary'): filled_fields += 1
        if candidate_data.get('expected_location'): filled_fields += 1
        
        completeness = round((filled_fields / total_fields) * 100)
        
        return {
            "success": True,
            "parsed_data": parsed_data,
            "updates_made": updates_made,
            "memories_created": memories_created,
            "completeness": completeness,
            "message": f"å·²è‡ªåŠ¨å¡«å…… {len(updates_made)} ä¸ªå­—æ®µï¼Œåˆ›å»º {len(memories_created)} æ¡è®°å¿†ï¼Œç®€å†å®Œå–„åº¦ï¼š{completeness}%"
        }
        
    except json.JSONDecodeError as e:
        print(f"JSON è§£æé”™è¯¯: {e}")
        raise HTTPException(status_code=500, detail="AI è¿”å›æ ¼å¼é”™è¯¯ï¼Œè¯·é‡è¯•")
    except Exception as e:
        print(f"è‡ªåŠ¨å¡«å……å¤±è´¥: {e}")
        await db.rollback()
        raise HTTPException(status_code=500, detail=f"è‡ªåŠ¨å¡«å……å¤±è´¥: {str(e)}")


# ============ å²—ä½å‘å¸ƒï¼ˆå…¬å¼€æ¥å£ï¼‰ ============

from pydantic import BaseModel as PydanticBaseModel, Field as PydanticField
from typing import List as TypingList

class PublicJobCreate(PydanticBaseModel):
    """å…¬å¼€åˆ›å»ºå²—ä½è¯·æ±‚"""
    user_id: int
    title: str
    company: str
    location: str
    description: str
    salary_min: Optional[int] = None
    salary_max: Optional[int] = None
    tags: TypingList[str] = []

@router.post("/jobs")
async def create_public_job(
    job_in: PublicJobCreate,
    db: AsyncSession = Depends(get_db)
):
    """
    å…¬å¼€æ¥å£åˆ›å»ºå²—ä½ï¼ˆé€šè¿‡ user_id é‰´æƒï¼‰
    """
    from app.models.user import User
    
    # éªŒè¯ç”¨æˆ·
    result = await db.execute(select(User).where(User.id == job_in.user_id))
    user = result.scalar_one_or_none()
    if not user:
        raise HTTPException(status_code=404, detail="ç”¨æˆ·ä¸å­˜åœ¨")
    
    # åˆ›å»ºå²—ä½
    job = Job(
        title=job_in.title,
        company=job_in.company,
        location=job_in.location,
        description=job_in.description,
        salary_min=job_in.salary_min,
        salary_max=job_in.salary_max,
        owner_id=user.id,
        status=JobStatus.ACTIVE
    )
    
    # å¤„ç†æ ‡ç­¾
    if job_in.tags:
        for tag_name in job_in.tags:
            tag_result = await db.execute(select(JobTag).where(JobTag.name == tag_name))
            tag = tag_result.scalar_one_or_none()
            if not tag:
                tag = JobTag(name=tag_name)
                db.add(tag)
            job.tags.append(tag)
    
    db.add(job)
    await db.commit()
    await db.refresh(job)
    
    return {
        "id": job.id,
        "title": job.title,
        "company": job.company,
        "location": job.location,
        "description": job.description,
        "salary_min": job.salary_min,
        "salary_max": job.salary_max,
        "status": job.status.value if job.status else "active",
        "created_at": job.created_at.isoformat() if job.created_at else None
    }


@router.get("/my-jobs")
async def list_my_jobs(
    user_id: int = Query(...),
    db: AsyncSession = Depends(get_db)
):
    """è·å–ç”¨æˆ·å‘å¸ƒçš„å²—ä½åˆ—è¡¨"""
    result = await db.execute(
        select(Job).options(selectinload(Job.tags))
        .where(Job.owner_id == user_id)
        .order_by(Job.created_at.desc())
    )
    jobs = result.scalars().all()
    
    return [
        {
            "id": j.id,
            "title": j.title,
            "company": j.company,
            "location": j.location,
            "description": j.description,
            "salary_min": j.salary_min,
            "salary_max": j.salary_max,
            "status": j.status.value if j.status else "active",
            "tags": [t.name for t in j.tags] if j.tags else [],
            "view_count": j.view_count or 0,
            "apply_count": j.apply_count or 0,
            "created_at": j.created_at.isoformat() if j.created_at else None,
        }
        for j in jobs
    ]


class PublicJobUpdate(PydanticBaseModel):
    """å…¬å¼€æ›´æ–°å²—ä½è¯·æ±‚"""
    user_id: int
    title: Optional[str] = None
    company: Optional[str] = None
    location: Optional[str] = None
    description: Optional[str] = None
    salary_min: Optional[int] = None
    salary_max: Optional[int] = None
    status: Optional[str] = None
    tags: Optional[TypingList[str]] = None


@router.put("/jobs/{job_id}")
async def update_public_job(
    job_id: int,
    job_in: PublicJobUpdate,
    db: AsyncSession = Depends(get_db)
):
    """æ›´æ–°å²—ä½"""
    result = await db.execute(
        select(Job).options(selectinload(Job.tags)).where(Job.id == job_id)
    )
    job = result.scalar_one_or_none()
    if not job:
        raise HTTPException(status_code=404, detail="å²—ä½ä¸å­˜åœ¨")
    if job.owner_id != job_in.user_id:
        raise HTTPException(status_code=403, detail="æ— æƒä¿®æ”¹æ­¤å²—ä½")
    
    if job_in.title is not None: job.title = job_in.title
    if job_in.company is not None: job.company = job_in.company
    if job_in.location is not None: job.location = job_in.location
    if job_in.description is not None: job.description = job_in.description
    if job_in.salary_min is not None: job.salary_min = job_in.salary_min
    if job_in.salary_max is not None: job.salary_max = job_in.salary_max
    if job_in.status is not None:
        job.status = JobStatus(job_in.status) if job_in.status in [s.value for s in JobStatus] else job.status
    if job_in.tags is not None:
        job.tags.clear()
        for tag_name in job_in.tags:
            tag_result = await db.execute(select(JobTag).where(JobTag.name == tag_name))
            tag = tag_result.scalar_one_or_none()
            if not tag:
                tag = JobTag(name=tag_name)
                db.add(tag)
            job.tags.append(tag)
    
    await db.commit()
    await db.refresh(job)
    return {"ok": True, "id": job.id}


@router.delete("/jobs/{job_id}")
async def delete_public_job(
    job_id: int,
    user_id: int = Query(...),
    db: AsyncSession = Depends(get_db)
):
    """åˆ é™¤å²—ä½"""
    result = await db.execute(select(Job).where(Job.id == job_id))
    job = result.scalar_one_or_none()
    if not job:
        raise HTTPException(status_code=404, detail="å²—ä½ä¸å­˜åœ¨")
    if job.owner_id != user_id:
        raise HTTPException(status_code=403, detail="æ— æƒåˆ é™¤æ­¤å²—ä½")
    
    await db.delete(job)
    await db.commit()
    return {"ok": True}


@router.get("/job-detail/{job_id}")
async def get_job_detail_with_applications(
    job_id: int,
    user_id: int = Query(...),
    db: AsyncSession = Depends(get_db)
):
    """è·å–å²—ä½è¯¦æƒ…åŠæŠ•é€’åˆ—è¡¨"""
    from app.models.user import User
    
    # è·å–å²—ä½ä¿¡æ¯
    result = await db.execute(
        select(Job).options(selectinload(Job.tags)).where(Job.id == job_id)
    )
    job = result.scalar_one_or_none()
    if not job:
        raise HTTPException(status_code=404, detail="å²—ä½ä¸å­˜åœ¨")
    if job.owner_id != user_id:
        raise HTTPException(status_code=403, detail="æ— æƒæŸ¥çœ‹æ­¤å²—ä½")
    
    # è·å–è¯¥å²—ä½çš„æ‰€æœ‰æŠ•é€’æµç¨‹
    flows_result = await db.execute(
        select(Flow)
        .options(selectinload(Flow.steps), selectinload(Flow.timeline))
        .where(Flow.job_id == job_id)
        .order_by(Flow.created_at.desc())
    )
    flows = flows_result.scalars().all()
    
    # è·å–å€™é€‰äººä¿¡æ¯
    applications = []
    for flow in flows:
        # è·å–å€™é€‰äºº
        cand_result = await db.execute(
            select(Candidate).where(Candidate.id == flow.candidate_id)
        )
        candidate = cand_result.scalar_one_or_none()
        
        # è·å–å€™é€‰äºº profile
        profile = None
        user_info = None
        if candidate:
            prof_result = await db.execute(
                select(CandidateProfile).where(CandidateProfile.candidate_id == candidate.id)
            )
            profile = prof_result.scalar_one_or_none()
            # è·å–ç”¨æˆ·åŸºæœ¬ä¿¡æ¯
            user_result = await db.execute(
                select(User).where(User.id == candidate.user_id)
            )
            user_info = user_result.scalar_one_or_none()
        
        applications.append({
            "flow_id": flow.id,
            "candidate_id": flow.candidate_id,
            "status": flow.status.value if flow.status else "unknown",
            "current_stage": flow.current_stage.value if flow.current_stage else "unknown",
            "current_step": flow.current_step,
            "match_score": flow.match_score or 0,
            "tokens_consumed": flow.tokens_consumed or 0,
            "next_action": flow.next_action,
            "details": flow.details,
            "last_action": flow.last_action,
            "created_at": flow.created_at.isoformat() if flow.created_at else None,
            "updated_at": flow.updated_at.isoformat() if flow.updated_at else None,
            "candidate_name": profile.display_name if profile else (user_info.name if user_info else f"å€™é€‰äºº#{flow.candidate_id}"),
            "candidate_role": profile.current_role if profile else None,
            "candidate_avatar": user_info.avatar_url if user_info else None,
            "candidate_experience": profile.experience_years if profile else None,
            "candidate_summary": profile.summary if profile else None,
        })
    
    # ç»Ÿè®¡æ•°æ®
    status_counts = {}
    for app in applications:
        s = app["status"]
        status_counts[s] = status_counts.get(s, 0) + 1
    
    return {
        "job": {
            "id": job.id,
            "title": job.title,
            "company": job.company,
            "location": job.location,
            "description": job.description,
            "salary_min": job.salary_min,
            "salary_max": job.salary_max,
            "status": job.status.value if job.status else "active",
            "tags": [t.name for t in job.tags] if job.tags else [],
            "view_count": job.view_count or 0,
            "apply_count": job.apply_count or 0,
            "created_at": job.created_at.isoformat() if job.created_at else None,
        },
        "applications": applications,
        "stats": {
            "total": len(applications),
            "status_counts": status_counts,
        }
    }
